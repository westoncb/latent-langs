STRUCTURE AttentionMechanism {
  PARAMETERS {
    query_size : Int
    key_size : Int 
    value_size : Int
    hidden_size : Int
  }

  NOTATION {
    α(q, K) := ATTENTION_WEIGHTS(q, K)  
    V(K, v) := VALUE_VECTORS(K, v)
    qᵀ := TRANSPOSE(q)
  }

  DEF Tensor := [[[Float]]]

  STRUCTURE DotProductAttention {
    DEF query_proj : Tensor[query_size, hidden_size]
    DEF key_proj : Tensor[key_size, hidden_size] 
    DEF value_proj : Tensor[value_size, hidden_size]

    DEF init(query_size, key_size, value_size, hidden_size) := (
      query_proj := XAVIER_INIT(query_size, hidden_size),
      key_proj := XAVIER_INIT(key_size, hidden_size),
      value_proj := XAVIER_INIT(value_size, hidden_size)
    )

    DEF forward(query : Tensor[batch_size, seq_len, query_size],
                key : Tensor[batch_size, seq_len, key_size], 
                value : Tensor[batch_size, seq_len, value_size]) -> Tensor[batch_size, seq_len, hidden_size] := (
      LET Q := MATMUL(query, query_proj),
          K := MATMUL(key, key_proj),
          V := MATMUL(value, value_proj),
          weights := α(Q, K),
          output := MATMUL(weights, V)
      IN output  
    )
  }

  STRUCTURE MultiHeadAttention {
    PARAMETERS {
      num_heads : Int
    }

    DEF heads : [DotProductAttention]

    DEF init(query_size, key_size, value_size, hidden_size, num_heads) := (
      heads := [DotProductAttention(query_size, key_size, value_size, hidden_size / num_heads) FOR _ : range(num_heads)]
    )

    DEF forward(query : Tensor[batch_size, seq_len, query_size],
                key : Tensor[batch_size, seq_len, key_size], 
                value : Tensor[batch_size, seq_len, value_size]) -> Tensor[batch_size, seq_len, hidden_size] := (
      LET head_outputs := [head.forward(query, key, value) FOR head : heads],
          concat_output := CONCAT(head_outputs, dim=-1)
      IN concat_output
    )
  }

  DEF ATTENTION_WEIGHTS(query : Tensor[batch_size, seq_len, hidden_size],
                        key : Tensor[batch_size, seq_len, hidden_size]) -> Tensor[batch_size, seq_len, seq_len] := (
    LET scores := MATMUL(query, TRANSPOSE(key, (0, 2, 1))) / SQRT(hidden_size),
        weights := SOFTMAX(scores, dim=-1)
    IN weights                  
  )

  DEF VALUE_VECTORS(key : Tensor[batch_size, seq_len, hidden_size], 
                    value : Tensor[batch_size, seq_len, hidden_size]) -> Tensor[batch_size, seq_len, hidden_size] := (
    value
  )

  [∀q, ∀K, ∀V . 
    MATMUL(α(q, K), V(K, V)).shape = (q.shape[0], q.shape[1], V.shape[2])
  ] # Output shape of attention is (batch_size, query_seq_len, value_hidden_size)

  [∀q, ∀K .
    ∑(α(q, K), dim=-1) = 1
  ] # Attention weights sum to 1 over key dimension  

  [∀q₁, ∀q₂, ∀K, ∀V .
    COSINE_SIMILARITY(q₁, q₂) ≈ 1 => COSINE_SIMILARITY(MATMUL(α(q₁, K), V(K, V)), MATMUL(α(q₂, K), V(K, V))) ≈ 1
  ] # Similar queries attend to similar values

  [∀q, ∀K₁, ∀K₂, ∀V₁, ∀V₂ .
    K₁ ≈ K₂ AND V₁ ≠ V₂ => MATMUL(α(q, K₁), V(K₁, V₁)) ≠ MATMUL(α(q, K₂), V(K₂, V₂)) 
  ] # Different values with similar keys produce different outputs

  RETURN MultiHeadAttention
}