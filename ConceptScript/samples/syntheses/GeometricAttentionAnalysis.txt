CONCEPT GeometricAttentionAnalysis {
  PARAMETERS {
    d : Nat -- Dimension of embedding space
    h : Nat -- Number of attention heads
    N : Nat -- Sequence length
  }

  CONTEXT {
    TYPES {
      INDUCTIVE Type {
        CASE Base
        CASE Arrow(Type, Type)
        CASE TensorProduct(Type, Type)
      }
      
      INDUCTIVE Term {
        CASE Var(String)
        CASE Abs(String, Type, Term)
        CASE App(Term, Term)
        CASE AttentionOp(Term, Term, Term) -- Query, Key, Value
      }
      
      Embedding := â„^d
      AttentionMap := Embedding â†’ Embedding â†’ â„
      
      Shape := CS(â„^d)
      DIR := ð•Š^(d-1)
      MPD := Multiset(Î”)
      Î” := {(b, d) | b â‰¤ d}
    }
    
    STRUCTURES {
      STRUCTURE TypedAttention {
        FIELD TypeOf : Term â†’ Type
          CASE Var(x) â†’ Base
          CASE Abs(x, T, t) â†’ Arrow(T, TypeOf(t))
          CASE App(f, t) â†’ (
            LET S = TypeOf(f)
            LET T = TypeOf(t)
            REQUIRE S IS Arrow(U, V)
            REQUIRE U = T
            V
          )
          CASE AttentionOp(q, k, v) â†’ (
            REQUIRE TypeOf(q) = TypeOf(k) = TypeOf(v) = TensorProduct(Base, Base)
            TensorProduct(Base, Base)
          )
        
        AXIOM SubjectReduction {
          TypeOf(t) = T âˆ§ t ~> t' â‡’ TypeOf(t') = T
        }
      }
      
      STRUCTURE AttentionGeometry {
        FIELD A : AttentionMap
        FIELD Z_A := {(x, y, t) âˆˆ Embedding Ã— Embedding Ã— â„ | A(x)(y) â‰¤ t}
        FIELD f_A : Z_A â†’ DIR Ã— â„
        FIELD PHT_A := R(f_A)_*ð•œ_Z_A IN D^b(Shv(DIR Ã— â„))
        FIELD PHT^i_A := H^i(PHT_A) IN Shv(DIR Ã— â„)
        FIELD Dgm_A(i, v) â†’ MPD
          WHERE âˆƒ p : â„ â†’ MPD . Dgm_A(i, v) = p(v) AND p = Dgm(PHT^i_A|_{v}Ã—â„)
        
        AXIOM ContinuityOfAttention {
          âˆ€ Îµ > 0, âˆƒ Î´ > 0, âˆ€ x y x' y' âˆˆ Embedding .
            â€–x - x'â€– < Î´ âˆ§ â€–y - y'â€– < Î´ â‡’ |A(x)(y) - A(x')(y')| < Îµ
        }
      }
      
      STRUCTURE MultiheadAttentionGeometry {
        FIELD Heads : Fin(h) â†’ AttentionGeometry
        FIELD CombinedPHT := âŠ•_{i âˆˆ Fin(h)} PHT_{Heads[i]}
        
        AXIOM OrthogonalHeads {
          âˆ€ i j âˆˆ Fin(h), x y âˆˆ Embedding . i â‰  j â‡’ 
            âŸ¨Heads[i].A(x)(y), Heads[j].A(x)(y)âŸ© = 0
        }
      }
    }
    
    NOTATION {
      ~> := Reduces
      âŠ• := DirectSum
      âŸ¨_, _âŸ© := InnerProduct
      Shv := Shv(DIR Ã— â„, â„)
      D^b := D^b(Shv)
      R := DerivedDirectImage
      H^i := CohomologyFunctor(_, i)
    }
  }

  TRANSFORMERS {
    REWRITE BetaReduction {
      App(Abs(x, T, t), s) ~> t[x â†¦ s]
    }
    
    REWRITE AttentionComputation {
      AttentionOp(q, k, v) ~> 
        LET Î± = SoftMax(Seq(âŸ¨Embed(k[i]), Embed(q)âŸ© / âˆšd for i in Fin(N)))
        IN Sum(i in Fin(N), Î±[i] * Embed(v[i]))
    }
    
    SIMPLIFY PersistenceDiagramDecomposition(A : AttentionMap) {
      Dgm_A(i, v) = âˆ_{I âˆˆ Ï€â‚€(BCT_A(v, ?))} GenDgm(Î²_i(A_I))
        WHERE A_I := {(x, y) âˆˆ Embedding Ã— Embedding | A(x)(y) âˆˆ I}
    }
  }

  PROOFS {
    THEOREM TypeSoundness {
      âˆ€ t : Term, T : Type . TypeOf(t) = T â‡’ (t ~>* v âˆ§ TypeOf(v) = T) âˆ¨ (t â†‘)
    }
    PROOF BY TypedAttention.SubjectReduction, Progress, Preservation {
      PROVE TypeOf(t) = T â‡’ (t ~>* v âˆ§ TypeOf(v) = T)
      PROVE TypeOf(t) = T â‡’ t IS_VALUE âˆ¨ (âˆƒ t' . t ~> t')
      PROVE TypeOf(t) = T âˆ§ t ~> t' â‡’ TypeOf(t') = T   
    }
    
    THEOREM AttentionStability âˆ€ A B : AttentionMap . 
      dI(PHT_A, PHT_B) â‰¤ sup_{x, y âˆˆ Embedding} |A(x)(y) - B(x)(y)| {
      GIVEN A B : AttentionMap
      
      LET Îµ = sup_{x, y âˆˆ Embedding} |A(x)(y) - B(x)(y)|
      
      HAVE âˆ€ x y t . (x, y, t) âˆˆ Z_A â‡” (x, y, t+Îµ) âˆˆ Z_B
      HENCE âˆƒ f : Z_A â†ª Z_B, g : Z_B â†ª Z_A . 
        f((x, y, t)) = (x, y, t+Îµ) âˆ§ g((x, y, t)) = (x, y, t-Îµ)
      
      SHOW dI(PHT_A, PHT_B) â‰¤ Îµ
        BY InterleaveEquivalence(f, g)
      QED
    }
    
    THEOREM GeometricInterpretationOfAttention 
      âˆ€ A : AttentionMap, i : Nat, v : DIR .
        Dgm_A(i, v) CHARACTERIZES AttentionFlow(A, v) {
      GIVEN A : AttentionMap, i : Nat, v : DIR
      
      DEFINE AttentionFlow(A, v) := 
        {(x, y) âˆˆ Embedding Ã— Embedding | âˆ‡_y A(x)(y) = Î»v, Î» > 0}
      
      <1> SHOW Dgm_A(i, v) CAPTURES ConnectedComponents(AttentionFlow(A, v))
        BY PersistenceDiagramDecomposition(A)
      
      <2> SHOW ConnectedComponents(AttentionFlow(A, v)) CHARACTERIZES 
            AttentionPatterns(A, v)
        WHERE AttentionPatterns(A, v) := 
          {S âŠ† Embedding Ã— Embedding | âˆ€ (x, y) âˆˆ S . A(x)(y) >> A(x)(y') âˆ€ y' âˆ‰ S}
      
      CONCLUDE Dgm_A(i, v) CHARACTERIZES AttentionFlow(A, v)
      QED
    }
  }
  
  EXAMPLES {
    EXAMPLE SelfAttentionAnalysis {
      LET sa = SelfAttention(512, 8)
      LET mag = MultiheadAttentionGeometry WITH {
        Heads = Î» i . AttentionGeometry WITH {
          A = Î» x y . sa.Heads[i].Alignment(x, y) / âˆš512
        }
      }
      
      THEN âˆ€ i âˆˆ Fin(8), v âˆˆ DIR . 
        Dgm_{mag.Heads[i]}(0, v) CHARACTERIZES KeyValueRelationships(sa.Heads[i], v)
        WHERE KeyValueRelationships(h, v) := 
          {(k, v) | h.Alignment(k, q) >> h.Alignment(k', q) âˆ€ k' â‰  k, âŸ¨q, vâŸ© â‰ˆ 1}
    }
    
    EXAMPLE CrossModalAttentionAnalysis {
      LET ca = CrossAttention(512, 128, 512, 8)
      LET mag = MultiheadAttentionGeometry WITH {
        Heads = Î» i . AttentionGeometry WITH {
          A = Î» x y . ca.Heads[i].Alignment(x, y) / âˆš(512 * 128)
        }
      }
      
      THEN âˆ€ i âˆˆ Fin(8), v âˆˆ DIR . 
        Dgm_{mag.Heads[i]}(1, v) CHARACTERIZES CrossModalAlignments(ca.Heads[i], v)
        WHERE CrossModalAlignments(h, v) := 
          {(q, k) | h.Alignment(k, q) >> h.Alignment(k', q) âˆ€ k' â‰  k, âŸ¨q, vâŸ© â‰ˆ 1}
    }
  }
}