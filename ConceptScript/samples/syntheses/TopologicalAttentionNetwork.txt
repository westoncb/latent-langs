CONCEPT TopologicalAttentionNetwork {
  PARAMETERS {
    d : Nat -- Input dimension
    h : Nat -- Number of attention heads
    ùïú : Field -- Coefficient field
  }

  CONTEXT {
    TYPES {
      INDUCTIVE LayerType {
        CASE Input
        CASE Hidden
        CASE Output
      }

      PointCloud := List(‚Ñù^d)
      
      PHDgm := Multiset(Œî)
      Œî := {(b, d) | b ‚â§ d}
      
      AttentionWeight := ‚Ñù
      
      STRUCTURE Neuron {
        FIELD layer : LayerType
        FIELD activation : ‚Ñù^d -> ‚Ñù
      }
      
      STRUCTURE Layer {
        FIELD type : LayerType
        FIELD neurons : List(Neuron)
      }
      
      STRUCTURE Network {
        FIELD layers : List(Layer)
      }
    }
    
    STRUCTURES {
      STRUCTURE PersistentHomology {
        FIELD computePHT : PointCloud -> PHDgm
        
        AXIOM Stability ‚àÄ X Y : PointCloud .
          dB(computePHT(X), computePHT(Y)) ‚â§ dH(X, Y)
          WHERE dB := BottleneckDistance
                dH := HausdorffDistance
      }
      
      STRUCTURE TopologicalAttention IMPLEMENTS Attention(PHDgm, PHDgm, ‚Ñù^d) {
        PARAMETERS {
          persistentHomology : PersistentHomology
        }
        
        IMPLEMENT Alignment(dgm1 : PHDgm, dgm2 : PHDgm) -> ‚Ñù
          WITH exp(-dB(dgm1, dgm2) / œÉ)
          WHERE dB := BottleneckDistance
                œÉ  := ScalingFactor
        
        IMPLEMENT FUNC(q : PHDgm, ks : Seq(PHDgm), vs : Seq(‚Ñù^d)) -> ‚Ñù^d
          WITH Œ£_i Œ±_i * vs[i]
          WHERE Œ± = SoftMax(Seq(Alignment(ks[i], q) for i in Fin(N)))
        
        AXIOM TopologicalInvariance ‚àÄ X Y : PointCloud .
          Homeo(X, Y) => Alignment(computePHT(X), computePHT(Y)) = 1
      }
      
      STRUCTURE TopologicalAttentionLayer IMPLEMENTS Layer {
        FIELD attention : TopologicalAttention
        FIELD inputDim : Nat
        FIELD outputDim : Nat
        
        FUNC forward(input : List(PointCloud)) -> List(‚Ñù^outputDim) {
          LET dgms = MAP(input, attention.persistentHomology.computePHT)
          LET attended = attention(dgms[0], dgms, input)
          RETURN MAP(attended, Œª x . Linear(x, inputDim, outputDim))
        }
      }
    }
    
    NOTATION {
      Homeo := Homeomorphism
      dB := BottleneckDistance
      dH := HausdorffDistance
    }
  }

  TRANSFORMERS {
    REWRITE AttentionLinearity {
      TopologicalAttention(a * dgms1 + b * dgms2, qs, a * vs1 + b * vs2) <=>
        a * TopologicalAttention(dgms1, qs, vs1) + b * TopologicalAttention(dgms2, qs, vs2)
    }
    
    SIMPLIFY HomeoInvariance {
      TopologicalAttention(Homeo(X), qs, vs) <=> TopologicalAttention(X, qs, vs)
    }
  }

  PROOFS {
    THEOREM TopologicalAttentionStability {
      FORALL (X Y : PointCloud, qs : Seq(PHDgm), vs : Seq(‚Ñù^d)) .
        ‚ÄñTopologicalAttention(X, qs, vs) - TopologicalAttention(Y, qs, vs)‚Äñ ‚â§ L * dH(X, Y)
        WHERE L := LipschitzConstant
    }
    PROOF {
      ASSUME X Y : PointCloud, qs : Seq(PHDgm), vs : Seq(‚Ñù^d)
      
      LET dgmX = persistentHomology.computePHT(X)
      LET dgmY = persistentHomology.computePHT(Y)
      
      <1>. SHOW dB(dgmX, dgmY) ‚â§ dH(X, Y)
        BY PersistentHomology.Stability
      
      <2>. LET Œ±X = SoftMax(Seq(Alignment(ks[i], dgmX) for i in Fin(N)))
           LET Œ±Y = SoftMax(Seq(Alignment(ks[i], dgmY) for i in Fin(N)))
      
      <3>. SHOW ‚ÄñŒ±X - Œ±Y‚Äñ ‚â§ L1 * dB(dgmX, dgmY)
        BY LipschitzContinuityOfSoftmax
      
      <4>. SHOW ‚ÄñTopologicalAttention(X, qs, vs) - TopologicalAttention(Y, qs, vs)‚Äñ
             = ‚ÄñŒ£_i (Œ±X[i] - Œ±Y[i]) * vs[i]‚Äñ
             ‚â§ ‚ÄñŒ±X - Œ±Y‚Äñ * MAX(‚Äñvs[i]‚Äñ)
             ‚â§ L1 * dB(dgmX, dgmY) * MAX(‚Äñvs[i]‚Äñ)
             ‚â§ L1 * dH(X, Y) * MAX(‚Äñvs[i]‚Äñ)
        BY <1>, <2>, <3>, TriangleInequality
      
      TAKE L := L1 * MAX(‚Äñvs[i]‚Äñ)
      QED
    }
    
    THEOREM TopologicalAttentionPreservesHomeo {
      FORALL (X Y : PointCloud, qs : Seq(PHDgm), vs : Seq(‚Ñù^d)) .
        Homeo(X, Y) => 
          TopologicalAttention(X, qs, vs) = TopologicalAttention(Y, qs, vs)
    }
    PROOF {
      ASSUME X Y : PointCloud, qs : Seq(PHDgm), vs : Seq(‚Ñù^d), Homeo(X, Y)
      
      <1>. SHOW persistentHomology.computePHT(X) = persistentHomology.computePHT(Y)
        BY HomeoInvarianceOfPersistentHomology
      
      <2>. LET dgm = persistentHomology.computePHT(X)
      
      <3>. SHOW Alignment(ks[i], dgm) = Alignment(ks[i], dgm) for all i
        TRIVIAL
      
      <4>. SHOW SoftMax(Seq(Alignment(ks[i], dgm) for i in Fin(N))) =
                SoftMax(Seq(Alignment(ks[i], dgm) for i in Fin(N)))
        BY <3>
      
      <5>. CONCLUDE TopologicalAttention(X, qs, vs) = TopologicalAttention(Y, qs, vs)
        BY <1>, <2>, <4>, DefinitionOfTopologicalAttention
      
      QED
    }
  }

  EXAMPLES {
    EXAMPLE SimpleTopologicalAttentionNetwork {
      LET inputDim = 3
      LET hiddenDim = 64
      LET outputDim = 1
      LET numHeads = 4
      
      DEFINE SimpleNetwork = Network WITH {
        layers = [
          TopologicalAttentionLayer WITH {
            type = LayerType.Input,
            inputDim = inputDim,
            outputDim = hiddenDim,
            attention = MultiheadAttention(numHeads) WITH {
              KEY = PHDgm,
              QUERY = PHDgm,
              VALUE = ‚Ñù^inputDim
            }
          },
          TopologicalAttentionLayer WITH {
            type = LayerType.Hidden,
            inputDim = hiddenDim,
            outputDim = hiddenDim,
            attention = MultiheadAttention(numHeads) WITH {
              KEY = PHDgm,
              QUERY = PHDgm,
              VALUE = ‚Ñù^hiddenDim
            }
          },
          Layer WITH {
            type = LayerType.Output,
            neurons = [Neuron WITH {
              layer = LayerType.Output,
              activation = Œª x . Linear(x, hiddenDim, outputDim)
            }]
          }
        ]
      }
      
      LET input : List(PointCloud) = [
        [(0,0,0), (1,0,0), (0,1,0)],
        [(0,0,1), (1,0,1), (0,1,1)],
        [(1,1,0), (1,1,1)]
      ]
      
      THEN SimpleNetwork.layers[0].forward(input) : List(‚Ñù^hiddenDim)
      AND SimpleNetwork.layers[1].forward(SimpleNetwork.layers[0].forward(input)) : List(‚Ñù^hiddenDim)
      AND SimpleNetwork.layers[2].neurons[0].activation(
        SimpleNetwork.layers[1].forward(SimpleNetwork.layers[0].forward(input))[0]
      ) : ‚Ñù
    }
  }
}