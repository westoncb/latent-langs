CONCEPT GeometricAttention {
  PARAMETERS {
    d : Nat -- Ambient dimension
    ùïú : Field -- Coefficient field
    N : Nat -- Sequence length
  }

  CONTEXT {
    TYPES {
      Shape := CS(‚Ñù^d)
      PHT := D^b(Shv(DIR √ó ‚Ñù, ùïú))
      Seq(T) := FUNC(Fin(N), T)
      GeomAttention := FUNC(Shape, Seq(Shape), Seq(Shape)) -> Shape
    }

    STRUCTURES {
      STRUCTURE PHT_Embedding {
        FIELD Embed : Shape -> PHT
        AXIOM Functoriality(f : Shape -> Shape) {
          Embed(f(M)) ‚âÉ f^*(Embed(M))
        }
      }

      STRUCTURE GeomDotProductAttention IMPLEMENTS GeomAttention {
        PARAMETERS {
          embed : PHT_Embedding
        }

        FIELD Alignment(K : Shape, Q : Shape) -> Real := 
          InnerProduct(embed.Embed(K), embed.Embed(Q)) / SQRT(DIM(PHT))

        IMPLEMENT FUNC(Q : Shape, Ks : Seq(Shape), Vs : Seq(Shape)) -> Shape
          WITH Reconstruct(Œ£_i Œ±_i * embed.Embed(Vs[i]))
          WHERE Œ± = SoftMax(Seq(Alignment(Ks[i], Q) for i in Fin(N)))

        FIELD Reconstruct : PHT -> Shape
          AXIOM ReconstructEmbed {
            ‚àÄ M : Shape . Reconstruct(embed.Embed(M)) ‚âÉ M
          }
      }

      STRUCTURE GeomMultiheadAttention(h : Nat) IMPLEMENTS GeomAttention {
        PARAMETERS {
          Heads : FUNC(Fin(h), GeomAttention)
          Concat : FUNC(Seq(Shape), Shape^h)
          Proj : FUNC(Shape^h, Shape)
        }

        IMPLEMENT FUNC(Q : Shape, Ks : Seq(Shape), Vs : Seq(Shape)) -> Shape
          WITH Proj(Concat(Seq(Heads[i](Q, Ks, Vs) for i in Fin(h))))
      }
    }

    NOTATION {
      ‚ü®K, Q‚ü© := Alignment(K, Q)
      Œ£_i x_i := SUM(i = 0 to N - 1, x_i)
    }
  }

  TRANSFORMERS {
    REWRITE GeomLinearity {
      GeomAttention(a * Ks + b * Ks', Q, a * Vs + b * Vs') <=>
        a * GeomAttention(Ks, Q, Vs) + b * GeomAttention(Ks', Q, Vs')
      WHERE a * Ks := Seq(ScalarMult(a, Ks[i]) for i in Fin(N))
            Ks + Ks' := Seq(ShapeAdd(Ks[i], Ks'[i]) for i in Fin(N))
    }

    SIMPLIFY IgnoreQueryOrder {
      GeomAttention(Ks, PERMUTE(Qs), Vs) <=> GeomAttention(Ks, Qs, Vs)
    }

    SIMPLIFY IgnoreKeyValueOrder {
      GeomAttention(PERMUTE(Ks), Q, PERMUTE(Vs)) <=> 
        PERMUTE(GeomAttention(Ks, Q, Vs))
    }
  }

  PROOFS {
    THEOREM GeomAttentionPreservesHomology {
      FORALL (Q : Shape, Ks Vs : Seq(Shape), i : Nat) .
        PHT^i(GeomDotProductAttention(Q, Ks, Vs)) ‚âÉ 
          Œ£_j Œ±_j * PHT^i(Vs[j])
      WHERE Œ± = SoftMax(Seq(‚ü®Ks[j], Q‚ü© for j in Fin(N)))
    }
    PROOF {
      <1>. LET att = GeomDotProductAttention(Q, Ks, Vs)
      <2>. att = Reconstruct(Œ£_j Œ±_j * embed.Embed(Vs[j])) [BY DEF GeomDotProductAttention]
      <3>. PHT^i(att) ‚âÉ PHT^i(Reconstruct(Œ£_j Œ±_j * embed.Embed(Vs[j]))) [BY <2>]
      <4>. PHT^i(Reconstruct(Œ£_j Œ±_j * embed.Embed(Vs[j]))) ‚âÉ
           Œ£_j Œ±_j * PHT^i(Vs[j]) [BY Functoriality, ReconstructEmbed]
      <5>. QED [BY <3>, <4>]
    }

    THEOREM GeomAttentionStability {
      FORALL (Q Q' : Shape, Ks Ks' Vs Vs' : Seq(Shape), d : Met) .
        d(GeomDotProductAttention(Q, Ks, Vs), GeomDotProductAttention(Q', Ks', Vs')) ‚â§
          C * (d(Q, Q') + MAX(d(Ks[i], Ks'[i])) + MAX(d(Vs[i], Vs'[i])))
      WHERE C is a constant depending on the Lipschitz constants of Embed, Reconstruct, and SoftMax
    }
    PROOF {
      ASSUME Q Q' : Shape, Ks Ks' Vs Vs' : Seq(Shape), d : Met
      <1>. LET att = GeomDotProductAttention(Q, Ks, Vs)
           LET att' = GeomDotProductAttention(Q', Ks', Vs')
      <2>. d(att, att') ‚â§ d(Reconstruct(Œ£_i Œ±_i * embed.Embed(Vs[i])),
                            Reconstruct(Œ£_i Œ±'_i * embed.Embed(Vs'[i])))
           [BY DEF GeomDotProductAttention]
      <3>. ‚â§ L_R * d(Œ£_i Œ±_i * embed.Embed(Vs[i]), Œ£_i Œ±'_i * embed.Embed(Vs'[i]))
           [BY Lipschitz continuity of Reconstruct]
      <4>. ‚â§ L_R * (d(Œ£_i Œ±_i * embed.Embed(Vs[i]), Œ£_i Œ±_i * embed.Embed(Vs'[i])) +
                    d(Œ£_i Œ±_i * embed.Embed(Vs'[i]), Œ£_i Œ±'_i * embed.Embed(Vs'[i])))
           [BY triangle inequality]
      <5>. ‚â§ L_R * (MAX(Œ±_i) * L_E * MAX(d(Vs[i], Vs'[i])) +
                    L_S * (L_A * d(Q, Q') + L_A * MAX(d(Ks[i], Ks'[i]))))
           [BY Lipschitz continuity of Embed, SoftMax, and Alignment]
      <6>. ‚â§ C * (d(Q, Q') + MAX(d(Ks[i], Ks'[i])) + MAX(d(Vs[i], Vs'[i])))
           [BY collecting terms]
      <7>. QED
    }
  }

  EXAMPLES {
    EXAMPLE SphericalAttention {
      DEFINE S^2 = {x ‚àà ‚Ñù^3 | ‚Äñx‚Äñ = 1}
      
      LET embed = PHT_Embedding WITH {
        Embed(M : S^2) := PHT_M
      }
      
      LET satt = GeomDotProductAttention WITH {
        embed := embed
      }
      
      LET Q = NorthPole
      LET Ks = Seq(RandomPointsOn(S^2, N))
      LET Vs = Seq(RandomPointsOn(S^2, N))
      
      THEN satt(Q, Ks, Vs) ‚àà S^2
      AND Dgm(satt(Q, Ks, Vs)) ‚âÉ [[], [(0,‚àû)], []]
    }

    EXAMPLE TorusAttention {
      DEFINE T^2 = S^1 √ó S^1
      
      LET embed = PHT_Embedding WITH {
        Embed(M : T^2) := PHT_M
      }
      
      LET tatt = GeomMultiheadAttention(4) WITH {
        Heads := Œª i . GeomDotProductAttention WITH { embed := embed }
      }
      
      LET Q = (0, 0)
      LET Ks = Seq(RandomPointsOn(T^2, N))
      LET Vs = Seq(RandomPointsOn(T^2, N))
      
      THEN tatt(Q, Ks, Vs) ‚àà T^2
      AND Dgm(tatt(Q, Ks, Vs)) ‚âÉ [[], [(0,‚àû), (0,‚àû)], [(0,‚àû)], []]
    }
  }
}