CONCEPT VariationalAutoencoder {
  PARAMETERS {
    X : TYPE  -- Data space
    Z : TYPE  -- Latent space
    Œò : TYPE  -- Parameter space for encoder
    Œ¶ : TYPE  -- Parameter space for decoder
  }

  CONTEXT {
    TYPES {
      DataPoint := X
      LatentVector := Z
      
      Distribution := FUNC(LatentVector) -> Real
      ConditionalDistribution := FUNC(DataPoint) -> Distribution
      
      Encoder := STRUCTURE {
        FIELD mean : FUNC(DataPoint, Œò) -> LatentVector
        FIELD logvar : FUNC(DataPoint, Œò) -> LatentVector
      }
      
      Decoder := FUNC(LatentVector, Œ¶) -> ConditionalDistribution
      
      LossFunction := FUNC(DataPoint, DataPoint, Distribution, Distribution) -> Real
    }

    STRUCTURES {
      STRUCTURE VAE {
        FIELD encoder : Encoder
        FIELD decoder : Decoder
        FIELD prior : Distribution
        
        FUNC encode(x : DataPoint, Œ∏ : Œò) -> Distribution {
          RETURN GaussianDistribution(encoder.mean(x, Œ∏), EXP(encoder.logvar(x, Œ∏)))
        }
        
        FUNC decode(z : LatentVector, œÜ : Œ¶) -> ConditionalDistribution {
          RETURN decoder(z, œÜ)
        }
        
        FUNC reparameterize(Œº : LatentVector, œÉ : LatentVector) -> LatentVector {
          LET Œµ = SampleFromStandardNormal()
          RETURN Œº + œÉ * Œµ
        }
        
        FUNC loss(x : DataPoint, x_recon : DataPoint, q_z : Distribution, p_z : Distribution) -> Real {
          RETURN ReconstructionLoss(x, x_recon) + KLDivergence(q_z, p_z)
        }
      }
    }

    ASSERTIONS {
      AXIOM EncoderGaussian {
        FORALL (vae : VAE, x : DataPoint, Œ∏ : Œò) .
          vae.encode(x, Œ∏) IS GaussianDistribution
      }
      
      AXIOM PriorIsStandardNormal {
        FORALL (vae : VAE) .
          vae.prior = StandardNormalDistribution
      }
      
      AXIOM EvidenceLowerBound {
        FORALL (vae : VAE, x : DataPoint, Œ∏ : Œò, œÜ : Œ¶) .
          LOG(p(x)) >= Expectation(q(z|x))[LOG(p(x|z))] - KLDivergence(q(z|x), p(z))
          WHERE q(z|x) = vae.encode(x, Œ∏)
                p(x|z) = vae.decode(z, œÜ)(x)
                p(z) = vae.prior
      }
    }

    NOTATION {
      p(x) := Marginal likelihood of data
      q(z|x) := Approximate posterior
      p(x|z) := Generative model
      p(z) := Prior distribution
      KL(P || Q) := Kullback-Leibler divergence between P and Q
      ùîº[x] := Expectation of x
    }
  }

  TRANSFORMERS {
    REWRITE SampleLatentVector(vae : VAE, x : DataPoint, Œ∏ : Œò) -> LatentVector {
      LET Œº = vae.encoder.mean(x, Œ∏)
      LET œÉ = SQRT(EXP(vae.encoder.logvar(x, Œ∏)))
      IN vae.reparameterize(Œº, œÉ)
    }
    
    SIMPLIFY ComputeELBO(vae : VAE, x : DataPoint, Œ∏ : Œò, œÜ : Œ¶) -> Real {
      LET q_z = vae.encode(x, Œ∏)
      LET z = SampleLatentVector(vae, x, Œ∏)
      LET x_recon = vae.decode(z, œÜ)
      IN -vae.loss(x, x_recon, q_z, vae.prior)
    }
  }

  PROOFS {
    THEOREM VAEOptimizesELBO {
      FORALL (vae : VAE, D : SET(DataPoint)) .
        OPTIMUM(Œ∏ : Œò, œÜ : Œ¶) [
          MEAN(x in D)[ComputeELBO(vae, x, Œ∏, œÜ)]
        ] APPROXIMATES MAXIMUM(Œ∏ : Œò, œÜ : Œ¶) [
          MEAN(x in D)[LOG(p(x))]
        ]
    }
    PROOF {
      GIVEN vae : VAE, D : SET(DataPoint)
      
      <1>. FORALL (x : DataPoint) .
        LOG(p(x)) = ELBO(x) + KL(q(z|x) || p(z|x))
        WHERE ELBO(x) = ùîº_q(z|x)[LOG(p(x,z)) - LOG(q(z|x))]
      <2>. KL(q(z|x) || p(z|x)) >= 0 BY properties of KL divergence
      <3>. HENCE LOG(p(x)) >= ELBO(x)
      <4>. MEAN(x in D)[LOG(p(x))] >= MEAN(x in D)[ELBO(x)]
      <5>. ComputeELBO(vae, x, Œ∏, œÜ) IS unbiased estimator of ELBO(x)
      <6>. OPTIMIZING MEAN(x in D)[ComputeELBO(vae, x, Œ∏, œÜ)]
           APPROXIMATES OPTIMIZING MEAN(x in D)[ELBO(x)]
      <7>. BY <4> and <6>, OPTIMIZING MEAN(x in D)[ComputeELBO(vae, x, Œ∏, œÜ)]
           APPROXIMATES MAXIMIZING MEAN(x in D)[LOG(p(x))]
      
      QED
    }

    THEOREM VAERegularization {
      FORALL (vae : VAE, x : DataPoint, Œ∏ : Œò) .
        KLDivergence(vae.encode(x, Œ∏), vae.prior) = 
          0.5 * SUM(
            1 + vae.encoder.logvar(x, Œ∏) - vae.encoder.mean(x, Œ∏)^2 - EXP(vae.encoder.logvar(x, Œ∏))
          )
    }
    PROOF {
      GIVEN vae : VAE, x : DataPoint, Œ∏ : Œò
      
      <1>. LET q(z|x) = vae.encode(x, Œ∏)
      <2>. LET p(z) = vae.prior = StandardNormalDistribution
      <3>. KL(q(z|x) || p(z)) = ùîº_q(z|x)[LOG(q(z|x)) - LOG(p(z))]
      <4>. q(z|x) = N(Œº, œÉ^2) WHERE Œº = vae.encoder.mean(x, Œ∏), œÉ^2 = EXP(vae.encoder.logvar(x, Œ∏))
      <5>. LOG(q(z|x)) = -0.5 * (LOG(2œÄ) + LOG(œÉ^2) + (z-Œº)^2/œÉ^2)
      <6>. LOG(p(z)) = -0.5 * (LOG(2œÄ) + z^2)
      <7>. KL = ùîº_q[-0.5 * (LOG(œÉ^2) + (z-Œº)^2/œÉ^2 - z^2)]
      <8>. = -0.5 * (LOG(œÉ^2) + 1 - Œº^2 - œÉ^2)
      <9>. = 0.5 * (1 + LOG(œÉ^2) - Œº^2 - œÉ^2)
      <10>. = 0.5 * SUM(1 + vae.encoder.logvar(x, Œ∏) - vae.encoder.mean(x, Œ∏)^2 - EXP(vae.encoder.logvar(x, Œ∏)))
      
      QED
    }
  }

  EXAMPLES {
    EXAMPLE ImageVAE {
      LET X = RealMatrix(28, 28)  -- 28x28 grayscale images
      LET Z = Real^32  -- 32-dimensional latent space
      
      LET vae = VAE {
        encoder = Encoder {
          mean = ConvolutionalNetwork(
            CONV(32, 3, 3) -> RELU -> CONV(64, 3, 3) -> RELU -> DENSE(32)
          ),
          logvar = ConvolutionalNetwork(
            CONV(32, 3, 3) -> RELU -> CONV(64, 3, 3) -> RELU -> DENSE(32)
          )
        },
        decoder = DeconvolutionalNetwork(
          DENSE(7*7*32) -> RESHAPE(7, 7, 32) -> DECONV(64, 3, 3) -> RELU -> 
          DECONV(32, 3, 3) -> RELU -> DECONV(1, 3, 3) -> SIGMOID
        ),
        prior = StandardNormalDistribution(32)
      }
      
      COMPUTE encoded = SampleLatentVector(vae, LoadImage("digit.png"), TrainedParams)
      COMPUTE decoded = vae.decode(encoded, TrainedParams)
      
      ASSERT ImageSimilarity(decoded, LoadImage("digit.png")) > 0.9
      
      COMPUTE interpolated = [
        vae.decode(LinearInterpolate(encoded, ZERO_VECTOR, t), TrainedParams)
        FOR t IN LINSPACE(0, 1, 10)
      ]
      
      VISUALIZE interpolated AS "Latent Space Interpolation"
    }
    
    EXAMPLE MoleculeVAE {
      LET X = GraphStructure  -- Molecular graphs
      LET Z = Real^64  -- 64-dimensional latent space
      
      LET vae = VAE {
        encoder = Encoder {
          mean = GraphNeuralNetwork(
            GRAPH_CONV(32) -> RELU -> GRAPH_CONV(64) -> RELU -> 
            GLOBAL_POOLING -> DENSE(64)
          ),
          logvar = GraphNeuralNetwork(
            GRAPH_CONV(32) -> RELU -> GRAPH_CONV(64) -> RELU -> 
            GLOBAL_POOLING -> DENSE(64)
          )
        },
        decoder = GraphGenerativeNetwork(
          DENSE(64) -> RELU -> DENSE(128) -> RESHAPE(32, 4) -> 
          GRAPH_DECONV(32) -> RELU -> GRAPH_DECONV(16) -> SIGMOID
        ),
        prior = StandardNormalDistribution(64)
      }
      
      COMPUTE encoded = SampleLatentVector(vae, LoadMolecule("aspirin.mol"), TrainedParams)
      COMPUTE decoded = vae.decode(encoded, TrainedParams)
      
      ASSERT MoleculeSimilarity(decoded, LoadMolecule("aspirin.mol")) > 0.8
      
      COMPUTE novel_molecule = vae.decode(SampleFromPrior(), TrainedParams)
      ASSERT IsDrugLike(novel_molecule)
      VISUALIZE novel_molecule AS "Generated Drug-Like Molecule"
    }
  }
}