// Framework Concept
ANN(
  Layers:_,
  Neurons:_,
  ActivationFunctions:_,
  Optimizers:_, 
  LossFunction:_,
  ForwardPropagation:_,
  BackPropagation:_,
  WeightUpdate:_,
  TrainingLoop:_
) :: {
  DEF(
    |>: Sequence,
    â‡‰: Parallel,
    âŠ³: Interrupt,
    â§–: Synchronize,
    â‹‰: Split,
    â‹‹: Aggregate,
    âŠ¸: Map,
    âŠ•: Accumulate,
    â‹’: Filter,
    âŠº: Initialize,
    âŠ»: Finalize,
    â§¢: Subscribe,
    â§£: Publish
  )

  Neuron(Weights:_, Bias:_, ActivationFunction:ActivationFunctions) :: {
    [InputSignals âŠ¸ Weights âŠ• Bias |> ActivationFunction â§£ Output]
  }

  Layer(Neurons:Neurons) :: {
    [InputSignals â‡‰ Neurons â‹‹ Outputs]
  }

  ForwardPropagation := (
    InputData |>
    [Layer{*} âŠ¸ Neurons â‹‹ Outputs] |>
    OutputPrediction
  )

  BackPropagation := (
    OutputPrediction |>
    [LossFunction(TargetOutput:_) â§£ ðŸ“ˆ] |>
    [ðŸ“ˆ âŠ¸ Layer{*} âŠ¸ Neurons âŠ¸ (Weights, Bias) âŠ• Gradients]
  )

  WeightUpdate := (
    [Gradients, LearningRate] âŠ¸ Optimizers |>
    [Layer{*} âŠ¸ Neurons âŠ¸ (Weights, Bias) âŠ• Updates]
  )

  TrainingIteration := (
    TrainingData â‹‰ (InputBatch, TargetBatch) |>
    ForwardPropagation |>
    BackPropagation |>
    WeightUpdate
  )

  TrainingLoop := (
    [ðŸ’¾(TrainingData), ðŸ’¾(ValidationData), ðŸ”„(Epochs), ðŸ§©(Hyperparameters)] âŠº |>
    TrainingIteration{Epochs} â§– [ValidationData âŠ¸ ForwardPropagation âŠ¸ ðŸ“ˆ â§£ PerformanceHistory] |>
    [PerformanceHistory â‹’ ðŸ›‘{EarlyStoppingCriterion}] âŠ³ TrainingLoop |>
    [Layers âŠ¸ Neurons âŠ» TrainedModel{Weights, Bias}]
  )
}

// Specific ANN Concepts

FeedForwardNN := ANN(
  Layers:Sequential,
  Neurons:FullyConnected,
  ActivationFunctions:_, // e.g., ReLU, Sigmoid, Tanh
  Optimizers:_, // e.g., SGD, Adam, RMSprop
  LossFunction:_, // e.g., MSE, CrossEntropy
  ForwardPropagation:FeedForward,
  BackPropagation:Backprop,
  WeightUpdate:GradientDescent,
  TrainingLoop:SupervisedLearning
)

ConvolutionalNN := ANN(
  Layers:(Convolutional, Pooling, FullyConnected),
  Neurons:(Convolutional2D, MaxPooling2D, Flatten, Dense),
  ActivationFunctions:_, // e.g., ReLU, LeakyReLU
  Optimizers:_, // e.g., Adam, Adadelta
  LossFunction:_, // e.g., CategoricalCrossEntropy
  ForwardPropagation:ConvolutionalForward,
  BackPropagation:ConvolutionalBackprop,
  WeightUpdate:GradientDescent,
  TrainingLoop:SupervisedLearning
)

RecurrentNN := ANN(
  Layers:(Recurrent, FullyConnected),
  Neurons:(SimpleRNN, LSTM, GRU, Dense),
  ActivationFunctions:_, // e.g., Tanh, Sigmoid
  Optimizers:_, // e.g., RMSprop, AdamW
  LossFunction:_, // e.g., MSE, SparseCategoricalCrossEntropy
  ForwardPropagation:RecurrentForward,
  BackPropagation:BackpropThroughTime,
  WeightUpdate:GradientDescent,
  TrainingLoop:SupervisedLearning
)

// Example usage
[CIFAR10_Dataset â‹‰ (Images, Labels) |>
 ConvolutionalNN(
   Layers:[
     Convolutional(Filters:32, KernelSize:3x3, Activation:ReLU),
     Pooling(Type:MaxPooling2D, PoolSize:2x2),
     FullyConnected(Units:128, Activation:ReLU),
     FullyConnected(Units:10, Activation:Softmax)
   ],
   Optimizers:Adam(LearningRate:0.001),
   LossFunction:CategoricalCrossEntropy,
   ðŸ§©:(BatchSize:64, Epochs:20)
 ) |>
 TrainingLoop $ TrainedModel{Accuracy:0.85} + PerformanceHistory{Plots}]