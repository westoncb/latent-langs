CONCEPT Perceptron[InputDim] {
  LANGUAGE {
    TYPE â„[N] = Vector[â„, N]
    TYPE Label = â„
    TYPE Example = (â„[InputDim], Label)
    TYPE Weights = â„[InputDim]
    FUNC Activation(z) = IF z â‰¥ 0 THEN +1 ELSE -1
    FUNC Classify(w, x) = Activation(DotProduct(w, x))
    PRED Mistakes(w, X) = ğšº_{(x,y) âˆˆ X} ğŸ™[Classify(w, x) â‰  y]
    PRED LinearSeparable(X) = âˆƒ (w : Weights) . âˆ€ ((x,y) : Example) âˆˆ X . DotProduct(w, x) Â· y > 0
  }
  
  NOTATION {
    "DotProduct(w,x)" â‡” "ğšº_i w_i Â· x_i"
    "w <- w + y Â· x" â‡” "âˆ€ i . w_i := w_i + y Â· x_i"
    "X ~ ğ’Ÿ^m" â‡” "X = Sample of m iid examples from distribution ğ’Ÿ"
    "R[X]" â‡” "ğ”¼_{(x,y) ~ ğ’Ÿ} [ğŸ™[Classify(w, x) â‰  y]]"  // True risk 
    "RÌ‚[w, X]" â‡” "Mistakes(w, X) / |X|"  // Empirical risk
  }
  
  TRANFORMERS {
    PROC Train(X) -> Weights {
      VAR w = [0, ..., 0]  // Initialize weights to zero
      WHILE âˆƒ ((x,y) : Example) âˆˆ X . Classify(w, x) â‰  y:
        LET (xÌ‚, Å·) = (x,y) âˆˆ X . Classify(w, x) â‰  y
        w <- w + Å· Â· xÌ‚     // Update weights
      RETURN w
    }
  }

  PROOFS {
    THEOREM Convergence {
      STATEMENT:
        âˆ€ (X : Set[Example]) . LinearSeparable(X) => 
          âˆƒ (w : Weights) (T â‰¤ (max_{(x,_) âˆˆ X} |x|)Â²) . 
            Train(X) returns w after at most T updates âˆ§ Mistakes(w, X) = 0

      PROOF:
        LET X : Set[Example], 
            M = max_{(x,_) âˆˆ X} |x|,
            Î³ = min_w min_{(x,y) âˆˆ X} y Â· DotProduct(w, x) / (|w| Â· M)
        
        ASSUME LinearSeparable(X), Åµ = argmin_w |w| . âˆ€ ((x,y) : Example) âˆˆ X . DotProduct(Åµ, x) Â· y > 0
        
        LET (xÌ‚, Å·) âˆˆ X . Classify(w, xÌ‚) â‰  Å·
        
        DotProduct(Åµ, w_{t+1})  
          = DotProduct(Åµ, w_t) + Å· Â· DotProduct(Åµ, xÌ‚)
          â‰¥ DotProduct(Åµ, w_t) + Î³ Â· |Åµ| Â· M         by defn of Î³
        
        |w_{t+1}|Â² 
          = |w_t|Â² + 2Å· Â· DotProduct(w_t, xÌ‚) + |xÌ‚|Â²
          â‰¤ |w_t|Â² + |xÌ‚|Â²                            since Classify(w_t, xÌ‚) â‰  Å·
          â‰¤ |w_t|Â² + MÂ²                               by defn of M
        
        HENCE after T updates:
          DotProduct(Åµ, w_T) â‰¥ T Â· Î³ Â· |Åµ| Â· M
          |w_T|Â² â‰¤ T Â· MÂ²  
        
        THEREFORE
          T Â· Î³Â² Â· |Åµ|Â² Â· MÂ² 
            â‰¤ DotProduct(Åµ, w_T)Â²
            â‰¤ |Åµ|Â² Â· |w_T|Â²      by Cauchy-Schwarz
            â‰¤ |Åµ|Â² Â· T Â· MÂ²
        HENCE T â‰¤ 1/Î³Â²
        
        FURTHERMORE, Mistakes(w_T, X) = 0, otherwise âˆƒ ((x,y) : Example) âˆˆ X . Classify(w_T, x) â‰  y,
          contradicting the termination condition of the algorithm.
          
        QED
    }
    
    THEOREM GeneralizationBound {
      STATEMENT:
        âˆ€ (Î´ > 0) . âˆ€ (X ~ ğ’Ÿ^m) . âˆ€ (w : Weights) . 
          Pr[|R[w] - RÌ‚[w, X]| > âˆš(log(2/Î´) / (2m))] â‰¤ Î´ 

      PROOF:
        FUNC Risk(w) = ğ”¼_X[RÌ‚[w,X]]
        
        LET m = |X|, S = {w | Mistakes(w, X) = 0}
        
        log |S|
          â‰¤ 2 Â· InputDim Â· log(e Â· m / InputDim)     by Sauer-Shelah Lemma
          = O(InputDim Â· log(m / InputDim))
        
        âˆ€ (w âˆˆ S) . Pr[|R[w] - RÌ‚[w, X]| > Îµ] 
          â‰¤ 2 Â· exp(-2mÎµÂ²)                            by Hoeffding's Inequality
          
        Pr[âˆƒ (w âˆˆ S) . |R[w] - RÌ‚[w, X]| > Îµ]
          â‰¤ |S| Â· 2 Â· exp(-2mÎµÂ²)                     by Union Bound
          â‰¤ 2 Â· exp(log |S| - 2mÎµÂ²)
          â‰¤ Î´                                       for Îµ = âˆš(log(2/Î´) + log |S|) / (2m)
                                                         = O(âˆš((InputDim Â· log(m/InputDim) + log(1/Î´)) / m))
        
        HENCE with probability â‰¥ 1-Î´, âˆ€ (w âˆˆ S) . |R[w] - RÌ‚[w, X]| â‰¤ O(âˆš((InputDim Â· log(m/InputDim) + log(1/Î´)) / m))
        
        QED  
    }
  }
}