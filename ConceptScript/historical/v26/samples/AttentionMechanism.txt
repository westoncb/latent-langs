CONCEPT AttentionMechanism {
  LANGUAGE {
    TYPE Vector[N] = List[â„] WHERE Length(Self) = N
    TYPE Matrix[M, N] = List[Vector[N]] WHERE Length(Self) = M
    TYPE AttentionLayer = (Query: Vector, Keys: Matrix, Values: Matrix) -> Vector
    TYPE Mask = Matrix

    FUNC Softmax(v: Vector): Vector
    FUNC Exp(v: Vector): Vector
    FUNC Normalize(v: Vector): Vector
    FUNC MatMul(A: Matrix[M, K], B: Matrix[K, N]): Matrix[M, N]
    FUNC Transpose(A: Matrix[M, N]): Matrix[N, M]
    FUNC Hadamard(A: Matrix[M, N], B: Matrix[M, N]): Matrix[M, N]
    FUNC Concat(v1: Vector[N], v2: Vector[M]): Vector[N+M]
    FUNC Mask(A: Matrix[M, N], mask: Mask[M, N]): Matrix[M, N]
  }

  NOTATION {
    Ïƒ(v) = Softmax(v)
    exp(v) = Exp(v)
    norm(v) = Normalize(v)
    A Â· B = MatMul(A, B)
    Aáµ€ = Transpose(A)
    A âŠ™ B = Hadamard(A, B)
    v1 ++ v2 = Concat(v1, v2)
    A â—¾ mask = Mask(A, mask)

    Î±(Q, K) = Ïƒ((Q Â· Káµ€) / âˆšDim(Q))
    Attention(Q, K, V) = Î±(Q, K) Â· V
    MultiHead(Q, K, V) = (head_1 ++ ... ++ head_h) Â· W_O
      WHERE head_i = Attention(Q Â· W_i_Q, K Â· W_i_K, V Â· W_i_V)
    MaskedAttention(Q, K, V, mask) = Î±(Q, K, mask) Â· V
    â„š = Vector  // Query vector space
    ğ•‚ = Vector  // Key vector space
    ğ• = Vector  // Value vector space
  }

  TRANSFORMERS {
    FUNC DotProductAttention(Q: â„š, K: ğ•‚, V: ğ•): ğ• = {
      LET d = Dim(Q)
      REQUIRE Dim(K) = d AND Dim(V) = d
      RETURN Ïƒ((Q Â· Káµ€) / âˆšd) Â· V
    }

    FUNC ScaledDotProductAttention(Q: Matrix, K: Matrix, V: Matrix): Matrix = {
      LET d = Dim(Q[0])
      REQUIRE âˆ€ (q: Q, k: K, v: V) . Dim(q) = Dim(k) = Dim(v) = d
      RETURN Ïƒ((Q Â· Káµ€) / âˆšd) Â· V
    }

    FUNC MultiHeadAttention(Q: Matrix, K: Matrix, V: Matrix, h: â„•): Matrix = {
      LET d = Dim(Q[0])
      REQUIRE âˆ€ (q: Q, k: K, v: V) . Dim(q) = Dim(k) = Dim(v) = d
      REQUIRE d % h = 0
      LET d_h = d / h

      LET heads = []
      FOR i = 1 TO h:
        LET W_i_Q, W_i_K, W_i_V = RandomMatrix(d, d_h)
        LET head_i = ScaledDotProductAttention(Q Â· W_i_Q, K Â· W_i_K, V Â· W_i_V)
        heads := Append(heads, head_i)

      RETURN (heads[1] ++ ... ++ heads[h]) Â· W_O
    }

    FUNC MaskedAttention(Q: Matrix, K: Matrix, V: Matrix, mask: Mask): Matrix = {
      REQUIRE Dim(mask) = (Dim(Q), Dim(K))
      LET scores = (Q Â· Káµ€) / âˆšDim(Q[0])
      LET masked_scores = scores â—¾ mask
      LET attention_weights = Ïƒ(masked_scores)
      RETURN attention_weights Â· V
    }
  }

  STRUCTURE TransformerBlock[InputDim, HiddenDim, NumHeads, OutputDim] {
    FUNC Apply(input: Matrix): Matrix = {
      LET Q, K, V = Linear(input, HiddenDim)
      LET attention_output = MultiHeadAttention(Q, K, V, NumHeads)
      LET intermediate_output = LayerNorm(Linear(attention_output, HiddenDim) + input)
      LET output = LayerNorm(Linear(intermediate_output, OutputDim) + intermediate_output)
      RETURN output
    }
  }

  PROOFS {
    THEOREM SoftmaxPreservesPositionInvariance {
      STATEMENT:
        âˆ€ (v: Vector, c: â„) . Ïƒ(v + [c, ..., c]) = Ïƒ(v)

      PROOF:
        LET v: Vector, c: â„

        HAVE exp(v + [c, ..., c]) = exp(v) Â· exp([c, ..., c])
          BY Properties of Exponentiation and Vector Addition

        LET k = exp(c)
        HAVE k > 0
          BY Properties of Exponentiation

        SHOW Ïƒ(v + [c, ..., c]) = Ïƒ(v):
          Ïƒ(v + [c, ..., c])
            = norm(exp(v + [c, ..., c]))      BY Definition of Ïƒ
            = norm(exp(v) Â· [k, ..., k])      BY Previous Result
            = norm(exp(v)) Â· k / k            BY Properties of Normalization
            = norm(exp(v)) Â· 1                BY Algebra
            = Ïƒ(v)                            BY Definition of Ïƒ

        QED
    }

    THEOREM AttentionIsInvariantToKeyScaling {
      STATEMENT:
        âˆ€ (Q: â„š, K: ğ•‚, V: ğ•, c: â„) . Attention(Q, cÂ·K, V) = Attention(Q, K, V)

      PROOF:
        LET Q: â„š, K: ğ•‚, V: ğ•, c: â„

        SHOW Attention(Q, cÂ·K, V) = Attention(Q, K, V):
          Attention(Q, cÂ·K, V)
            = Î±(Q, cÂ·K) Â· V                                 BY Definition of Attention
            = Ïƒ((Q Â· (cÂ·K)áµ€) / âˆšDim(Q)) Â· V                  BY Definition of Î±
            = Ïƒ(c Â· (Q Â· Káµ€) / âˆšDim(Q)) Â· V                  BY Properties of Transpose and Scalar Multiplication
            = Ïƒ((Q Â· Káµ€) / âˆšDim(Q) + log(c)) Â· V             BY Properties of Logarithm
            = Ïƒ((Q Â· Káµ€) / âˆšDim(Q)) Â· V                      BY SoftmaxPreservesPositionInvariance
            = Î±(Q, K) Â· V                                   BY Definition of Î±
            = Attention(Q, K, V)                            BY Definition of Attention

        QED
    }

    THEOREM AttentionIsInvariantToValueScaling {
      STATEMENT:
        âˆ€ (Q: â„š, K: ğ•‚, V: ğ•, c: â„) . Attention(Q, K, cÂ·V) = c Â· Attention(Q, K, V)

      PROOF:
        LET Q: â„š, K: ğ•‚, V: ğ•, c: â„

        Attention(Q, K, cÂ·V)
          = Î±(Q, K) Â· (cÂ·V)                 BY Definition of Attention
          = c Â· (Î±(Q, K) Â· V)               BY Properties of Scalar Multiplication and Matrix Multiplication
          = c Â· Attention(Q, K, V)          BY Definition of Attention

        QED
    }
  }
}