CONCEPT GenerativeAdversarialNetwork[InputDim, LatentDim] {
  LANGUAGE {
    TYPE â„[N] = Vector[â„, N]  // N-dimensional real vector
    TYPE ð’Ÿ[A] = Distribution[A]  // Distribution over type A
    TYPE Gen = â„[LatentDim] -> â„[InputDim]  // Generator network
    TYPE Dis = â„[InputDim] -> â„  // Discriminator network
    FUNC ð”¼[D: ð’Ÿ[A], f: A -> â„]: â„  // Expectation of f with respect to distribution D
    PRED Generates(G: Gen, D: Dis, X: ð’Ÿ[â„[InputDim]]) = ð’«(G(z)) â‰ˆ ð’«(X)
    PRED Distinguishes(D: Dis, X: ð’Ÿ[â„[InputDim]], G: Gen) = ð”¼[X, x => log D(x)] + ð”¼[ð’©(0,1), z => log (1 - D(G(z)))] is maximized
  }

  NOTATION {
    ð’±(D,X,G) = ð”¼[X, x => log D(x)] + ð”¼[ð’©(0,1), z => log (1 - D(G(z)))]  // Discriminator loss
    â„’(G,D) = ð”¼[ð’©(0,1), z => log (1 - D(G(z)))]  // Generator loss
    G(z) = G(z) where z ~ ð’©(0,1)  // Generator applied to latent variable z
    D(x) = D(x) where x ~ X  // Discriminator applied to real data x
    âˆ‡_Î¸(f) = Gradient of f with respect to parameters Î¸
  }

  TRANSFORMERS {
    TACTIC MaximizeDiscriminator(D: Dis, X: ð’Ÿ[â„[InputDim]], G: Gen) -> Dis {
      RETURN D - Î» Â· âˆ‡_D(ð’±(D,X,G)) for some learning rate Î»
    }

    TACTIC MinimizeGenerator(G: Gen, D: Dis) -> Gen {
      RETURN G - Î» Â· âˆ‡_G(â„’(G,D)) for some learning rate Î»
    }

    PROC TrainGAN(G: Gen, D: Dis, X: ð’Ÿ[â„[InputDim]], n_critics: â„•, n_iters: â„•) -> (Gen, Dis) {
      FOR i = 1 TO n_iters:
        FOR j = 1 TO n_critics:
          D := MaximizeDiscriminator(D, X, G)
        G := MinimizeGenerator(G, D)
      RETURN (G, D)
    }
  }

  STRUCTURE MultiLayerPerceptron[L: â„•, U: â„•] {
    PARAM weights: List[â„[U, U]]  // Layer weight matrices
    PARAM biases: List[â„[U]]      // Layer bias vectors

    FUNC Apply(x: â„[U]) -> â„[U] {
      VAR h = x
      FOR (W, b) IN Zip(weights, biases):
        h := ReLU(W Â· h + b)
      RETURN Sigmoid(Last(weights) Â· h + Last(biases))
    }

    REQUIRE Length(weights) = Length(biases) = L
  }

  STRUCTURE DCGAN[GeneratorLayers: â„•, GeneratorUnits: â„•, DiscriminatorLayers: â„•, DiscriminatorUnits: â„•] {
    LET Generator = MultiLayerPerceptron[GeneratorLayers, GeneratorUnits]
    LET Discriminator = MultiLayerPerceptron[DiscriminatorLayers, DiscriminatorUnits]
  }

  STRUCTURE WGAN[GeneratorLayers: â„•, GeneratorUnits: â„•, DiscriminatorLayers: â„•, DiscriminatorUnits: â„•] {
    LET Generator = MultiLayerPerceptron[GeneratorLayers, GeneratorUnits]
    LET Discriminator = MultiLayerPerceptron[DiscriminatorLayers, DiscriminatorUnits]

    REQUIRE âˆ€ (x: â„[InputDim]) . |Discriminator.Apply(x)| â‰¤ 1  // Lipschitz constraint
  }

  PROOFS {
    THEOREM GANConvergence {
      STATEMENT:
        âˆ€ (G: Gen) (D: Dis) (X: ð’Ÿ[â„[InputDim]]) .
          LET (G', D') = TrainGAN(G, D, X, n_critics, n_iters)
          n_critics -> âˆž âˆ§ n_iters -> âˆž
            => Generates(G', D', X) âˆ§ Distinguishes(D', X, G')

      PROOF:
        LET G: Gen, D: Dis, X: ð’Ÿ[â„[InputDim]]
        ASSUME (G', D') = TrainGAN(G, D, X, n_critics, n_iters),
               n_critics -> âˆž, n_iters -> âˆž

        SHOW Distinguishes(D', X, G'):
          ð’±(D',X,G')
            = ð”¼[X, x => log D'(x)] + ð”¼[ð’©(0,1), z => log (1 - D'(G'(z)))]  // Defn of ð’±
            â‰¥ ð’±(D,X,G) - Îµ for any Îµ > 0  // By MaximizeDiscriminator as n_critics -> âˆž
          HENCE Distinguishes(D', X, G')  // Defn of Distinguishes
            
        SHOW Generates(G', D', X):
          â„’(G',D')
            = ð”¼[ð’©(0,1), z => log (1 - D'(G'(z)))]  // Defn of â„’
            â‰¤ â„’(G,D') + Îµ for any Îµ > 0  // By MinimizeGenerator as n_iters -> âˆž
          ð’«(G'(z))
            = âˆ« ð’«(G'(z)|z) Â· ð’«(z) dz   // Law of Total Probability
            â‰ˆ âˆ« ð’«(x|z) Â· ð’«(z) dz       // As â„’(G',D') is minimized
            = ð’«(X)                     // Marginal Distribution
          HENCE Generates(G', D', X)   // Defn of Generates
                  
        QED
    }

    THEOREM WassersteinGANConvergence {
      STATEMENT:
        âˆ€ (X: ð’Ÿ[â„[InputDim]]) .
          LET G = WGAN[4,128,4,128].Generator, D = WGAN[4,128,4,128].Discriminator
          (G', D') = TrainGAN(G, D, X, n_critics, n_iters) âˆ§
          n_critics -> âˆž âˆ§ n_iters -> âˆž
            => W(ð’«(G'(z)), ð’«(X)) -> 0  // Wasserstein distance
            
      PROOF:
        LET G = WGAN[4,128,4,128].Generator, D = WGAN[4,128,4,128].Discriminator, X: ð’Ÿ[â„[InputDim]]
        ASSUME (G', D') = TrainGAN(G, D, X, n_critics, n_iters),
               n_critics -> âˆž, n_iters -> âˆž

        LET ð’Ÿ_G = ð’«(G'(z)), ð’Ÿ_X = ð’«(X)

        HAVE W(ð’Ÿ_G, ð’Ÿ_X) â‰¥ - ð’±(D,X,G) - Îµ for any Îµ > 0:
          W(ð’Ÿ_G, ð’Ÿ_X)
            = sup_{f: 1-Lipschitz} ð”¼[ð’Ÿ_G, x => f(x)] - ð”¼[ð’Ÿ_X, x => f(x)]  // Defn of W
            â‰¥ ð”¼[ð’Ÿ_G, x => D'(x)] - ð”¼[ð’Ÿ_X, x => D'(x)]  // D' is 1-Lipschitz
            = ð”¼[ð’©(0,1), z => D'(G'(z))] - ð”¼[X, x => D'(x)]
            = - ð’±(D',X,G')  // Defn of ð’±
            â‰¥ - (ð’±(D,X,G) + Îµ) for any Îµ > 0  // By MaximizeDiscriminator as n_critics -> âˆž

        HAVE W(ð’Ÿ_G, ð’Ÿ_X) â‰¤ - ð’±(D,X,G) + Îµ for any Îµ > 0:
          W(ð’Ÿ_G, ð’Ÿ_X)
            = sup_{f: 1-Lipschitz} ð”¼[ð’Ÿ_G, x => f(x)] - ð”¼[ð’Ÿ_X, x => f(x)]  // Defn of W
            â‰¤ ð”¼[ð’Ÿ_G, x => D(x)] - ð”¼[ð’Ÿ_X, x => D(x)] + Îµ for any Îµ > 0  // By MinimizeGenerator as n_iters -> âˆž 
            = - ð’±(D,X,G) + Îµ

        THEREFORE - ð’±(D,X,G) - Îµ â‰¤ W(ð’Ÿ_G, ð’Ÿ_X) â‰¤ - ð’±(D,X,G) + Îµ
        HENCE W(ð’Ÿ_G, ð’Ÿ_X) -> 0

        QED
    }
  }
}