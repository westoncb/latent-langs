CONCEPT NeuralNetwork[InputDim, OutputDim] {
  LANGUAGE {
    TYPE â„[N] = Vector[â„, N]
    TYPE â„[M,N] = Matrix[â„, M, N]
    TYPE Layer = â„[InputDim] -> â„[OutputDim]
    TYPE Model = InputEncoder -> List[Layer] -> OutputDecoder
    TYPE Loss = (â„[OutputDim], â„[OutputDim]) -> â„

    FUNC Ïƒ(x: â„): â„ = 1 / (1 + exp(-x))
    FUNC ReLU(x: â„): â„ = max(0, x)
    FUNC Softmax(v: â„[N]): â„[N] = exp(v) / sum(exp(v))
    FUNC ArgMax(v: â„[N]): â„• = Index of maximum element in v
    FUNC SquaredError(y: â„[N], Å·: â„[N]): â„ = ||y - Å·||^2
    FUNC CrossEntropy(y: â„[N], Å·: â„[N]): â„ = -sum(y * log(Å·))

    AXIOM DimMatch {
      âˆ€ (L: Layer) (x: â„[InputDim]) . Dim(L(x)) = OutputDim
    }
  }

  NOTATION {
    âˆ€ x âˆˆ ğ’³ . P(x) â‡” âˆ€ (x: ğ’³) . P(x)
    âˆƒ x âˆˆ ğ’³ . P(x) â‡” âˆƒ (x: ğ’³) . P(x)
    x âˆ¼ ğ’Ÿ â‡” Sample(x, ğ’Ÿ)
    ğ”¼[xâˆ¼ğ’Ÿ] f(x) â‡” Expectation(f, ğ’Ÿ)
    f âˆ˜ g â‡” Compose(f, g)
    fâ»Â¹ â‡” Inverse(f)
    âˆ‡f â‡” Gradient(f)
  }

  TRANSFORMERS {
    FUNC Linear(W: â„[InputDim, OutputDim], b: â„[OutputDim]): Layer = {
      Î» (x: â„[InputDim]) . W Â· x + b
    }

    FUNC Sequential(layers: List[Layer]): Layer = {
      MATCH layers WITH
      | [] -> Identity
      | [L, ...Ls] -> Sequential(Ls) âˆ˜ L
    }

    TACTIC GradDescent(model: Model, X: List[â„[InputDim]], Y: List[â„[OutputDim]], Î·: â„, epochs: â„•) -> Model = {
      FOR i = 1 TO epochs:
        FOR (x, y) IN Zip(X, Y):
          LET Å· = model(x)
          LET âˆ‡loss = âˆ‡(Î» model . Loss(model(x), y))
          model := model - Î· * âˆ‡loss(model)
      RETURN model
    }
  }

  STRUCTURE FeedForwardNet {
    PARAM InputEncoder: â„[InputDim] -> â„[H]
    PARAM OutputDecoder: â„[H] -> â„[OutputDim]
    PARAM Layers: List[â„[H] -> â„[H]]
    PARAM H: â„•

    FUNC Forward(x: â„[InputDim]): â„[OutputDim] = {
      LET h = InputEncoder(x)
      FOR L IN Layers:
        h := L(h)
      RETURN OutputDecoder(h)
    }

    FUNC Train(X: List[â„[InputDim]], Y: List[â„[OutputDim]], Î·: â„, epochs: â„•): FeedForwardNet = {
      GradDescent(Forward, X, Y, Î·, epochs)
    }
  }

  STRUCTURE RecurrentNet {
    PARAM InputEncoder: â„[InputDim] -> â„[H]
    PARAM OutputDecoder: â„[H] -> â„[OutputDim]
    PARAM CellFunction: (â„[H], â„[H]) -> â„[H]
    PARAM H: â„•

    FUNC Forward(x: List[â„[InputDim]]): List[â„[OutputDim]] = {
      VAR h = Zeros(H)
      VAR outputs = []
      FOR x_t IN x:
        h := CellFunction(InputEncoder(x_t), h)
        outputs := outputs + [OutputDecoder(h)]
      RETURN outputs
    }

    FUNC Train(X: List[List[â„[InputDim]]], Y: List[List[â„[OutputDim]]], Î·: â„, epochs: â„•): RecurrentNet = {
      GradDescent(Î» model . FlatMap(model âˆ˜ Forward, X), FlatMap(Identity, Y), Î·, epochs)
    }
  }

  PROOFS {
    THEOREM UniversalApproximation {
      STATEMENT:
        âˆ€ (ğ’³: Set[â„[InputDim]]) (f: ğ’³ -> â„[OutputDim]) (Îµ: â„) (Ïƒ: â„ -> â„) .
          Ïƒ is non-polynomial continuous function =>
            âˆƒ (model: FeedForwardNet) .
              âˆ€ x âˆˆ ğ’³ . ||model(x) - f(x)|| < Îµ

      PROOF:
        LET ğ’³: Set[â„[InputDim]], f: ğ’³ -> â„[OutputDim], Îµ: â„, Ïƒ: â„ -> â„
        ASSUME Ïƒ is non-polynomial continuous function

        DEFINE ApproxNet(n: â„•): FeedForwardNet {
          LET H = n
          LET InputEncoder(x) = [Ïƒ(wÂ·x + b) for w âˆˆ ğ’², b âˆˆ ğ’·]
            where ğ’²: Set[â„[InputDim]] and ğ’·: Set[â„] are randomly initialized
          LET Layers = []
          LET OutputDecoder(h) = vÂ·h + c
            where v: â„[H] and c: â„[OutputDim] are optimized by GradDescent
        }

        CHOOSE n: â„• sufficiently large
        LET model = ApproxNet(n)
        HAVE âˆ€ x âˆˆ ğ’³ . ||model(x) - f(x)|| < Îµ  by Universal Approximation Theorem for FeedForwardNets

        QED
    }

    THEOREM GeneralizationBound {
      STATEMENT:
        âˆ€ (X: List[â„[InputDim]]) (Y: List[â„[OutputDim]]) (m: â„•) (Î´: â„) (model: Model) .
          m = Length(X) âˆ§ (0 < Î´ < 1) =>
            ğ”¼[xâˆ¼ğ’³, yâˆ¼ğ’´] Loss(model(x), y) â‰¤
              (1/m) * Î£áµ¢ Loss(model(Xáµ¢), Yáµ¢) + ğ’ª(âˆš(Log(1/Î´) / m))

      PROOF:
        LET X: List[â„[InputDim]], Y: List[â„[OutputDim]], m: â„•, Î´: â„, model: Model
        ASSUME m = Length(X) âˆ§ (0 < Î´ < 1)

        DEFINE EmpiricalRisk(model) = (1/m) * Î£áµ¢ Loss(model(Xáµ¢), Yáµ¢)
        DEFINE TrueRisk(model) = ğ”¼[xâˆ¼ğ’³, yâˆ¼ğ’´] Loss(model(x), y)

        LET ğ’¢ = {model | model: Model}  // Hypothesis class
        LET ğ’(ğ’¢) = Covering number of ğ’¢

        HAVE âˆ€ (Î´: â„) . â„™(sup_{model âˆˆ ğ’¢} |EmpiricalRisk(model) - TrueRisk(model)| > Îµ) â‰¤ ğ’(ğ’¢) Â· exp(-2mÎµ^2)
          by Uniform Convergence Bound

        LET Îµ = âˆš(Log(ğ’(ğ’¢)/Î´) / 2m)
        HENCE â„™(sup_{model âˆˆ ğ’¢} |EmpiricalRisk(model) - TrueRisk(model)| > Îµ) â‰¤ Î´

        THEREFORE, with probability at least 1 - Î´:
          TrueRisk(model) â‰¤ EmpiricalRisk(model) + ğ’ª(âˆš(Log(ğ’(ğ’¢)/Î´) / m))
        
        QED
    }

    THEOREM StableRecurrence {
      STATEMENT:
        âˆ€ (Cell: (â„[H], â„[H]) -> â„[H]) (x: List[â„[InputDim]]) .
          ||âˆ‡Cell|| â‰¤ 1 =>
            âˆƒ (K: â„) . âˆ€ (t: â„•) . ||h_t|| â‰¤ K * max_{iâ‰¤t} ||x_i||
            where h_t = Recurrent(Cell, InputEncoder)(x)[t]

      PROOF:
        LET Cell: (â„[H], â„[H]) -> â„[H], x: List[â„[InputDim]]
        ASSUME ||âˆ‡Cell|| â‰¤ 1

        LET h_t = Recurrent(Cell, InputEncoder)(x)[t]
        LET x_t = InputEncoder(x[t])

        HAVE ||h_t|| â‰¤ ||Cell(h_{t-1}, x_t)|| â‰¤ ||h_{t-1}|| + ||x_t||
          by Lipschitz continuity of Cell and triangle inequality

        HENCE ||h_t|| â‰¤ Î£_{iâ‰¤t} ||x_i||
          by Unrolling the recurrence and telescoping sum

        LET K = 1
        THEREFORE ||h_t|| â‰¤ K * max_{iâ‰¤t} ||x_i||

        QED
    }
  }
}