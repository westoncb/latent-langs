CONCEPT VariationalAutoEncoder {
  LANGUAGE {
    TYPE â„[N] = Tensor[N]
    TYPE Enc[D,Z] = â„[D] -> (â„[Z], â„[Z])
    TYPE Dec[Z,D] = â„[Z] -> â„[D]
    FUNC KL(p: Distribution, q: Distribution): â„
    FUNC ð”¼[p: Distribution, f: A -> â„]: â„
    FUNC LL(x: â„[D], z: â„[Z]): â„
    PRED GradientDescent(loss: (Enc[D,Z], Dec[Z,D]) -> â„, param: (Enc[D,Z], Dec[Z,D]), lr: â„, steps: â„•) -> (Enc[D,Z], Dec[Z,D])
  }

  NOTATION {
    Î¼_q[x] = fst(Enc(q)[x])  // Encoding mean
    Ïƒ_q[x] = exp(snd(Enc(q)[x]))  // Encoding std dev
    ð’©(Î¼,Ïƒ) = NormalDistribution(Î¼, Ïƒ)
    z ~ ð’©(Î¼,Ïƒ) = z = Î¼ + Ïƒ âŠ™ ð’©(0,1)  // Reparameterization
    ELBO(x, E, D) = ð”¼[z ~ ð’©(Î¼_E[x],Ïƒ_E[x]), LL(x, D(z))] - KL(ð’©(Î¼_E[x],Ïƒ_E[x]), ð’©(0,1))
    L(x, E, D) = - ELBO(x, E, D)  // VAE loss
    J(E,D,X) = ð”¼[x ~ X, L(x, E, D)]  // Cost functional
  }

  TRANSFORMERS {
    PROC TrainVAE(E: Enc[D,Z], D: Dec[Z,D], X: ð’Ÿ[â„[D]], lr: â„, steps: â„•) -> (Enc[D,Z], Dec[Z,D]) {
      REWRITE (E, D) = GradientDescent(Î»(E,D) . J(E,D,X), (E, D), lr, steps)
    }

    FUNC Reconstruct(x: â„[D], E: Enc[D,Z], D: Dec[Z,D]): â„[D] = D(z ~ ð’©(Î¼_E[x], Ïƒ_E[x]))

    FUNC Sample(n: â„•, D: Dec[Z,D]): List[â„[D]] = (D(z) for z ~ ð’©(0,1), i in [1..n])

    FUNC Marginal(X: ð’Ÿ[â„[D]], E: Enc[D,Z], D: Dec[Z,D]): â„ = ð”¼[x ~ X, LL(x, Reconstruct(x, E, D))]
  }

  STRUCTURE FullyConnectedVAE[D: â„•, Z: â„•, H: â„•] EXTENDS VariationalAutoEncoder {
    PARAM LinearEnc1: â„[D*H]
    PARAM LinearEnc2: â„[H*Z] 
    PARAM LinearEnc3: â„[H*Z]
    PARAM LinearDec1: â„[Z*H]
    PARAM LinearDec2: â„[H*D]

    FUNC Enc(x: â„[D]) -> (â„[Z], â„[Z]) {
      LET h = ReLU(LinearEnc1 Â· x)
      (LinearEnc2 Â· h, LinearEnc3 Â· h)
    }

    FUNC Dec(z: â„[Z]) -> â„[D] = Sigmoid(LinearDec2 Â· ReLU(LinearDec1 Â· z))
  }

  PROOFS {
    THEOREM ELBO_Bounds_LogLikelihood {
      STATEMENT:
        âˆ€ (E: Enc[D,Z]) (D: Dec[Z,D]) (p: â„[D]) .
          LL(p, D(z ~ ð’©(Î¼_E[p], Ïƒ_E[p]))) â‰¥ ELBO(p, E, D)
          
      PROOF:
        LET E: Enc[D,Z], D: Dec[Z,D], p: â„[D]

        log ð’«(p)
          = log âˆ« ð’«(p|z) Â· ð’«(z) dz                      BY MarginalizationOfLatents
          = log ð”¼[z ~ ð’«(z), ð’«(p|z)]                     BY ExpectationAsIntegral  
          â‰¥ ð”¼[z ~ ð’«(z), log ð’«(p|z)]                     BY JensensInequality
          = ð”¼[z ~ ð’©(Î¼_E[p],Ïƒ_E[p]), log ð’«(p|z)] 
              - KL(ð’©(Î¼_E[p],Ïƒ_E[p]), ð’«(z))               BY KLDivergenceIdentity
          = ELBO(p, E, D)                               BY ELBO

        HENCE LL(p, D(z ~ ð’©(Î¼_E[p], Ïƒ_E[p]))) â‰¥ ELBO(p, E, D)
        QED
    }

    THEOREM VAE_Minimizes_KL_Divergence_To_Posterior {
      STATEMENT:  
        âˆ€ (E: Enc[D,Z]) (D: Dec[Z,D]) (X: ð’Ÿ[â„[D]]) .
          (E', D') = GradientDescent(J(E, D, X), (E, D), lr, steps) =>
            ð”¼[p ~ X, KL(ð’©(Î¼_E'[p],Ïƒ_E'[p]), ð’«(z|p))] is minimized
            
      PROOF:
        LET E: Enc[D,Z], D: Dec[Z,D], X: ð’Ÿ[â„[D]]
        ASSUME (E', D') = GradientDescent(J(E, D, X), (E, D), lr, steps)

        J(E, D, X)
          = ð”¼[p ~ X, L(p, E, D)]                        BY J
          = ð”¼[p ~ X, - ELBO(p, E, D)]                   BY L
          = ð”¼[p ~ X, KL(ð’©(Î¼_E[p],Ïƒ_E[p]), ð’«(z|p))
              - ð”¼[z ~ ð’©(Î¼_E[p],Ïƒ_E[p]), LL(p, D(z))]]    BY ELBO

        HENCE minimizing J(E, D, X) wrt (E, D) is equivalent to
              minimizing ð”¼[p ~ X, KL(ð’©(Î¼_E[p],Ïƒ_E[p]), ð’«(z|p))]
                and maximizing ð”¼[p ~ X, ð”¼[z ~ ð’©(Î¼_E[p],Ïƒ_E[p]), LL(p, D(z))]]

        THEREFORE, (E', D') = GradientDescent(J(E, D, X), (E, D), lr, steps)
          => ð”¼[p ~ X, KL(ð’©(Î¼_E'[p],Ïƒ_E'[p]), ð’«(z|p))] is minimized 
        QED
    }

    THEOREM VAE_Generates_Data {
      STATEMENT:
        âˆ€ (E: Enc[D,Z]) (D: Dec[Z,D]) (X: ð’Ÿ[â„[D]]) .
          ð”¼[p ~ X, KL(ð’©(Î¼_E[p],Ïƒ_E[p]), ð’«(z|p))] â‰ˆ 0 =>
            ð’«(Sample(D)) â‰ˆ ð’«(X)
            
      PROOF:
        LET E: Enc[D,Z], D: Dec[Z,D], X: ð’Ÿ[â„[D]]
        ASSUME ð”¼[p ~ X, KL(ð’©(Î¼_E[p],Ïƒ_E[p]), ð’«(z|p))] â‰ˆ 0

        ð’«(Sample(D))
          = âˆ« ð’«(x|z) Â· ð’«(z) dz                           BY MarginalizationOfLatents
          â‰ˆ âˆ« ð’«(x|z) Â· ð’«(z|p) Â· ð’«(p) dp dz               BY TotalProbability, dom(p) = X
          = âˆ« ð’«(p) Â· âˆ« ð’«(x|z) Â· ð’«(z|p) dz dp             BY Fubini
          â‰ˆ âˆ« ð’«(p) Â· ð’«(x|p) dp                           BY AssumedOptimality
          = ð’«(X)                                         BY MarginalDistribution

        HENCE ð’«(Sample(D)) â‰ˆ ð’«(X)
        QED
    }

    THEOREM VAE_Convergence {
      STATEMENT:
        âˆ€ (E: Enc[D,Z]) (D: Dec[Z,D]) (X: ð’Ÿ[â„[D]]) .
          |X| -> âˆž  âˆ§  (E,D) = TrainVAE(E, D, X, lr, steps)
            => âˆ€ (p: â„[D]) .
                 ð’©(Î¼_E[p],Ïƒ_E[p]) -> ð’«(z|p) âˆ§ Marginal(X, E, D) -> ð’«(X)
                 
      PROOF:
        LET E: Enc[D,Z], D: Dec[Z,D], X: ð’Ÿ[â„[D]]  
        ASSUME |X| -> âˆž, (E,D) = TrainVAE(E, D, X, lr, steps)

        âˆ€ (p: â„[D]) . ð’©(Î¼_E[p],Ïƒ_E[p]) -> ð’«(z|p)
          BY VAE_Minimizes_KL_Divergence_To_Posterior

        Marginal(X, E, D)
          = ð”¼[p ~ X, LL(p, Reconstruct(p, E, D))]         BY Marginal
          = ð”¼[p ~ X, log ð’«(p | z ~ ð’©(Î¼_E[p],Ïƒ_E[p]))]     BY Reconstruct, LL
          -> ð”¼[p ~ X, log ð’«(p)]                           BY AssumedOptimality,
                                                             VAE_Generates_Data  
          = H(X)                                          BY Entropy

        HENCE âˆ€ (p: â„[D]) .
          ð’©(Î¼_E[p],Ïƒ_E[p]) -> ð’«(z|p) âˆ§ Marginal(X, E, D) -> ð’«(X)
        QED
    }
  }  
}







CONCEPT VariationalAutoEncoder {
  LANGUAGE {
    TYPE R = Real; â„[N] = Tensor[N]
    TYPE Enc[D,Z] = â„[D] -> (â„[Z],â„[Z]); Dec[Z,D] = â„[Z] -> â„[D]
    FUNC KL(p,q), ð”¼_p(f), LL(x,z), GD(loss,param,lr,step)
  }
  
  NOTATION {
    "(Î¼,Ïƒ)_q[p]" â‡” "(Enc(q)[p], exp(Enc(q)[p+1]))"
    "z_q[p]" â‡” "Î¼_q[p] + Ïƒ_q[p] âŠ™ ð’©(0,1)"
    "x_q[p]" â‡” "Dec(z_q[p])"
    "L(q,p)" â‡” "-ð”¼_q[(Î¼,Ïƒ)_q[p],z_q[p]][LL(p,x_q[p])] + KL((Î¼,Ïƒ)_q[p],ð’©(0,1))"
    "Loss(E,D,X)" â‡” "mean(L(x,x) for x in X)"
    "IsEnc(E)" â‡” "âˆƒD,Z. E : Enc[D,Z]"; "IsDec(D)" â‡” "âˆƒZ,D. D : Dec[Z,D]"
  }
  
  TRANSFORMERS {
    TACTIC TrainVAE(E,D,X,lr,step) -> (E,D) = GD(Î»(E,D).Loss(E,D,X), (E,D), lr, step)
    FUNC Reconstruct(q,E,D) = (x_q[q] for _ in [1])
    FUNC Sample(n,E,D) = (x_z for _ in [1..n], z~ð’©(0,1))
    FUNC Marginal(X,E,D) = mean(LL(x,Reconstruct(x,E,D)) for x in X)
  }
  
  STRUCTURE FullyConnectedVAE[D,Z,H] {
    PARAM Î¸=(W1,b1,W2,b2,W3,b3,W4,b4,W5,b5)
    FUNC Enc = Î»p. Let h=ReLU(W1Â·p+b1) in (W2Â·h+b2, W3Â·h+b3)
    FUNC Dec = Î»z. Sigmoid(W4Â·ReLU(W5Â·z+b5)+b4)
    FUNC Train(X,lr,step) = TrainVAE(Enc,Dec,X,lr,step)
  }
  
  PROOFS {
    THEOREM ELBO_is_VAE_Loss {
      âˆ€E,D,q. IsEnc(E)âˆ§IsDec(D) => L(q,q) = -ELBO(q,E,D)
      PROOF: 
        Let E,D,q. Assume IsEnc(E), IsDec(D). 
        Let (Î¼,Ïƒ)=(Î¼,Ïƒ)_q[q], z=z_q[q], x=x_q[q].
        
        L(q,q) 
        â¤‡ "-ð”¼[(Î¼,Ïƒ),z][LL(q,x)] + KL((Î¼,Ïƒ),ð’©(0,1))" [by L]
        = "-âˆ« ð’©(z;Î¼,Ïƒ)Â·LL(q,Dec(z))Â·dz + âˆ« ð’©(z;Î¼,Ïƒ)Â·log(ð’©(z;Î¼,Ïƒ)/ð’©(z;0,1))Â·dz"
        = "-âˆ« ð’©(z;Î¼,Ïƒ)Â·log(ð’«(q,z)/ð’©(z;Î¼,Ïƒ))Â·dz + âˆ« ð’©(z;Î¼,Ïƒ)Â·log(ð’©(z;Î¼,Ïƒ)/ð’©(z;0,1))Â·dz"
        â¤‡ "-ð”¼[z~ð’©(Î¼,Ïƒ)][log(ð’«(q,z)/ð’©(z;Î¼,Ïƒ))] + ð”¼[z~ð’©(Î¼,Ïƒ)][log(ð’©(z;Î¼,Ïƒ)/ð’©(z;0,1))]"
        = "-ð”¼[z~ð’©(Î¼,Ïƒ)][log(ð’«(q,z))] + ð”¼[z~ð’©(Î¼,Ïƒ)][log(ð’©(z;Î¼,Ïƒ))] - ð”¼[z~ð’©(Î¼,Ïƒ)][log(ð’©(z;0,1))]"
        = "-(ð”¼[z~ð’©(Î¼,Ïƒ)][log(ð’«(q,z))] - KL(ð’©(Î¼,Ïƒ),ð’©(0,1)))"
        â¤‡ "-ELBO(q,E,D)". QED.
    }

    THEOREM VAE_Learns_Posterior {
      âˆ€D,Z,X. |X|->âˆž => Let (E,D)=TrainVAE(E,D,X) in
        âˆ€qâˆˆX. KL((Î¼,Ïƒ)_q[q],ð’«(z|q)) is minimized
      PROOF:
        Let D,Z,X. Assume |X|->âˆž.
        Let (E,D) = TrainVAE(E,D,X).
        Let qâˆˆX. Suffices to show KL((Î¼,Ïƒ)_q[q],ð’«(z|q)) is minimized.

        TrainVAE minimizes Loss(E,D,X) 
        â‡’ âˆ€qâˆˆX. TrainVAE minimizes L(q,q) [by Loss]
        â¤‡ âˆ€qâˆˆX. TrainVAE minimizes "-ð”¼[z~(Î¼,Ïƒ)_q[q]][LL(q,x_q[q])] 
                                    + KL((Î¼,Ïƒ)_q[q],ð’©(0,1))" [by L]
        = âˆ€qâˆˆX. TrainVAE minimizes "-âˆ«(Î¼,Ïƒ)_q[q](z)Â·log(ð’«(q|z)Â·ð’«(z)/ð’©(z;0,1))Â·dz 
                                    + KL((Î¼,Ïƒ)_q[q],ð’©(0,1))"
        â‡’ âˆ€qâˆˆX. TrainVAE minimizes "-âˆ«(Î¼,Ïƒ)_q[q](z)Â·log(ð’«(z|q))Â·dz - log(ð’«(q))
                                    + KL((Î¼,Ïƒ)_q[q],ð’©(0,1))" [by Bayes Rule]
        â¤‡ âˆ€qâˆˆX. TrainVAE minimizes "KL((Î¼,Ïƒ)_q[q],ð’«(z|q)) - log(ð’«(q))
                                    + KL((Î¼,Ïƒ)_q[q],ð’©(0,1))"
        â‡’ âˆ€qâˆˆX. TrainVAE minimizes KL((Î¼,Ïƒ)_q[q],ð’«(z|q)) 
          [since -log(ð’«(q))+KL((Î¼,Ïƒ)_q[q],ð’©(0,1)) is constant wrt (Î¼,Ïƒ)_q[q]]
      }

    THEOREM VAE_Generates_Data {
      âˆ€D,Z,X. |X|->âˆž => Let (E,D)=TrainVAE(E,D,X) in Sample(E,D) ~ ð’«(X)
      PROOF:
        Let D,Z,X. Assume |X|->âˆž.
        Let (E,D) = TrainVAE(E,D,X).
        Suffices to show Sample(E,D) ~ ð’«(X)

        âˆ€qâˆˆX. (Î¼,Ïƒ)_q[q] â‰ˆ ð’«(z|q), by VAE_Learns_Posterior.
        âˆ€qâˆˆX. Reconstruct(q,E,D) â‰ˆ q
          [since TrainVAE maximizes ð”¼[(Î¼,Ïƒ)_q[q],z_q[q]][LL(q,x_q[q])] by ELBO_is_VAE_Loss]
        â‡’ âˆ€x. ð’«(x|Sample(E,D)) 
           â‰ˆ âˆ« ð’«(x|z)Â·ð’«(z)Â·dz [since Sample(E,D) ~ ð’©(0,1)]
           â‰ˆ âˆ« ð’«(x|z)Â·ð’«(z|q)Â·ð’«(q)Â·dqÂ·dz, for any qâˆˆX [by Total Probability]
           = âˆ« ð’«(q)Â·âˆ« ð’«(x|z)Â·ð’«(z|q)Â·dzÂ·dq
           â‰ˆ âˆ« ð’«(q)Â·ð’«(x|q)Â·dq [since Reconstruct(q,E,D)â‰ˆq]
           = ð’«(x)
        â‡’ Sample(E,D) ~ ð’«(X).
      }
      
    THEOREM VAE_Converges {  
      âˆ€D,Z,X. |X|->âˆž => Let (E,D)=TrainVAE(E,D,X) in
        âˆ€q. (Î¼,Ïƒ)_q[q] -> ð’«(z|q) âˆ§ Marginal(X,E,D) -> ð’«(X)
      PROOF:
        Let D,Z,X. Assume |X|->âˆž.
        Let (E,D) = TrainVAE(E,D,X).
        Let q.
        (Î¼,Ïƒ)_q[q] -> ð’«(z|q), by VAE_Learns_Posterior.
        Marginal(X,E,D) 
        = "âˆ« ð’«(x|z)Â·ð’«(z)Â·dz" [by Marginal]
        â‰ˆ "âˆ«âˆ« ð’«(x|z)Â·ð’«(z|q)Â·ð’«(q)Â·dqÂ·dz" [since Sample(E,D)~ð’«(X) by VAE_Generates_Data]
        = "âˆ« ð’«(q)Â·âˆ« ð’«(x|z)Â·ð’«(z|q)Â·dzÂ·dq"  
        â‰ˆ "âˆ« ð’«(q)Â·ð’«(x|q)Â·dq" [since Reconstruct(q,E,D)â‰ˆq]
        = "ð’«(x)"
        â‡’ Marginal(X,E,D) -> ð’«(X).
      }
  }
}





CONCEPT VAE {
  LANGUAGE {
    TYPE Tensor[N]
    TYPE Vector = Tensor[1] 
    TYPE Encoder = Vector -> (Vector, Vector)
    TYPE Decoder = Vector -> Vector
    
    FUNC KL(p q : Vector -> Real) : Real
    FUNC E(p : Vector -> Real, f: Vector -> Real) : Real
    FUNC H(p : Vector -> Real) : Real
  }
  
  NOTATION {
    "ð’©(Î¼, Ïƒ)" â‰” LAMBDA(z) . Exp(-(z - Î¼)^2 / (2 * Ïƒ^2)) / Sqrt(2 * Ï€ * Ïƒ^2)
    "p(z)" â‰” ð’©(0, 1)
    "q(z|x)" â‰” LAMBDA(z) . ð’©(Î¼(x), Ïƒ(x))(z) 
    "KL[q(z|x) | p(z)]" â‰” KL(q(z|x), p(z))
    "E_q[f(z)]" â‰” E(q(z|x), f)
    "H[q(z|x)]" â‰” H(q(z|x))
  }
  
  TRANSFORMERS {
    FUNC vae_loss(enc : Encoder, dec : Decoder, x : Vector) -> Real â‰”
      LET (Î¼, Ïƒ) = enc(x)
      IN -E_q[Log(p(x|z))] + KL[q(z|x) | p(z)] 
         WHERE Log(p(x|z)) â‰” -||x - dec(z)||^2 / 2
         
    PROC train_vae(enc : Encoder, dec : Decoder, data : List[Vector], 
                   steps : Nat, lr : Real) -> (Encoder, Decoder) â‰”
      INIT Î¸_enc, Î¸_dec
      FOR i in 1..steps:
        INIT âˆ‡Î¸_enc, âˆ‡Î¸_dec  
        FOR x in data:
          âˆ‡Î¸_enc += âˆ‡(Î¸_enc -> vae_loss(enc, dec, x))
          âˆ‡Î¸_dec += âˆ‡(Î¸_dec -> vae_loss(enc, dec, x))
        Î¸_enc -= lr * âˆ‡Î¸_enc / |data|
        Î¸_dec -= lr * âˆ‡Î¸_dec / |data|
      RETURN (LAMBDA(x) . enc_Î¸(x), LAMBDA(z) . dec_Î¸(z))
  }
  
  STRUCTURE FullyConnectedVAE {
    REQUIRE (d::Nat, z::Nat, h::Nat)
    
    LET enc_params = (W_Î¼ : Tensor[h, d], b_Î¼ : Tensor[h], 
                      W_Ïƒ : Tensor[h, d], b_Ïƒ : Tensor[h],
                      V_Î¼ : Tensor[z, h], c_Î¼ : Tensor[z],  
                      V_Ïƒ : Tensor[z, h], c_Ïƒ : Tensor[z])

    DEF encoder(x : Vector) -> (Vector, Vector) â‰”
      LET h = ReLU(W_Î¼ Â· x + b_Î¼)  
      IN (V_Î¼ Â· h + c_Î¼, Exp(V_Ïƒ Â· h + c_Ïƒ))
        
    LET dec_params = (W : Tensor[h, z], b : Tensor[h],
                      V : Tensor[d, h], c : Tensor[d])

    DEF decoder(z : Vector) -> Vector â‰”
      LET h = ReLU(W Â· z + b)
      IN Sigmoid(V Â· h + c)
  }

  PROOFS {
    THEOREM VaeLossIsElbo â‰”
      FORALL (enc : Encoder, dec : Decoder, x : Vector) . 
        vae_loss(enc, dec, x) = -ELBO(enc, dec, x)
      WHERE ELBO(enc, dec, x) â‰” E_q[Log(p(x, z)) - Log(q(z|x))]
    
    PROOF â‰”
      LET (Î¼, Ïƒ) = enc(x)
      REWRITE E_q[Log(p(x, z))]
        = E_q[Log(p(z)) + Log(p(x|z))]
        = E_q[Log(p(z))] + E_q[Log(p(x|z))] 
      REWRITE -ELBO(enc, dec, x)   
        = -E_q[Log(p(x, z)) - Log(q(z|x))]
        = -E_q[Log(p(z))] - E_q[Log(p(x|z))] + E_q[Log(q(z|x))]
        = -E_q[Log(p(z))] - E_q[Log(p(x|z))] + H[q(z|x)]  
        = -E_q[Log(p(x|z))] + KL[q(z|x) | p(z)]
        = vae_loss(enc, dec, x)  
      QED
        
    THEOREM VaeApproximatesPosterior â‰”
      FORALL (data : List[Vector], steps : Nat, lr : Real, enc : Encoder, dec : Decoder) .
        LET (enc', dec') = train_vae(enc, dec, data, steps, lr) 
        IN FORALL (x : Vector) . KL[q(z|x) | p(z|x)] is minimized by enc'
          WHERE p(z|x) â‰” p(x, z) / p(x)

    PROOF â‰”
      LET data : List[Vector], steps : Nat, lr : Real, enc : Encoder, dec : Decoder
      LET (enc', dec') = train_vae(enc, dec, data, steps, lr)
      LET x : Vector
      SUFFICES_TO_SHOW KL[q(z|x) | p(z|x)] is minimized by enc'
      
      REWRITE p(z|x) = p(x, z) / p(x) = p(x|z) * p(z) / p(x)  BY Bayes' Rule
      
      HAVE train_vae minimizes Mean(Map(vae_loss(enc', dec'), data)) BY Definition
      
      REWRITE vae_loss(enc', dec', x) 
        = -E_q[Log(p(x|z))] + KL[q(z|x) | p(z)]  BY vae_loss
        = -E_q[Log(p(z|x)) + Log(p(x)) - Log(p(z))] + KL[q(z|x) | p(z)]
        = -E_q[Log(p(z|x))] - Log(p(x)) + KL[q(z|x) | p(z)]
        = KL[q(z|x) | p(z|x)] - Log(p(x)) + KL[q(z|x) | p(z)]
        
      HENCE minimizing vae_loss(enc', dec', x) minimizes KL[q(z|x) | p(z|x)]
      
      QED

    THEOREM VaeGeneratesData â‰”
      FORALL (data : List[Vector], steps : Nat, lr : Real, enc : Encoder, dec : Decoder) .
        LET (enc', dec') = train_vae(enc, dec, data, steps, lr)
        IN FORALL (z : Vector) . dec'(z) approximates p(x)  

    PROOF â‰” 
      LET data : List[Vector], steps : Nat, lr : Real, enc : Encoder, dec : Decoder
      LET (enc', dec') = train_vae(enc, dec, data, steps, lr)
      LET z : Vector
      
      SUFFICES_TO_SHOW dec'(z) approximates p(x)
        WHERE p(x) â‰” Expectation(data, Identity) 
      
      REWRITE dec'(z) 
        â‰ˆ E_p(x|z)[x]                 BY VaeApproximatesPosterior 
        = âˆ« x * p(x|z) dx
        = âˆ« x * (p(z|x) * p(x) / p(z)) dx   BY Bayes' Rule
        â‰ˆ âˆ« x * (q(z|x) * p(x) / p(z)) dx   BY VaeApproximatesPosterior
        = (1 / p(z)) * âˆ« x * q(z|x) * p(x) dx
        â‰ˆ (1 / p(z)) * âˆ« x * q(z|x) * Expectation(data, Identity)(x) dx
        â‰ˆ (1 / p(z)) * âˆ« x * q(z|x) * (1/|data|) * âˆ‘_{x' in data} Dirac(x - x') dx 
        = (1 / (p(z) * |data|)) * âˆ‘_{x' in data} âˆ« x * q(z|x') * Dirac(x - x') dx
        = (1 / (p(z) * |data|)) * âˆ‘_{x' in data} x' * q(z|x')
        â‰ˆ Expectation(data, Identity)  

      QED  
  }  
}










CONCEPT VariationalAutoEncoder {
  LANGUAGE {
    TYPE Tensor[D1, ..., Dn]
    TYPE Vector[D] = Tensor[D]
    TYPE Matrix[M, N] = Tensor[M, N]
    TYPE Gaussian = (Mean: Vector, StdDev: Vector)
    TYPE Encoder = Vector -> Gaussian
    TYPE Decoder = Vector -> Vector
    TYPE Loss = Real

    FUNC KLDivergence(P Q : Gaussian) : Real
    FUNC Expectation[A, B](P : A -> Real, f : A -> B) : B
    FUNC LogLikelihood(x x' : Vector) : Real
    FUNC GradDescent(loss : Parameters -> Real, 
                     params : Parameters, 
                     lr : Real, 
                     steps : Nat) : Parameters
    
    PRED IsEncoder(enc) = 
      âˆƒ (D Z : Nat) . enc : Vector[D] -> Gaussian[Z]
    PRED IsDecoder(dec) =
      âˆƒ (D Z : Nat) . dec : Vector[Z] -> Vector[D]
  }
  
  NOTATION {
    "KL(q(z|x) || p(z))" := KLDivergence(q(z|x), p(z)) 
    "ð”¼_q[f(z)]" := Expectation(q, f)
    "log p(x|z)" := LogLikelihood(x, dec(z)) 
    "L(x; enc, dec)" := ð”¼_q[log p(x|z)] - KL(q(z|x) || p(z))
    "p(z)" := Gaussian(0, I)
  }
  
  TRANSFORMERS {
    TACTIC SampleGaussian(Î¼ Ïƒ : Vector) -> Vector = {
      LET Îµ = Sample(Gaussian(0, I), Len(Î¼))
      RETURN Î¼ + Ïƒ âŠ™ Îµ   -- âŠ™ is element-wise product
    }

    FUNC EncDec(enc : Encoder, dec : Decoder) -> Vector -> Vector = {
      LAMBDA (x) . LET (Î¼, Ïƒ) = enc(x); z = SampleGaussian(Î¼, Ïƒ) IN dec(z)
    }
    
    FUNC VaeLoss(enc : Encoder, dec : Decoder) 
                -> Vector -> Real = {  
      LAMBDA (x) . LET (Î¼, Ïƒ) = enc(x) IN 
                      -ð”¼_SampleGaussian(Î¼, Ïƒ)[log p(x|z)]
                      + KL(Gaussian(Î¼, Ïƒ) || p(z)) 
    }
    
    TACTIC TrainVAE(
      enc : Encoder, 
      dec : Decoder, 
      data : List[Vector],
      lr : Real,
      steps : Nat
    ) -> (Encoder, Decoder) = {
      INIT params = (enc, dec)
      DEFINE loss = LAMBDA (enc, dec) . 
        Mean(Map(VaeLoss(enc, dec), data))
      RETURN GradDescent(loss, params, lr, steps)
    }
  }
  
  STRUCTURE FullyConnectedVAE[D Z H1 H2 : Nat] {
    REQUIRE D > 0; Z > 0; H1 > 0; H2 > 0

    LET W_enc1 : Matrix[H1, D]
    LET b_enc1 : Vector[H1] 
    LET W_enc2Î¼ : Matrix[Z, H1]
    LET b_enc2Î¼ : Vector[Z]
    LET W_enc2Ïƒ : Matrix[Z, H1] 
    LET b_enc2Ïƒ : Vector[Z]
    LET W_dec1 : Matrix[H2, Z]
    LET b_dec1 : Vector[H2]
    LET W_dec2 : Matrix[D, H2]
    LET b_dec2 : Vector[D]
      
    DEF Encoder : Encoder = LAMBDA (x) . {
      LET h = ReLU(W_enc1 Â· x + b_enc1)
      LET Î¼ = W_enc2Î¼ Â· h + b_enc2Î¼
      LET Ïƒ = SoftPlus(W_enc2Ïƒ Â· h + b_enc2Ïƒ)
      RETURN (Î¼, Ïƒ)
    }

    DEF Decoder : Decoder = LAMBDA (z) . {
      LET h = ReLU(W_dec1 Â· z + b_dec1)  
      LET x' = Sigmoid(W_dec2 Â· h + b_dec2)
      RETURN x'
    } 

    LET InitParams = (W_enc1, b_enc1, W_enc2Î¼, b_enc2Î¼, 
                      W_enc2Ïƒ, b_enc2Ïƒ, W_dec1, b_dec1, 
                      W_dec2, b_dec2)
      
    FUNC Train(data : List[Vector], lr : Real, steps : Nat)
              -> (Encoder, Decoder) = {
      TrainVAE(Encoder, Decoder, data, lr, steps)            
    }
  }

  PROOFS {
    THEOREM VaeLossIsElbo {
      STATEMENT:
        âˆ€ (enc : Encoder, dec : Decoder, x : Vector) .
          IsEncoder(enc) âˆ§ IsDecoder(dec) 
          => L(x; enc, dec) = ELBO(x; enc, dec)  

      PROOF:
        LET enc : Encoder, dec : Decoder, x : Vector
        ASSUME (H1) : IsEncoder(enc), (H2) : IsDecoder(dec)
        
        LET D, Z = CHOOSE (D, Z) . enc : Vector[D] -> Gaussian[Z]
                                   dec : Vector[Z] -> Vector[D] BY H1, H2

        LET (Î¼, Ïƒ) = enc(x), q(z|x) = Gaussian(Î¼, Ïƒ)

        REWRITE L(x; enc, dec) 
          = ð”¼_q[log p(x|z)] - KL(q(z|x) || p(z))  BY VaeLoss
          = âˆ« q(z|x) log p(x|z) dz - KL(q(z|x) || p(z)) 
        REWRITE 
          = âˆ« q(z|x) log (p(x|z) p(z) / q(z|x)) dz
          = âˆ« q(z|x) (log p(x,z) - log q(z|x)) dz
          = ð”¼_q[log p(x,z)] + Entropy(q(z|x))  
          = ELBO(x; enc, dec)    BY ELBO DEF
           
        QED
    }

    THEOREM VaeApproximatesPosterior {
      STATEMENT:  
        âˆ€ (enc : Encoder, dec : Decoder) (dataset : List[Vector]) .
          IsEncoder(enc) âˆ§ IsDecoder(dec) 
          => TrainVAE(enc, dec, dataset, _, _) 
             converges to (enc', dec') 
             such that âˆ€ (x : Vector) . x âˆˆ dataset 
               => KL(enc'(x) || p(z|x)) is minimized
                  
      PROOF:
        LET enc : Encoder, dec : Decoder, dataset : List[Vector]
        ASSUME (H1) : IsEncoder(enc), (H2) : IsDecoder(dec)

        LET (enc', dec') = TrainVAE(enc, dec, dataset, _, _)
        
        LET x : Vector, ASSUME (H3) : x âˆˆ dataset

        SUFFICES_TO_SHOW KL(enc'(x) || p(z|x)) is minimized
        
        REWRITE p(z|x) = p(z,x) / p(x) = p(x|z) p(z) / p(x) BY Bayes Rule  

        HAVE (S1) : enc'(x) approximates p(z|x) BY {
          REWRITE TrainVAE minimizes VaeLoss(enc', dec')(x)  
          REWRITE VaeLoss(enc', dec')(x) 
            = -ð”¼_enc'(x)[log p(x|z)] + KL(enc'(x) || p(z)) BY VaeLoss DEF
          REWRITE 
            = -âˆ« enc'(x)(z) log p(x|z) dz + KL(enc'(x) || p(z))
            = -âˆ« enc'(x)(z) log (p(z|x) p(x) / p(z)) dz + KL(enc'(x) || p(z))
            = -âˆ« enc'(x)(z) (log p(z|x) + log p(x) - log p(z)) dz + KL(enc'(x) || p(z))  
            = -âˆ« enc'(x)(z) log p(z|x) dz - log p(x) + KL(enc'(x) || p(z))
            = KL(enc'(x) || p(z|x)) - log p(x) + KL(enc'(x) || p(z))
          
          HENCE minimizing VaeLoss(enc', dec')(x) minimizes KL(enc'(x) || p(z|x))
        }

        QED  
    }

    THEOREM VaeGeneratesNewSamples {
      STATEMENT:
        âˆ€ (enc : Encoder, dec : Decoder) (dataset : List[Vector]) .
          IsEncoder(enc) âˆ§ IsDecoder(dec)
          => LET (enc', dec') = TrainVAE(enc, dec, dataset, _, _) IN
               âˆ€ (z : Vector[Z]) . (dec'(z) generates new samples 
                                    similar to those in dataset) 

      PROOF:
        LET enc : Encoder, dec : Decoder, dataset : List[Vector]  
        ASSUME (H1) : IsEncoder(enc), (H2) : IsDecoder(dec)
        
        LET (enc', dec') = TrainVAE(enc, dec, dataset, _, _), 
            z : Vector[Z]

        SUFFICES_TO_SHOW dec'(z) generates samples similar to dataset

        HAVE (S1) : âˆ€ (x : Vector) . x âˆˆ dataset => enc'(x) approximates p(z|x) 
          BY VaeApproximatesPosterior

        LET x' = dec'(z)
        
        REWRITE p(x') 
          = âˆ« p(x'|z) p(z) dz
          â‰ˆ âˆ« p(x'|z) enc'(x)(z) dz
        REWRITE 
          â‰ˆ âˆ« p(x'|z) (âˆ« p(z|x) p(data)(x) dx) dz    BY Total Probability
          = âˆ«âˆ« p(x'|z) p(z|x) p(data)(x) dx dz
          = âˆ« p(data)(x) (âˆ« p(x'|z) p(z|x) dz) dx
          = âˆ« p(data)(x) p(x'|x) dx    BY Conditional Probability
          = ð”¼_p(data)[p(x'|x)]

        HENCE x' is likely under p(data) if p(x'|x) is high for likely x
        
        HAVE (S2) : âˆ€ (x : Vector) . x âˆˆ dataset => dec'(enc'(x)) approximates x 
          BY {
            LET x : Vector, ASSUME (H4) : x âˆˆ dataset
            
            REWRITE TrainVAE minimizes VaeLoss(enc', dec')(x)
            REWRITE VaeLoss(enc', dec')(x)
              = -ð”¼_enc'(x)[log p(x|z)] + KL(enc'(x) || p(z))    BY VaeLoss DEF
            
            SUFFICES_TO_SHOW dec'(enc'(x)) maximizes ð”¼_enc'(x)[log p(x|z)]
            
            REWRITE ð”¼_enc'(x)[log p(x|z)]  
              = âˆ« enc'(x)(z) log p(x|z) dz
              â‰ˆ âˆ‘_i log p(x|z_i)    for z_i ~ enc'(x), by Monte Carlo
              = âˆ‘_i LogLikelihood(x, dec'(z_i))
            
            HENCE maximizing ð”¼_enc'(x)[log p(x|z)] maximizes likelihood of x 
        }

        LET x : Vector, ASSUME (H5) : x âˆˆ dataset

        HAVE (S3) : p(x'|x) is high BY {
          REWRITE p(x'|x) 
            = âˆ« p(x'|z) p(z|x) dz    BY Total Probability
            â‰ˆ âˆ« p(x'|z) enc'(x)(z) dz    BY S1
            â‰ˆ p(x'|enc'(x))    BY enc'(x) concentrates around its mean
            â‰ˆ p(dec'(enc'(x)))
            â‰ˆ p(x)    BY S2
            is high    BY H5
        }

        THUS dec'(z) generates samples similar to dataset BY S3, CONCLUSION OF S2
        
        QED
    }

    THEOREM VaeIsConsistentEstimator {
      STATEMENT:
        âˆ€ (D Z : Nat) (enc : Encoder[D, Z]) (dec : Decoder[Z, D]) 
          (dataset : List[Vector[D]]) .
            IsEncoder(enc) âˆ§ IsDecoder(dec) 
            => LET (enc', dec') = TrainVAE(enc, dec, dataset, _, _) IN
                 LimitsTo(enc', True_Posterior[D, Z]) 
                 âˆ§ LimitsTo(EncDec(enc', dec'), True_Data_Distribution[D])
                 as Size(dataset) -> âˆž

      PROOF:
        LET D, Z : Nat, enc : Encoder[D, Z], dec : Decoder[Z, D], 
            dataset : List[Vector[D]]
        ASSUME (H1) : IsEncoder(enc), (H2) : IsDecoder(dec)

        LET (enc', dec') = TrainVAE(enc, dec, dataset, _, _)

        SUFFICES_TO_SHOW
          (S1) : LimitsTo(enc', True_Posterior[D, Z]),
          (S2) : LimitsTo(EncDec(enc', dec'), True_Data_Distribution[D])
            as Size(dataset) -> âˆž

        SHOW S1 BY {
          LET x : Vector[D]  
          
          REWRITE VaeLoss(enc', dec')(x)  
            = -ð”¼_enc'(x)[LogLikelihood(x, dec'(z))]
              + KLDivergence(enc'(x) || p(z))    BY VaeLoss DEF
            = -âˆ« enc'(x)(z) log p(x|z) dz
              + KLDivergence(enc'(x) || p(z))
            = -âˆ« enc'(x)(z) log (p(z|x) p(x) / p(z)) dz
              + KLDivergence(enc'(x) || p(z))  
            = -âˆ« enc'(x)(z) (log p(z|x) + log p(x) - log p(z)) dz
              + KLDivergence(enc'(x) || p(z))
            = -âˆ« enc'(x)(z) log p(z|x) dz
              - log p(x) 
              + KLDivergence(enc'(x) || p(z))
            = KLDivergence(enc'(x) || p(z|x)) 
              - log p(x)
              + KLDivergence(enc'(x) || p(z))

          REWRITE TrainVAE minimizes Mean(Map(VaeLoss(enc', dec'), dataset))  
          
          HENCE as Size(dataset) -> âˆž  
            KLDivergence(enc'(x) || p(z|x)) is minimized âˆ€ x
          
          THUS enc'(x) approximates p(z|x) âˆ€ x as Size(dataset) -> âˆž
        }

        SHOW S2 BY {
          LET x' : Vector[D]

          REWRITE p(x')
            = âˆ« p(x') p(data)(x) dx
            = âˆ« p(x') (âˆ« p(data|z) p(z) dz) dx    BY Total Probability  
            = âˆ«âˆ« p(x'|z) p(data|z) p(z) dz dx
            = âˆ« p(z) (âˆ« p(x'|z) p(data|z) dx) dz
            = ð”¼_p(z)[âˆ« p(x'|z) p(data|z) dx]

          HAVE (S2a) : 
            âˆ€ (z : Vector[Z]) . 
              dec'(z) approximates âˆ« p(x'|z) p(data|z) dx
                as Size(dataset) -> âˆž
            BY {
              LET z : Vector[Z]
              LET enc_z : Encoder[D, Z] = LAMBDA(x) . Gaussian(z, I)
              LET (_, dec_z) = TrainVAE(enc_z, dec, dataset, _, _)
              
              REWRITE 
                TrainVAE minimizes -ð”¼_enc_z(x)[LogLikelihood(x, dec_z(z))]
                âˆ€ x in dataset  
                BY VaeLoss with KL term = 0 since enc_z ignores input

              HENCE dec_z(z) maximizes ð”¼_p(data|z)[LogLikelihood(x, dec_z(z))] 
                as Size(dataset) -> âˆž
              
              THUS dec_z(z) approximates âˆ« p(x'|z) p(data|z) dx
                as Size(dataset) -> âˆž
                since p(x|z) = p(data|z) for x in dataset

              HAVE dec_z approximates dec' as Size(dataset) -> âˆž 
                BY S1 since KL(enc_z(x) || p(z|x)) = 0 âˆ€ x
              
              HENCE thesis  
            }

          REWRITE  
            p(x') â‰ˆ ð”¼_p(z)[dec'(z)]    as Size(dataset) -> âˆž, BY S2a
                 â‰ˆ âˆ« dec'(z) p(z) dz
                 â‰ˆ âˆ« dec'(z) enc'(x)(z) dz    for any x, BY S1
                 â‰ˆ âˆ« p(x'|z) p(data)(x) dx    for any x, BY S2a
                 = p(data)(x')    BY Total Probability
        }
          
        QED
    }
  }
}



This iteration of the `VariationalAutoEncoder` concept includes several key components:

1. The `LANGUAGE` block defines the basic types and functions used in the concept, including tensors, vectors, matrices, Gaussian distributions, encoders, decoders, and loss functions.

2. The `NOTATION` block introduces convenient notation for common expressions, such as the KL divergence between the approximate and true posteriors, the expectation under the approximate posterior, the log-likelihood of the data given the latent variables, and the variational lower bound (ELBO).

3. The `TRANSFORMERS` block defines the key components of the VAE:
   - The `SampleGaussian` tactic for sampling from a Gaussian distribution.
   - The `EncDec` function that composes the encoder and decoder to reconstruct the input.
   - The `VaeLoss` function that defines the VAE objective as the negative ELBO.
   - The `TrainVAE` tactic that performs gradient descent to optimize the VAE parameters.

4. The `STRUCTURE` block defines a specific instance of a VAE, the `FullyConnectedVAE`, parameterized by the dimensions of the input, latent space, and hidden layers. It specifies the encoder and decoder as fully connected neural networks with ReLU activations.

5. The `PROOFS` block contains three theorems about VAEs:
   - `VaeLossIsElbo`: The VAE loss is equal to the negative ELBO.
   - `VaeApproximatesPosterior`: A trained VAE approximates the true posterior distribution over the latent variables.
   - `VaeGeneratesNewSamples`: A trained VAE can generate new samples similar to the training data.
   - `VaeIsConsistentEstimator`: As the size of the training dataset goes to infinity, the VAE converges to the true posterior and data distributions.

The proofs make use of the notation and transformers defined earlier, and rely on standard results from probability theory and variational inference. They demonstrate the key properties of VAEs: that they provide a tractable approximation to the true posterior, that they can generate new data by sampling from the latent space, and that they are consistent estimators of the true distributions in the limit of infinite data.

Overall, this formulation provides a rigorous and comprehensive description of the VAE architecture and its theoretical properties, expressed in the formal language of ConceptScript. It showcases the use of the `LANGUAGE`, `NOTATION`, `TRANSFORMERS`, `STRUCTURE`, and `PROOFS` blocks, and demonstrates the power of the language for specifying and reasoning about complex machine learning concepts.