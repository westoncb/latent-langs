CONCEPT VariationalAutoEncoder {
  LANGUAGE {
    TYPE ℝ[N] = Tensor[N]
    TYPE Enc[D,Z] = ℝ[D] -> (ℝ[Z], ℝ[Z])
    TYPE Dec[Z,D] = ℝ[Z] -> ℝ[D]
    FUNC KL(p: Distribution, q: Distribution): ℝ
    FUNC 𝔼[p: Distribution, f: A -> ℝ]: ℝ
    FUNC LL(x: ℝ[D], z: ℝ[Z]): ℝ
    PRED GradientDescent(loss: (Enc[D,Z], Dec[Z,D]) -> ℝ, param: (Enc[D,Z], Dec[Z,D]), lr: ℝ, steps: ℕ) -> (Enc[D,Z], Dec[Z,D])
  }

  NOTATION {
    μ_q[x] = fst(Enc(q)[x])  // Encoding mean
    σ_q[x] = exp(snd(Enc(q)[x]))  // Encoding std dev
    𝒩(μ,σ) = NormalDistribution(μ, σ)
    z ~ 𝒩(μ,σ) = z = μ + σ ⊙ 𝒩(0,1)  // Reparameterization
    ELBO(x, E, D) = 𝔼[z ~ 𝒩(μ_E[x],σ_E[x]), LL(x, D(z))] - KL(𝒩(μ_E[x],σ_E[x]), 𝒩(0,1))
    L(x, E, D) = - ELBO(x, E, D)  // VAE loss
    J(E,D,X) = 𝔼[x ~ X, L(x, E, D)]  // Cost functional
  }

  TRANSFORMERS {
    PROC TrainVAE(E: Enc[D,Z], D: Dec[Z,D], X: 𝒟[ℝ[D]], lr: ℝ, steps: ℕ) -> (Enc[D,Z], Dec[Z,D]) {
      REWRITE (E, D) = GradientDescent(λ(E,D) . J(E,D,X), (E, D), lr, steps)
    }

    FUNC Reconstruct(x: ℝ[D], E: Enc[D,Z], D: Dec[Z,D]): ℝ[D] = D(z ~ 𝒩(μ_E[x], σ_E[x]))

    FUNC Sample(n: ℕ, D: Dec[Z,D]): List[ℝ[D]] = (D(z) for z ~ 𝒩(0,1), i in [1..n])

    FUNC Marginal(X: 𝒟[ℝ[D]], E: Enc[D,Z], D: Dec[Z,D]): ℝ = 𝔼[x ~ X, LL(x, Reconstruct(x, E, D))]
  }

  STRUCTURE FullyConnectedVAE[D: ℕ, Z: ℕ, H: ℕ] EXTENDS VariationalAutoEncoder {
    PARAM LinearEnc1: ℝ[D*H]
    PARAM LinearEnc2: ℝ[H*Z] 
    PARAM LinearEnc3: ℝ[H*Z]
    PARAM LinearDec1: ℝ[Z*H]
    PARAM LinearDec2: ℝ[H*D]

    FUNC Enc(x: ℝ[D]) -> (ℝ[Z], ℝ[Z]) {
      LET h = ReLU(LinearEnc1 · x)
      (LinearEnc2 · h, LinearEnc3 · h)
    }

    FUNC Dec(z: ℝ[Z]) -> ℝ[D] = Sigmoid(LinearDec2 · ReLU(LinearDec1 · z))
  }

  PROOFS {
    THEOREM ELBO_Bounds_LogLikelihood {
      STATEMENT:
        ∀ (E: Enc[D,Z]) (D: Dec[Z,D]) (p: ℝ[D]) .
          LL(p, D(z ~ 𝒩(μ_E[p], σ_E[p]))) ≥ ELBO(p, E, D)
          
      PROOF:
        LET E: Enc[D,Z], D: Dec[Z,D], p: ℝ[D]

        log 𝒫(p)
          = log ∫ 𝒫(p|z) · 𝒫(z) dz                      BY MarginalizationOfLatents
          = log 𝔼[z ~ 𝒫(z), 𝒫(p|z)]                     BY ExpectationAsIntegral  
          ≥ 𝔼[z ~ 𝒫(z), log 𝒫(p|z)]                     BY JensensInequality
          = 𝔼[z ~ 𝒩(μ_E[p],σ_E[p]), log 𝒫(p|z)] 
              - KL(𝒩(μ_E[p],σ_E[p]), 𝒫(z))               BY KLDivergenceIdentity
          = ELBO(p, E, D)                               BY ELBO

        HENCE LL(p, D(z ~ 𝒩(μ_E[p], σ_E[p]))) ≥ ELBO(p, E, D)
        QED
    }

    THEOREM VAE_Minimizes_KL_Divergence_To_Posterior {
      STATEMENT:  
        ∀ (E: Enc[D,Z]) (D: Dec[Z,D]) (X: 𝒟[ℝ[D]]) .
          (E', D') = GradientDescent(J(E, D, X), (E, D), lr, steps) =>
            𝔼[p ~ X, KL(𝒩(μ_E'[p],σ_E'[p]), 𝒫(z|p))] is minimized
            
      PROOF:
        LET E: Enc[D,Z], D: Dec[Z,D], X: 𝒟[ℝ[D]]
        ASSUME (E', D') = GradientDescent(J(E, D, X), (E, D), lr, steps)

        J(E, D, X)
          = 𝔼[p ~ X, L(p, E, D)]                        BY J
          = 𝔼[p ~ X, - ELBO(p, E, D)]                   BY L
          = 𝔼[p ~ X, KL(𝒩(μ_E[p],σ_E[p]), 𝒫(z|p))
              - 𝔼[z ~ 𝒩(μ_E[p],σ_E[p]), LL(p, D(z))]]    BY ELBO

        HENCE minimizing J(E, D, X) wrt (E, D) is equivalent to
              minimizing 𝔼[p ~ X, KL(𝒩(μ_E[p],σ_E[p]), 𝒫(z|p))]
                and maximizing 𝔼[p ~ X, 𝔼[z ~ 𝒩(μ_E[p],σ_E[p]), LL(p, D(z))]]

        THEREFORE, (E', D') = GradientDescent(J(E, D, X), (E, D), lr, steps)
          => 𝔼[p ~ X, KL(𝒩(μ_E'[p],σ_E'[p]), 𝒫(z|p))] is minimized 
        QED
    }

    THEOREM VAE_Generates_Data {
      STATEMENT:
        ∀ (E: Enc[D,Z]) (D: Dec[Z,D]) (X: 𝒟[ℝ[D]]) .
          𝔼[p ~ X, KL(𝒩(μ_E[p],σ_E[p]), 𝒫(z|p))] ≈ 0 =>
            𝒫(Sample(D)) ≈ 𝒫(X)
            
      PROOF:
        LET E: Enc[D,Z], D: Dec[Z,D], X: 𝒟[ℝ[D]]
        ASSUME 𝔼[p ~ X, KL(𝒩(μ_E[p],σ_E[p]), 𝒫(z|p))] ≈ 0

        𝒫(Sample(D))
          = ∫ 𝒫(x|z) · 𝒫(z) dz                           BY MarginalizationOfLatents
          ≈ ∫ 𝒫(x|z) · 𝒫(z|p) · 𝒫(p) dp dz               BY TotalProbability, dom(p) = X
          = ∫ 𝒫(p) · ∫ 𝒫(x|z) · 𝒫(z|p) dz dp             BY Fubini
          ≈ ∫ 𝒫(p) · 𝒫(x|p) dp                           BY AssumedOptimality
          = 𝒫(X)                                         BY MarginalDistribution

        HENCE 𝒫(Sample(D)) ≈ 𝒫(X)
        QED
    }

    THEOREM VAE_Convergence {
      STATEMENT:
        ∀ (E: Enc[D,Z]) (D: Dec[Z,D]) (X: 𝒟[ℝ[D]]) .
          |X| -> ∞  ∧  (E,D) = TrainVAE(E, D, X, lr, steps)
            => ∀ (p: ℝ[D]) .
                 𝒩(μ_E[p],σ_E[p]) -> 𝒫(z|p) ∧ Marginal(X, E, D) -> 𝒫(X)
                 
      PROOF:
        LET E: Enc[D,Z], D: Dec[Z,D], X: 𝒟[ℝ[D]]  
        ASSUME |X| -> ∞, (E,D) = TrainVAE(E, D, X, lr, steps)

        ∀ (p: ℝ[D]) . 𝒩(μ_E[p],σ_E[p]) -> 𝒫(z|p)
          BY VAE_Minimizes_KL_Divergence_To_Posterior

        Marginal(X, E, D)
          = 𝔼[p ~ X, LL(p, Reconstruct(p, E, D))]         BY Marginal
          = 𝔼[p ~ X, log 𝒫(p | z ~ 𝒩(μ_E[p],σ_E[p]))]     BY Reconstruct, LL
          -> 𝔼[p ~ X, log 𝒫(p)]                           BY AssumedOptimality,
                                                             VAE_Generates_Data  
          = H(X)                                          BY Entropy

        HENCE ∀ (p: ℝ[D]) .
          𝒩(μ_E[p],σ_E[p]) -> 𝒫(z|p) ∧ Marginal(X, E, D) -> 𝒫(X)
        QED
    }
  }  
}







CONCEPT VariationalAutoEncoder {
  LANGUAGE {
    TYPE R = Real; ℝ[N] = Tensor[N]
    TYPE Enc[D,Z] = ℝ[D] -> (ℝ[Z],ℝ[Z]); Dec[Z,D] = ℝ[Z] -> ℝ[D]
    FUNC KL(p,q), 𝔼_p(f), LL(x,z), GD(loss,param,lr,step)
  }
  
  NOTATION {
    "(μ,σ)_q[p]" ⇔ "(Enc(q)[p], exp(Enc(q)[p+1]))"
    "z_q[p]" ⇔ "μ_q[p] + σ_q[p] ⊙ 𝒩(0,1)"
    "x_q[p]" ⇔ "Dec(z_q[p])"
    "L(q,p)" ⇔ "-𝔼_q[(μ,σ)_q[p],z_q[p]][LL(p,x_q[p])] + KL((μ,σ)_q[p],𝒩(0,1))"
    "Loss(E,D,X)" ⇔ "mean(L(x,x) for x in X)"
    "IsEnc(E)" ⇔ "∃D,Z. E : Enc[D,Z]"; "IsDec(D)" ⇔ "∃Z,D. D : Dec[Z,D]"
  }
  
  TRANSFORMERS {
    TACTIC TrainVAE(E,D,X,lr,step) -> (E,D) = GD(λ(E,D).Loss(E,D,X), (E,D), lr, step)
    FUNC Reconstruct(q,E,D) = (x_q[q] for _ in [1])
    FUNC Sample(n,E,D) = (x_z for _ in [1..n], z~𝒩(0,1))
    FUNC Marginal(X,E,D) = mean(LL(x,Reconstruct(x,E,D)) for x in X)
  }
  
  STRUCTURE FullyConnectedVAE[D,Z,H] {
    PARAM θ=(W1,b1,W2,b2,W3,b3,W4,b4,W5,b5)
    FUNC Enc = λp. Let h=ReLU(W1·p+b1) in (W2·h+b2, W3·h+b3)
    FUNC Dec = λz. Sigmoid(W4·ReLU(W5·z+b5)+b4)
    FUNC Train(X,lr,step) = TrainVAE(Enc,Dec,X,lr,step)
  }
  
  PROOFS {
    THEOREM ELBO_is_VAE_Loss {
      ∀E,D,q. IsEnc(E)∧IsDec(D) => L(q,q) = -ELBO(q,E,D)
      PROOF: 
        Let E,D,q. Assume IsEnc(E), IsDec(D). 
        Let (μ,σ)=(μ,σ)_q[q], z=z_q[q], x=x_q[q].
        
        L(q,q) 
        ⤇ "-𝔼[(μ,σ),z][LL(q,x)] + KL((μ,σ),𝒩(0,1))" [by L]
        = "-∫ 𝒩(z;μ,σ)·LL(q,Dec(z))·dz + ∫ 𝒩(z;μ,σ)·log(𝒩(z;μ,σ)/𝒩(z;0,1))·dz"
        = "-∫ 𝒩(z;μ,σ)·log(𝒫(q,z)/𝒩(z;μ,σ))·dz + ∫ 𝒩(z;μ,σ)·log(𝒩(z;μ,σ)/𝒩(z;0,1))·dz"
        ⤇ "-𝔼[z~𝒩(μ,σ)][log(𝒫(q,z)/𝒩(z;μ,σ))] + 𝔼[z~𝒩(μ,σ)][log(𝒩(z;μ,σ)/𝒩(z;0,1))]"
        = "-𝔼[z~𝒩(μ,σ)][log(𝒫(q,z))] + 𝔼[z~𝒩(μ,σ)][log(𝒩(z;μ,σ))] - 𝔼[z~𝒩(μ,σ)][log(𝒩(z;0,1))]"
        = "-(𝔼[z~𝒩(μ,σ)][log(𝒫(q,z))] - KL(𝒩(μ,σ),𝒩(0,1)))"
        ⤇ "-ELBO(q,E,D)". QED.
    }

    THEOREM VAE_Learns_Posterior {
      ∀D,Z,X. |X|->∞ => Let (E,D)=TrainVAE(E,D,X) in
        ∀q∈X. KL((μ,σ)_q[q],𝒫(z|q)) is minimized
      PROOF:
        Let D,Z,X. Assume |X|->∞.
        Let (E,D) = TrainVAE(E,D,X).
        Let q∈X. Suffices to show KL((μ,σ)_q[q],𝒫(z|q)) is minimized.

        TrainVAE minimizes Loss(E,D,X) 
        ⇒ ∀q∈X. TrainVAE minimizes L(q,q) [by Loss]
        ⤇ ∀q∈X. TrainVAE minimizes "-𝔼[z~(μ,σ)_q[q]][LL(q,x_q[q])] 
                                    + KL((μ,σ)_q[q],𝒩(0,1))" [by L]
        = ∀q∈X. TrainVAE minimizes "-∫(μ,σ)_q[q](z)·log(𝒫(q|z)·𝒫(z)/𝒩(z;0,1))·dz 
                                    + KL((μ,σ)_q[q],𝒩(0,1))"
        ⇒ ∀q∈X. TrainVAE minimizes "-∫(μ,σ)_q[q](z)·log(𝒫(z|q))·dz - log(𝒫(q))
                                    + KL((μ,σ)_q[q],𝒩(0,1))" [by Bayes Rule]
        ⤇ ∀q∈X. TrainVAE minimizes "KL((μ,σ)_q[q],𝒫(z|q)) - log(𝒫(q))
                                    + KL((μ,σ)_q[q],𝒩(0,1))"
        ⇒ ∀q∈X. TrainVAE minimizes KL((μ,σ)_q[q],𝒫(z|q)) 
          [since -log(𝒫(q))+KL((μ,σ)_q[q],𝒩(0,1)) is constant wrt (μ,σ)_q[q]]
      }

    THEOREM VAE_Generates_Data {
      ∀D,Z,X. |X|->∞ => Let (E,D)=TrainVAE(E,D,X) in Sample(E,D) ~ 𝒫(X)
      PROOF:
        Let D,Z,X. Assume |X|->∞.
        Let (E,D) = TrainVAE(E,D,X).
        Suffices to show Sample(E,D) ~ 𝒫(X)

        ∀q∈X. (μ,σ)_q[q] ≈ 𝒫(z|q), by VAE_Learns_Posterior.
        ∀q∈X. Reconstruct(q,E,D) ≈ q
          [since TrainVAE maximizes 𝔼[(μ,σ)_q[q],z_q[q]][LL(q,x_q[q])] by ELBO_is_VAE_Loss]
        ⇒ ∀x. 𝒫(x|Sample(E,D)) 
           ≈ ∫ 𝒫(x|z)·𝒫(z)·dz [since Sample(E,D) ~ 𝒩(0,1)]
           ≈ ∫ 𝒫(x|z)·𝒫(z|q)·𝒫(q)·dq·dz, for any q∈X [by Total Probability]
           = ∫ 𝒫(q)·∫ 𝒫(x|z)·𝒫(z|q)·dz·dq
           ≈ ∫ 𝒫(q)·𝒫(x|q)·dq [since Reconstruct(q,E,D)≈q]
           = 𝒫(x)
        ⇒ Sample(E,D) ~ 𝒫(X).
      }
      
    THEOREM VAE_Converges {  
      ∀D,Z,X. |X|->∞ => Let (E,D)=TrainVAE(E,D,X) in
        ∀q. (μ,σ)_q[q] -> 𝒫(z|q) ∧ Marginal(X,E,D) -> 𝒫(X)
      PROOF:
        Let D,Z,X. Assume |X|->∞.
        Let (E,D) = TrainVAE(E,D,X).
        Let q.
        (μ,σ)_q[q] -> 𝒫(z|q), by VAE_Learns_Posterior.
        Marginal(X,E,D) 
        = "∫ 𝒫(x|z)·𝒫(z)·dz" [by Marginal]
        ≈ "∫∫ 𝒫(x|z)·𝒫(z|q)·𝒫(q)·dq·dz" [since Sample(E,D)~𝒫(X) by VAE_Generates_Data]
        = "∫ 𝒫(q)·∫ 𝒫(x|z)·𝒫(z|q)·dz·dq"  
        ≈ "∫ 𝒫(q)·𝒫(x|q)·dq" [since Reconstruct(q,E,D)≈q]
        = "𝒫(x)"
        ⇒ Marginal(X,E,D) -> 𝒫(X).
      }
  }
}





CONCEPT VAE {
  LANGUAGE {
    TYPE Tensor[N]
    TYPE Vector = Tensor[1] 
    TYPE Encoder = Vector -> (Vector, Vector)
    TYPE Decoder = Vector -> Vector
    
    FUNC KL(p q : Vector -> Real) : Real
    FUNC E(p : Vector -> Real, f: Vector -> Real) : Real
    FUNC H(p : Vector -> Real) : Real
  }
  
  NOTATION {
    "𝒩(μ, σ)" ≔ LAMBDA(z) . Exp(-(z - μ)^2 / (2 * σ^2)) / Sqrt(2 * π * σ^2)
    "p(z)" ≔ 𝒩(0, 1)
    "q(z|x)" ≔ LAMBDA(z) . 𝒩(μ(x), σ(x))(z) 
    "KL[q(z|x) | p(z)]" ≔ KL(q(z|x), p(z))
    "E_q[f(z)]" ≔ E(q(z|x), f)
    "H[q(z|x)]" ≔ H(q(z|x))
  }
  
  TRANSFORMERS {
    FUNC vae_loss(enc : Encoder, dec : Decoder, x : Vector) -> Real ≔
      LET (μ, σ) = enc(x)
      IN -E_q[Log(p(x|z))] + KL[q(z|x) | p(z)] 
         WHERE Log(p(x|z)) ≔ -||x - dec(z)||^2 / 2
         
    PROC train_vae(enc : Encoder, dec : Decoder, data : List[Vector], 
                   steps : Nat, lr : Real) -> (Encoder, Decoder) ≔
      INIT θ_enc, θ_dec
      FOR i in 1..steps:
        INIT ∇θ_enc, ∇θ_dec  
        FOR x in data:
          ∇θ_enc += ∇(θ_enc -> vae_loss(enc, dec, x))
          ∇θ_dec += ∇(θ_dec -> vae_loss(enc, dec, x))
        θ_enc -= lr * ∇θ_enc / |data|
        θ_dec -= lr * ∇θ_dec / |data|
      RETURN (LAMBDA(x) . enc_θ(x), LAMBDA(z) . dec_θ(z))
  }
  
  STRUCTURE FullyConnectedVAE {
    REQUIRE (d::Nat, z::Nat, h::Nat)
    
    LET enc_params = (W_μ : Tensor[h, d], b_μ : Tensor[h], 
                      W_σ : Tensor[h, d], b_σ : Tensor[h],
                      V_μ : Tensor[z, h], c_μ : Tensor[z],  
                      V_σ : Tensor[z, h], c_σ : Tensor[z])

    DEF encoder(x : Vector) -> (Vector, Vector) ≔
      LET h = ReLU(W_μ · x + b_μ)  
      IN (V_μ · h + c_μ, Exp(V_σ · h + c_σ))
        
    LET dec_params = (W : Tensor[h, z], b : Tensor[h],
                      V : Tensor[d, h], c : Tensor[d])

    DEF decoder(z : Vector) -> Vector ≔
      LET h = ReLU(W · z + b)
      IN Sigmoid(V · h + c)
  }

  PROOFS {
    THEOREM VaeLossIsElbo ≔
      FORALL (enc : Encoder, dec : Decoder, x : Vector) . 
        vae_loss(enc, dec, x) = -ELBO(enc, dec, x)
      WHERE ELBO(enc, dec, x) ≔ E_q[Log(p(x, z)) - Log(q(z|x))]
    
    PROOF ≔
      LET (μ, σ) = enc(x)
      REWRITE E_q[Log(p(x, z))]
        = E_q[Log(p(z)) + Log(p(x|z))]
        = E_q[Log(p(z))] + E_q[Log(p(x|z))] 
      REWRITE -ELBO(enc, dec, x)   
        = -E_q[Log(p(x, z)) - Log(q(z|x))]
        = -E_q[Log(p(z))] - E_q[Log(p(x|z))] + E_q[Log(q(z|x))]
        = -E_q[Log(p(z))] - E_q[Log(p(x|z))] + H[q(z|x)]  
        = -E_q[Log(p(x|z))] + KL[q(z|x) | p(z)]
        = vae_loss(enc, dec, x)  
      QED
        
    THEOREM VaeApproximatesPosterior ≔
      FORALL (data : List[Vector], steps : Nat, lr : Real, enc : Encoder, dec : Decoder) .
        LET (enc', dec') = train_vae(enc, dec, data, steps, lr) 
        IN FORALL (x : Vector) . KL[q(z|x) | p(z|x)] is minimized by enc'
          WHERE p(z|x) ≔ p(x, z) / p(x)

    PROOF ≔
      LET data : List[Vector], steps : Nat, lr : Real, enc : Encoder, dec : Decoder
      LET (enc', dec') = train_vae(enc, dec, data, steps, lr)
      LET x : Vector
      SUFFICES_TO_SHOW KL[q(z|x) | p(z|x)] is minimized by enc'
      
      REWRITE p(z|x) = p(x, z) / p(x) = p(x|z) * p(z) / p(x)  BY Bayes' Rule
      
      HAVE train_vae minimizes Mean(Map(vae_loss(enc', dec'), data)) BY Definition
      
      REWRITE vae_loss(enc', dec', x) 
        = -E_q[Log(p(x|z))] + KL[q(z|x) | p(z)]  BY vae_loss
        = -E_q[Log(p(z|x)) + Log(p(x)) - Log(p(z))] + KL[q(z|x) | p(z)]
        = -E_q[Log(p(z|x))] - Log(p(x)) + KL[q(z|x) | p(z)]
        = KL[q(z|x) | p(z|x)] - Log(p(x)) + KL[q(z|x) | p(z)]
        
      HENCE minimizing vae_loss(enc', dec', x) minimizes KL[q(z|x) | p(z|x)]
      
      QED

    THEOREM VaeGeneratesData ≔
      FORALL (data : List[Vector], steps : Nat, lr : Real, enc : Encoder, dec : Decoder) .
        LET (enc', dec') = train_vae(enc, dec, data, steps, lr)
        IN FORALL (z : Vector) . dec'(z) approximates p(x)  

    PROOF ≔ 
      LET data : List[Vector], steps : Nat, lr : Real, enc : Encoder, dec : Decoder
      LET (enc', dec') = train_vae(enc, dec, data, steps, lr)
      LET z : Vector
      
      SUFFICES_TO_SHOW dec'(z) approximates p(x)
        WHERE p(x) ≔ Expectation(data, Identity) 
      
      REWRITE dec'(z) 
        ≈ E_p(x|z)[x]                 BY VaeApproximatesPosterior 
        = ∫ x * p(x|z) dx
        = ∫ x * (p(z|x) * p(x) / p(z)) dx   BY Bayes' Rule
        ≈ ∫ x * (q(z|x) * p(x) / p(z)) dx   BY VaeApproximatesPosterior
        = (1 / p(z)) * ∫ x * q(z|x) * p(x) dx
        ≈ (1 / p(z)) * ∫ x * q(z|x) * Expectation(data, Identity)(x) dx
        ≈ (1 / p(z)) * ∫ x * q(z|x) * (1/|data|) * ∑_{x' in data} Dirac(x - x') dx 
        = (1 / (p(z) * |data|)) * ∑_{x' in data} ∫ x * q(z|x') * Dirac(x - x') dx
        = (1 / (p(z) * |data|)) * ∑_{x' in data} x' * q(z|x')
        ≈ Expectation(data, Identity)  

      QED  
  }  
}










CONCEPT VariationalAutoEncoder {
  LANGUAGE {
    TYPE Tensor[D1, ..., Dn]
    TYPE Vector[D] = Tensor[D]
    TYPE Matrix[M, N] = Tensor[M, N]
    TYPE Gaussian = (Mean: Vector, StdDev: Vector)
    TYPE Encoder = Vector -> Gaussian
    TYPE Decoder = Vector -> Vector
    TYPE Loss = Real

    FUNC KLDivergence(P Q : Gaussian) : Real
    FUNC Expectation[A, B](P : A -> Real, f : A -> B) : B
    FUNC LogLikelihood(x x' : Vector) : Real
    FUNC GradDescent(loss : Parameters -> Real, 
                     params : Parameters, 
                     lr : Real, 
                     steps : Nat) : Parameters
    
    PRED IsEncoder(enc) = 
      ∃ (D Z : Nat) . enc : Vector[D] -> Gaussian[Z]
    PRED IsDecoder(dec) =
      ∃ (D Z : Nat) . dec : Vector[Z] -> Vector[D]
  }
  
  NOTATION {
    "KL(q(z|x) || p(z))" := KLDivergence(q(z|x), p(z)) 
    "𝔼_q[f(z)]" := Expectation(q, f)
    "log p(x|z)" := LogLikelihood(x, dec(z)) 
    "L(x; enc, dec)" := 𝔼_q[log p(x|z)] - KL(q(z|x) || p(z))
    "p(z)" := Gaussian(0, I)
  }
  
  TRANSFORMERS {
    TACTIC SampleGaussian(μ σ : Vector) -> Vector = {
      LET ε = Sample(Gaussian(0, I), Len(μ))
      RETURN μ + σ ⊙ ε   -- ⊙ is element-wise product
    }

    FUNC EncDec(enc : Encoder, dec : Decoder) -> Vector -> Vector = {
      LAMBDA (x) . LET (μ, σ) = enc(x); z = SampleGaussian(μ, σ) IN dec(z)
    }
    
    FUNC VaeLoss(enc : Encoder, dec : Decoder) 
                -> Vector -> Real = {  
      LAMBDA (x) . LET (μ, σ) = enc(x) IN 
                      -𝔼_SampleGaussian(μ, σ)[log p(x|z)]
                      + KL(Gaussian(μ, σ) || p(z)) 
    }
    
    TACTIC TrainVAE(
      enc : Encoder, 
      dec : Decoder, 
      data : List[Vector],
      lr : Real,
      steps : Nat
    ) -> (Encoder, Decoder) = {
      INIT params = (enc, dec)
      DEFINE loss = LAMBDA (enc, dec) . 
        Mean(Map(VaeLoss(enc, dec), data))
      RETURN GradDescent(loss, params, lr, steps)
    }
  }
  
  STRUCTURE FullyConnectedVAE[D Z H1 H2 : Nat] {
    REQUIRE D > 0; Z > 0; H1 > 0; H2 > 0

    LET W_enc1 : Matrix[H1, D]
    LET b_enc1 : Vector[H1] 
    LET W_enc2μ : Matrix[Z, H1]
    LET b_enc2μ : Vector[Z]
    LET W_enc2σ : Matrix[Z, H1] 
    LET b_enc2σ : Vector[Z]
    LET W_dec1 : Matrix[H2, Z]
    LET b_dec1 : Vector[H2]
    LET W_dec2 : Matrix[D, H2]
    LET b_dec2 : Vector[D]
      
    DEF Encoder : Encoder = LAMBDA (x) . {
      LET h = ReLU(W_enc1 · x + b_enc1)
      LET μ = W_enc2μ · h + b_enc2μ
      LET σ = SoftPlus(W_enc2σ · h + b_enc2σ)
      RETURN (μ, σ)
    }

    DEF Decoder : Decoder = LAMBDA (z) . {
      LET h = ReLU(W_dec1 · z + b_dec1)  
      LET x' = Sigmoid(W_dec2 · h + b_dec2)
      RETURN x'
    } 

    LET InitParams = (W_enc1, b_enc1, W_enc2μ, b_enc2μ, 
                      W_enc2σ, b_enc2σ, W_dec1, b_dec1, 
                      W_dec2, b_dec2)
      
    FUNC Train(data : List[Vector], lr : Real, steps : Nat)
              -> (Encoder, Decoder) = {
      TrainVAE(Encoder, Decoder, data, lr, steps)            
    }
  }

  PROOFS {
    THEOREM VaeLossIsElbo {
      STATEMENT:
        ∀ (enc : Encoder, dec : Decoder, x : Vector) .
          IsEncoder(enc) ∧ IsDecoder(dec) 
          => L(x; enc, dec) = ELBO(x; enc, dec)  

      PROOF:
        LET enc : Encoder, dec : Decoder, x : Vector
        ASSUME (H1) : IsEncoder(enc), (H2) : IsDecoder(dec)
        
        LET D, Z = CHOOSE (D, Z) . enc : Vector[D] -> Gaussian[Z]
                                   dec : Vector[Z] -> Vector[D] BY H1, H2

        LET (μ, σ) = enc(x), q(z|x) = Gaussian(μ, σ)

        REWRITE L(x; enc, dec) 
          = 𝔼_q[log p(x|z)] - KL(q(z|x) || p(z))  BY VaeLoss
          = ∫ q(z|x) log p(x|z) dz - KL(q(z|x) || p(z)) 
        REWRITE 
          = ∫ q(z|x) log (p(x|z) p(z) / q(z|x)) dz
          = ∫ q(z|x) (log p(x,z) - log q(z|x)) dz
          = 𝔼_q[log p(x,z)] + Entropy(q(z|x))  
          = ELBO(x; enc, dec)    BY ELBO DEF
           
        QED
    }

    THEOREM VaeApproximatesPosterior {
      STATEMENT:  
        ∀ (enc : Encoder, dec : Decoder) (dataset : List[Vector]) .
          IsEncoder(enc) ∧ IsDecoder(dec) 
          => TrainVAE(enc, dec, dataset, _, _) 
             converges to (enc', dec') 
             such that ∀ (x : Vector) . x ∈ dataset 
               => KL(enc'(x) || p(z|x)) is minimized
                  
      PROOF:
        LET enc : Encoder, dec : Decoder, dataset : List[Vector]
        ASSUME (H1) : IsEncoder(enc), (H2) : IsDecoder(dec)

        LET (enc', dec') = TrainVAE(enc, dec, dataset, _, _)
        
        LET x : Vector, ASSUME (H3) : x ∈ dataset

        SUFFICES_TO_SHOW KL(enc'(x) || p(z|x)) is minimized
        
        REWRITE p(z|x) = p(z,x) / p(x) = p(x|z) p(z) / p(x) BY Bayes Rule  

        HAVE (S1) : enc'(x) approximates p(z|x) BY {
          REWRITE TrainVAE minimizes VaeLoss(enc', dec')(x)  
          REWRITE VaeLoss(enc', dec')(x) 
            = -𝔼_enc'(x)[log p(x|z)] + KL(enc'(x) || p(z)) BY VaeLoss DEF
          REWRITE 
            = -∫ enc'(x)(z) log p(x|z) dz + KL(enc'(x) || p(z))
            = -∫ enc'(x)(z) log (p(z|x) p(x) / p(z)) dz + KL(enc'(x) || p(z))
            = -∫ enc'(x)(z) (log p(z|x) + log p(x) - log p(z)) dz + KL(enc'(x) || p(z))  
            = -∫ enc'(x)(z) log p(z|x) dz - log p(x) + KL(enc'(x) || p(z))
            = KL(enc'(x) || p(z|x)) - log p(x) + KL(enc'(x) || p(z))
          
          HENCE minimizing VaeLoss(enc', dec')(x) minimizes KL(enc'(x) || p(z|x))
        }

        QED  
    }

    THEOREM VaeGeneratesNewSamples {
      STATEMENT:
        ∀ (enc : Encoder, dec : Decoder) (dataset : List[Vector]) .
          IsEncoder(enc) ∧ IsDecoder(dec)
          => LET (enc', dec') = TrainVAE(enc, dec, dataset, _, _) IN
               ∀ (z : Vector[Z]) . (dec'(z) generates new samples 
                                    similar to those in dataset) 

      PROOF:
        LET enc : Encoder, dec : Decoder, dataset : List[Vector]  
        ASSUME (H1) : IsEncoder(enc), (H2) : IsDecoder(dec)
        
        LET (enc', dec') = TrainVAE(enc, dec, dataset, _, _), 
            z : Vector[Z]

        SUFFICES_TO_SHOW dec'(z) generates samples similar to dataset

        HAVE (S1) : ∀ (x : Vector) . x ∈ dataset => enc'(x) approximates p(z|x) 
          BY VaeApproximatesPosterior

        LET x' = dec'(z)
        
        REWRITE p(x') 
          = ∫ p(x'|z) p(z) dz
          ≈ ∫ p(x'|z) enc'(x)(z) dz
        REWRITE 
          ≈ ∫ p(x'|z) (∫ p(z|x) p(data)(x) dx) dz    BY Total Probability
          = ∫∫ p(x'|z) p(z|x) p(data)(x) dx dz
          = ∫ p(data)(x) (∫ p(x'|z) p(z|x) dz) dx
          = ∫ p(data)(x) p(x'|x) dx    BY Conditional Probability
          = 𝔼_p(data)[p(x'|x)]

        HENCE x' is likely under p(data) if p(x'|x) is high for likely x
        
        HAVE (S2) : ∀ (x : Vector) . x ∈ dataset => dec'(enc'(x)) approximates x 
          BY {
            LET x : Vector, ASSUME (H4) : x ∈ dataset
            
            REWRITE TrainVAE minimizes VaeLoss(enc', dec')(x)
            REWRITE VaeLoss(enc', dec')(x)
              = -𝔼_enc'(x)[log p(x|z)] + KL(enc'(x) || p(z))    BY VaeLoss DEF
            
            SUFFICES_TO_SHOW dec'(enc'(x)) maximizes 𝔼_enc'(x)[log p(x|z)]
            
            REWRITE 𝔼_enc'(x)[log p(x|z)]  
              = ∫ enc'(x)(z) log p(x|z) dz
              ≈ ∑_i log p(x|z_i)    for z_i ~ enc'(x), by Monte Carlo
              = ∑_i LogLikelihood(x, dec'(z_i))
            
            HENCE maximizing 𝔼_enc'(x)[log p(x|z)] maximizes likelihood of x 
        }

        LET x : Vector, ASSUME (H5) : x ∈ dataset

        HAVE (S3) : p(x'|x) is high BY {
          REWRITE p(x'|x) 
            = ∫ p(x'|z) p(z|x) dz    BY Total Probability
            ≈ ∫ p(x'|z) enc'(x)(z) dz    BY S1
            ≈ p(x'|enc'(x))    BY enc'(x) concentrates around its mean
            ≈ p(dec'(enc'(x)))
            ≈ p(x)    BY S2
            is high    BY H5
        }

        THUS dec'(z) generates samples similar to dataset BY S3, CONCLUSION OF S2
        
        QED
    }

    THEOREM VaeIsConsistentEstimator {
      STATEMENT:
        ∀ (D Z : Nat) (enc : Encoder[D, Z]) (dec : Decoder[Z, D]) 
          (dataset : List[Vector[D]]) .
            IsEncoder(enc) ∧ IsDecoder(dec) 
            => LET (enc', dec') = TrainVAE(enc, dec, dataset, _, _) IN
                 LimitsTo(enc', True_Posterior[D, Z]) 
                 ∧ LimitsTo(EncDec(enc', dec'), True_Data_Distribution[D])
                 as Size(dataset) -> ∞

      PROOF:
        LET D, Z : Nat, enc : Encoder[D, Z], dec : Decoder[Z, D], 
            dataset : List[Vector[D]]
        ASSUME (H1) : IsEncoder(enc), (H2) : IsDecoder(dec)

        LET (enc', dec') = TrainVAE(enc, dec, dataset, _, _)

        SUFFICES_TO_SHOW
          (S1) : LimitsTo(enc', True_Posterior[D, Z]),
          (S2) : LimitsTo(EncDec(enc', dec'), True_Data_Distribution[D])
            as Size(dataset) -> ∞

        SHOW S1 BY {
          LET x : Vector[D]  
          
          REWRITE VaeLoss(enc', dec')(x)  
            = -𝔼_enc'(x)[LogLikelihood(x, dec'(z))]
              + KLDivergence(enc'(x) || p(z))    BY VaeLoss DEF
            = -∫ enc'(x)(z) log p(x|z) dz
              + KLDivergence(enc'(x) || p(z))
            = -∫ enc'(x)(z) log (p(z|x) p(x) / p(z)) dz
              + KLDivergence(enc'(x) || p(z))  
            = -∫ enc'(x)(z) (log p(z|x) + log p(x) - log p(z)) dz
              + KLDivergence(enc'(x) || p(z))
            = -∫ enc'(x)(z) log p(z|x) dz
              - log p(x) 
              + KLDivergence(enc'(x) || p(z))
            = KLDivergence(enc'(x) || p(z|x)) 
              - log p(x)
              + KLDivergence(enc'(x) || p(z))

          REWRITE TrainVAE minimizes Mean(Map(VaeLoss(enc', dec'), dataset))  
          
          HENCE as Size(dataset) -> ∞  
            KLDivergence(enc'(x) || p(z|x)) is minimized ∀ x
          
          THUS enc'(x) approximates p(z|x) ∀ x as Size(dataset) -> ∞
        }

        SHOW S2 BY {
          LET x' : Vector[D]

          REWRITE p(x')
            = ∫ p(x') p(data)(x) dx
            = ∫ p(x') (∫ p(data|z) p(z) dz) dx    BY Total Probability  
            = ∫∫ p(x'|z) p(data|z) p(z) dz dx
            = ∫ p(z) (∫ p(x'|z) p(data|z) dx) dz
            = 𝔼_p(z)[∫ p(x'|z) p(data|z) dx]

          HAVE (S2a) : 
            ∀ (z : Vector[Z]) . 
              dec'(z) approximates ∫ p(x'|z) p(data|z) dx
                as Size(dataset) -> ∞
            BY {
              LET z : Vector[Z]
              LET enc_z : Encoder[D, Z] = LAMBDA(x) . Gaussian(z, I)
              LET (_, dec_z) = TrainVAE(enc_z, dec, dataset, _, _)
              
              REWRITE 
                TrainVAE minimizes -𝔼_enc_z(x)[LogLikelihood(x, dec_z(z))]
                ∀ x in dataset  
                BY VaeLoss with KL term = 0 since enc_z ignores input

              HENCE dec_z(z) maximizes 𝔼_p(data|z)[LogLikelihood(x, dec_z(z))] 
                as Size(dataset) -> ∞
              
              THUS dec_z(z) approximates ∫ p(x'|z) p(data|z) dx
                as Size(dataset) -> ∞
                since p(x|z) = p(data|z) for x in dataset

              HAVE dec_z approximates dec' as Size(dataset) -> ∞ 
                BY S1 since KL(enc_z(x) || p(z|x)) = 0 ∀ x
              
              HENCE thesis  
            }

          REWRITE  
            p(x') ≈ 𝔼_p(z)[dec'(z)]    as Size(dataset) -> ∞, BY S2a
                 ≈ ∫ dec'(z) p(z) dz
                 ≈ ∫ dec'(z) enc'(x)(z) dz    for any x, BY S1
                 ≈ ∫ p(x'|z) p(data)(x) dx    for any x, BY S2a
                 = p(data)(x')    BY Total Probability
        }
          
        QED
    }
  }
}



This iteration of the `VariationalAutoEncoder` concept includes several key components:

1. The `LANGUAGE` block defines the basic types and functions used in the concept, including tensors, vectors, matrices, Gaussian distributions, encoders, decoders, and loss functions.

2. The `NOTATION` block introduces convenient notation for common expressions, such as the KL divergence between the approximate and true posteriors, the expectation under the approximate posterior, the log-likelihood of the data given the latent variables, and the variational lower bound (ELBO).

3. The `TRANSFORMERS` block defines the key components of the VAE:
   - The `SampleGaussian` tactic for sampling from a Gaussian distribution.
   - The `EncDec` function that composes the encoder and decoder to reconstruct the input.
   - The `VaeLoss` function that defines the VAE objective as the negative ELBO.
   - The `TrainVAE` tactic that performs gradient descent to optimize the VAE parameters.

4. The `STRUCTURE` block defines a specific instance of a VAE, the `FullyConnectedVAE`, parameterized by the dimensions of the input, latent space, and hidden layers. It specifies the encoder and decoder as fully connected neural networks with ReLU activations.

5. The `PROOFS` block contains three theorems about VAEs:
   - `VaeLossIsElbo`: The VAE loss is equal to the negative ELBO.
   - `VaeApproximatesPosterior`: A trained VAE approximates the true posterior distribution over the latent variables.
   - `VaeGeneratesNewSamples`: A trained VAE can generate new samples similar to the training data.
   - `VaeIsConsistentEstimator`: As the size of the training dataset goes to infinity, the VAE converges to the true posterior and data distributions.

The proofs make use of the notation and transformers defined earlier, and rely on standard results from probability theory and variational inference. They demonstrate the key properties of VAEs: that they provide a tractable approximation to the true posterior, that they can generate new data by sampling from the latent space, and that they are consistent estimators of the true distributions in the limit of infinite data.

Overall, this formulation provides a rigorous and comprehensive description of the VAE architecture and its theoretical properties, expressed in the formal language of ConceptScript. It showcases the use of the `LANGUAGE`, `NOTATION`, `TRANSFORMERS`, `STRUCTURE`, and `PROOFS` blocks, and demonstrates the power of the language for specifying and reasoning about complex machine learning concepts.