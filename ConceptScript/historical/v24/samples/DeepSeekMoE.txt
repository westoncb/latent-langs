CONCEPT DeepSeekMoE {
  LANGUAGE {
    TYPE Token
    TYPE Expert  
    TYPE SharedExpert <: Expert
    TYPE RoutedExpert <: Expert
    TYPE Embedding = Vector[Real, d]  
    TYPE FFN = Embedding -> Embedding
    TYPE Optimizer = (
      lr : Real, 
      beta1 : Real,
      beta2 : Real, 
      eps : Real,
      weight_decay : Real,
      Params  
    ) -> Params
    TYPE Corpus
    
    CONST NumParams : Nat
    CONST NumLayers : Nat  
    CONST NumExperts : Nat
    CONST NumSharedExperts : Nat 
    CONST NumRoutedExperts : Nat
    CONST HiddenDimension : Nat
    CONST NumAttentionHeads : Nat
    CONST RoutedExpertsPerToken : Nat
    CONST RelativeExpertSize : Real  
    CONST SequenceLength : Nat
    CONST BatchSize : Nat
    CONST InitStdDev : Real 
    CONST ActivationFactor : Real
    
    FUNC Tokenize : Corpus -> List(Token)
    FUNC TrainTokenizer : Corpus -> Tokenizer  
    FUNC SampleTrainingData : Corpus -> (Nat, List(List(Token)))
    
    FUNC Initialize : Params  
    FUNC Train : (Corpus, Params) -> Params
    
    FUNC Router : Token -> List(RoutedExpert)
    FUNC Eval : Expert -> Embedding -> Embedding
    FUNC Combine : List(Embedding) -> Embedding  
    
    FUNC ExpertLoss : (List(Real), List(Real)) -> Real
    FUNC DeviceLoss : (List(Real), List(Real)) -> Real
    
    PRED Specialized : Expert -> Bool
    
    AXIOM LoadBalanceLemma {  
      âˆ€ (t : Token) (experts : List(RoutedExpert)).
        Uniform(Map(Expert -> Real, e -> P(Route(t) = e), experts))
    }
      
    NOTATION "ğ‘" = NumExperts  
    NOTATION "ğ¾ğ‘ " = NumSharedExperts
    NOTATION "ğ‘š" = ActivationFactor
    NOTATION "ğ¾" = NumRoutedExperts
    NOTATION "ğ‘‘" = HiddenDimension  
  }

  STRUCTURE TrainedModel {
    REQUIRE NumParams = NumLayers * (  
      12 * HiddenDimension^2 + 
      13 * HiddenDimension +
      (NumLayers - 1) * (
        NumSharedExperts * RelativeExpertSize * 8 * HiddenDimension^2 + 
        NumRoutedExperts * RelativeExpertSize * 8 * HiddenDimension^2
      )  
    )
    
    REQUIRE NumExperts = NumSharedExperts + NumRoutedExperts
    
    REQUIRE âˆ€ (layer : 1..NumLayers - 1). {  
      NumSharedExperts(layer) = NumSharedExperts
      NumRoutedExperts(layer) = NumRoutedExperts
      âˆ€ (expert : Expert) âˆˆ layer.
        Dimension(expert) = 4 * RelativeExpertSize * HiddenDimension  
    }
      
    REQUIRE PERFORMANCE(
      Accuracy(DeepSeekMoE(2B), Benchmarks) > Accuracy(GShard(2B), Benchmarks),  
      Accuracy(DeepSeekMoE(2B), Benchmarks) â‰ˆ Accuracy(GShard(2.9B), Benchmarks),
      Accuracy(DeepSeekMoE(2B), Benchmarks) â‰ˆ Accuracy(Dense(2B), Benchmarks) 
    )
        
    REQUIRE PERFORMANCE(
      Accuracy(DeepSeekMoE(16B), Benchmarks) â‰ˆ Accuracy(DeepSeek(7B), Benchmarks),  
      Accuracy(DeepSeekMoE(16B), Benchmarks) â‰ˆ Accuracy(LLaMA2(7B), Benchmarks)
    )
      
    REQUIRE PERFORMANCE(  
      Accuracy(DeepSeekMoE(145B), Benchmarks) > Accuracy(GShard(137B), Benchmarks),
      Accuracy(DeepSeekMoE(145B), Benchmarks) â‰ˆ Accuracy(DeepSeek(67B), Benchmarks)
    )  
      
    REQUIRE (NumParams < 16.5B) AND (  
      ActivatedParams = 
        NumSharedExperts * 8 * RelativeExpertSize * HiddenDimension^2 +
        RoutedExpertsPerToken * 8 * RelativeExpertSize * HiddenDimension^2
        < 3B  
    )
  }
   
  STRUCTURE TrainingProcess {
    DEF Tokenizer : Tokenizer = TrainTokenizer(Corpus)
      
    DEF Vocab : Nat = MATCH NumParams {  
      2B -> 8K
      16B -> 100K
      145B -> 100K  
    }
       
    DEF (NumTokens, TokenizedData) : (Nat, List(List(Token))) = 
      SampleTrainingData(Corpus)
        WHERE MATCH NumParams {
          2B -> NumTokens = 100B  
          16B -> NumTokens = 2T
          145B -> NumTokens = 245B
        }  
         
    DEF InitParams : Params = Initialize(Params) WHERE InitStdDev = 0.006
      
    DEF TrainedParams : Params = Train(
      TokenizedData,
      InitParams  
    ) WHERE {
      MATCH NumParams {  
        2B -> {
          Optimizer = (  
            lr = 1.08e-3, beta1 = 0.9, beta2 = 0.95,
            eps = 1e-8, weight_decay = 0.1  
          )
          SequenceLength = 2K  
          BatchSize = 2K
          NumSteps = 25K
          AlphaExpBal = 0.01  
        }
        16B -> {
          Optimizer = (  
            lr = 4.2e-4, beta1 = 0.9, beta2 = 0.95,
            eps = 1e-8, weight_decay = 0.1
          )  
          SequenceLength = 4K
          BatchSize = 4.5K  
          NumSteps = 106449
          AlphaExpBal = 0.001
        }  
        145B -> {
          Optimizer = (
            lr = 3.0e-4, beta1 = 0.9, beta2 = 0.95,  
            eps = 1e-8, weight_decay = 0.1
          )
          SequenceLength = 4K  
          BatchSize = 4.5K
          NumSteps = 13K  
          AlphaExpBal = 0.003
          AlphaDevBal = 0.05  
        }
      }
        
      REQUIRE âˆ€ (step : 1..NumSteps).
        LET lr_scale = MATCH step {  
          IN 1..2K -> step / 2K
          IN 80%*NumSteps+1..90%*NumSteps -> 0.316  
          ELSE -> IF NumParams = 145B THEN 1 ELSE 0.1
        }  
        IN
        Optimizer.lr(step) = lr_scale * Optimizer.lr  
    }
  }
    
  STRUCTURE MoE(shared_experts, routed_experts) : FFN {
    REQUIRE NumExperts(shared_experts ++ routed_experts) = ğ‘  
    REQUIRE NumSharedExperts(shared_experts) = ğ¾ğ‘ 
    REQUIRE NumRoutedExperts(routed_experts) = ğ‘š * (ğ‘ - ğ¾ğ‘ )  
    REQUIRE âˆ€ (e : Expert) âˆˆ routed_experts. Dimension(e) = ğ‘‘ / ğ‘š
      
    FUNC Forward(input : List(Token)) -> List(Embedding) = {  
      LET activated_shared = 
        Map(t -> MapReduce(Eval, Combine, shared_experts, t), input)
        
      LET activated_routed =
        FlatMap(t -> Map(Eval(t), Router(t)), input)  
        
      Map(Combine, Zip(activated_shared, activated_routed))
    }  
  }

  FUNC Router(t : Token) -> List(RoutedExpert) =  
    Topk(Map(e -> Softmax(DotProduct(e, t)), routed_experts), ğ‘šğ¾ - ğ¾ğ‘ )
    
  FUNC ExpertLoss(frequencies, probabilities) : Real =  
    ğ›¼_exp * Sum(Zip(frequencies, probabilities))
      
  FUNC DeviceLoss(frequencies, probabilities) : Real =
    ğ›¼_dev * Sum(Map(Combine, Zip(frequencies, probabilities)))  

  PROOFS {  
    THEOREM ExpertSpecialization {
      STATEMENT: âˆ€ (input : List(Token)).  
        LET routed_experts = Drop(ğ¾ğ‘ , MoE.experts) IN
        âˆ€ (e : RoutedExpert) âˆˆ Activate(input, routed_experts).  
          Specialized(e)
            
      <PROOF>  
        LET N = NumRoutedExperts,
            K = RoutedExpertsPerToken,  
            old_N = N / 4,
            old_K = 2 IN {  
          NumCombinations(N, K)
            > NumCombinations(old_N, old_K) BY CombinatoricsLemma  
          HENCE HigherSpecialization(routed_experts)  
        }
      </PROOF>  
    }

    THEOREM ComputationEfficiency {  
      STATEMENT: âˆ€ (input : List(Token)).
        ComputationCost(DeepSeekMoE, input) â‰ˆ 40% * ComputationCost(DenseModel, input)  
          
      <PROOF>
        MATCH NumParams {  
          16B -> {
            ActivatedParams(DeepSeekMoE)  
              = NumSharedExperts * 8 * RelativeExpertSize * HiddenDimension^2 +
                RoutedExpertsPerToken * 8 * RelativeExpertSize * HiddenDimension^2  
              â‰ˆ (2 * 4 + 6 * 4) * HiddenDimension^2
              â‰ˆ 32 * HiddenDimension^2  
                  
            TotalParams(DenseModel)
              â‰ˆ 12 * NumLayers * HiddenDimension^2  
              â‰ˆ 12 * 28 * HiddenDimension^2
              â‰ˆ 80 * HiddenDimension^2  
                
            PROVE  
              ComputationCost(DeepSeekMoE, input) / ComputationCost(DenseModel, input)
                â‰ˆ ActivatedParams(DeepSeekMoE) / TotalParams(DenseModel)  
                â‰ˆ 40%  
          }
            
          145B -> {  
            ActivatedParams(DeepSeekMoE)
              â‰ˆ (4 * 2 + 12 * 2) * HiddenDimension^2  
              â‰ˆ 32 * HiddenDimension^2
                
            ActivatedParams(DeepSeekMoE, HalfRouted)  
              â‰ˆ (2 * 2 + 6 * 2) * HiddenDimension^2
              â‰ˆ 16 * HiddenDimension^2  
                
            TotalParams(DenseModel)  
              â‰ˆ 12 * 62 * HiddenDimension^2
              â‰ˆ 176 * HiddenDimension^2  
                
            PROVE  
              ComputationCost(DeepSeekMoE, input) / ComputationCost(DenseModel, input)
                â‰ˆ 32 / 176 â‰ˆ 18.2%
                  
              ComputationCost(DeepSeekMoE, HalfRouted, input) / ComputationCost(DenseModel, input)    
                â‰ˆ 16 / 176 â‰ˆ 9.1%  
          }
        }  
      </PROOF>
    }
      
    THEOREM ParameterRedundancy {
      STATEMENT: âˆ€ (ratio : Real).
        LET accuracy_drop(model, ratio) =   
              Accuracy(model, full_experts) - Accuracy(model, (1-ratio) * full_experts) IN  
        accuracy_drop(DeepSeekMoE, ratio) > accuracy_drop(GShard, ratio)
          
      <PROOF>  
        LET shared_pct = NumSharedExperts / NumExperts,  
            routed_pct = 1 - shared_pct IN {
          ASSUME accuracy_drop(GShard, ratio) = routed_pct * ratio    
          ASSUME accuracy_drop(DeepSeekMoE, ratio) =
            shared_pct * 0 + routed_pct * 2 * ratio  
            
          PROVE accuracy_drop(DeepSeekMoE, ratio) > accuracy_drop(GShard, ratio) BY  
            routed_pct * 2 * ratio > routed_pct * ratio
        }    
      </PROOF>
    }  
  }
}