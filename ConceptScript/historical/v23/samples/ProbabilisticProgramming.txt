CONCEPT ProbabilisticProgramming {

  LANGUAGE {
    TYPE Var
    TYPE Value
    TYPE Dist = Var -> ùí´(Value)
    TYPE Model = Var -> (ùí´(Value) -> ùí´(Value)) -> ùí´(Value)
    TYPE Inference = Model -> (Var -> Value) -> ùí´(Value)

    FUNC Expectation[A] : (A -> ‚Ñù) -> ùí´(A) -> ‚Ñù
    FUNC KLDivergence : ùí´(Value) -> ùí´(Value) -> ‚Ñù‚Å∫
    FUNC Entropy : ùí´(Value) -> ‚Ñù‚Å∫
    
    NOTATION "ùîº[X‚ààD] f" = Expectation(D, f)
    NOTATION "D_KL(P ‚à• Q)" = KLDivergence(P, Q)
    NOTATION "H(P)" = Entropy(P)
  }

  STRUCTURE {
    LET X = SampleSpace(Value)
    
    DEF Var = Symbol
    DEF Value = Any
    DEF Dist(v) = Œª x . P(v = x) 
    DEF Model(v)(P) = Œª x . ‚à´ P(v = x | Pa(v) = pa) dP(pa)
    
    DEF VariationalInference(M)(Q) = 
      ArgMin[Q] ùîº[v‚ààM(v)] D_KL(M(v) ‚à• Q(v))
      
    DEF MaximumLikelihood(M)(D) = 
      ArgMax[Œ∏] ‚àè PŒ∏(D)
      
    DEF MaximumAPosteriori(M)(D) =
      ArgMax[Œ∏] P(Œ∏) ‚àè PŒ∏(D)
      
    AXIOM DistributionPositive : ‚àÄ (P : ùí´(X)) . P(x) ‚â• 0
    AXIOM DistributionUnity : ‚àÄ (P : ùí´(X)) . ‚àë P(x) = 1
    
    REWRITE ExpectationLinearity : 
      ùîº[X‚ààD] (a¬∑f + b¬∑g) => a¬∑ùîº[X‚ààD] f + b¬∑ùîº[X‚ààD] g
      
    REWRITE KLDivergencePositive :
      D_KL(P ‚à• Q) => ‚à´ P(x) log (P(x) / Q(x)) dx ‚â• 0
      
    REWRITE EntropySubadditivity : 
      H(P √ó Q) ‚â§ H(P) + H(Q)
  }

  PROOFS {
    THEOREM SoundnessOfVariationalInference {
      STATEMENT : Q·µõ = M -> ‚àÄ v . D_KL(M(v) ‚à• Q·µõ(v)) ‚â§ D_KL(M(v) ‚à• Q(v))
      
      PROOF {
        LET Q·µõ = VariationalInference(M)
        
        ASSUME [v : Var] [Q : Var -> ùí´(Value)] : 
          ùîº[x‚ààQ·µõ(v)] log (Q·µõ(v, x) / M(v, x)) ‚â§ 
          ùîº[x‚ààQ(v)]  log (Q(v, x)  / M(v, x))

        REWRITE KLDivergencePositive
        REWRITE ExpectationLinearity
        
        SHOW D_KL(M(v) ‚à• Q·µõ(v)) ‚â§ D_KL(M(v) ‚à• Q(v))
      }
    }

    THEOREM MaximumLikelihoodIsMAP[n : ‚Ñï] {
      STATEMENT : 
        ‚àÄ (M : Model) (D : Value^n) . 
        MaximumLikelihood(M)(D) = MaximumAPosteriori(M)(D) 
        ASSUMING P(Œ∏) = Uniform
      
      PROOF {
        ASSUME [M : Model] [D : Value^n] : P(Œ∏) = Uniform
        
        HAVE ArgMax[Œ∏] ‚àè PŒ∏(D) <-> ArgMax[Œ∏] log ‚àè PŒ∏(D)
          BY MonotonicityOfLog
        
        REWRITE log ‚àè PŒ∏(D) = ‚àë log PŒ∏(D)

        HAVE ArgMax[Œ∏] P(Œ∏) ‚àè PŒ∏(D) <-> ArgMax[Œ∏] log P(Œ∏) + ‚àë log PŒ∏(D)
          BY MonotonicityOfLog  
          
        REWRITE log P(Œ∏) = -log |Œò| ASSUMING P(Œ∏) = Uniform
        
        HAVE ArgMax[Œ∏] log P(Œ∏) + ‚àë log PŒ∏(D) <-> ArgMax[Œ∏] ‚àë log PŒ∏(D)
          BY ConstantTermsDoNotAffectArgMax
                
        SHOW MaximumLikelihood(M)(D) = MaximumAPosteriori(M)(D)
      }
    }

    THEOREM EntropyBoundsMutualInformation {
      STATEMENT : I(X; Y) ‚â§ min {H(X), H(Y)}
      
      PROOF {
        REWRITE I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
        
        HAVE H(X|Y) ‚â• 0 AND H(Y|X) ‚â• 0  
          BY NonNegativityOfEntropy
          
        HENCE H(X) ‚â• I(X; Y) AND H(Y) ‚â• I(X; Y)
        
        SHOW I(X; Y) ‚â§ min {H(X), H(Y)}
      }
    }
  }
  
  CONCEPT BayesianNetwork EXTENDS ProbabilisticProgramming {
    LANGUAGE {
      TYPE DAG = (ùí´(Var), Var -> ùí´(Var))
      FUNC Ancestors : DAG -> Var -> ùí´(Var)
      FUNC MarkovBlanket : DAG -> Var -> ùí´(Var)
    }
    
    STRUCTURE {
      DEF Ancestors((V, E))(v) = { w ‚àà V | Path(w, v) ‚àà E* }  
      DEF MarkovBlanket((V, E))(v) = Parents(v) ‚à™ Children(v) ‚à™ Spouses(v)
      
      AXIOM DAGIsAcyclic : ‚àÄ ((V, E) : DAG) . ‚àÄ v ‚àà V . v ‚àâ Ancestors((V, E))(v)
      
      DEF BayesianNetwork : DAG -> Model
      BayesianNetwork((V, E)) = Œª v . ‚àè Pv|Pa(v)
      
      REWRITE IndependenceInBN : 
        (v ‚ä• NonDescendants(v) | Parents(v))
        IN BayesianNetwork((V, E))
    }
    
    PROOFS {
      THEOREM EquivalenceUnderFactorization {
        STATEMENT : 
          ‚àÄ ((V, E) : DAG) .
          JPD(BayesianNetwork((V, E))) =
          ‚àè Pv|An(v) =
          ‚àè Pv|Pa(v)

        PROOF {        
          LET ((V, E) : DAG), (G : BayesianNetwork((V, E)))
          
          HAVE ‚àÄ v ‚àà V . Pv|An(v) = Pv|Pa(v) {
            ASSUME [v ‚àà V]
            
            LET NonDescendants(v) = V \ ({v} ‚à™ Descendants(v))
            
            HAVE v ‚ä• NonDescendants(v) | Parents(v) IN G 
              BY IndependenceInBN
            
            HAVE Ancestors(v) = Parents(v) ‚à™ NonDescendants(v)
            
            HAVE Pv|An(v) = Pv|Pa(v) BY ConditionalIndependence
          }
            
          SHOW JPD(G) = ‚àè Pv|An(v) = ‚àè Pv|Pa(v) BY ChainRule, INDUCTION
        }
      }

      THEOREM MarkovEquivalence {
        STATEMENT :
          ‚àÄ ((V, E1) : DAG) ((V, E2) : DAG) .
          BayesianNetwork((V, E1)) = BayesianNetwork((V, E2))
          <-> SameSkeletonSameV((V, E1), (V, E2))

        PROOF {
          ASSUME [(V, E1) : DAG] [(V, E2) : DAG] {
            BayesianNetwork((V, E1)) = BayesianNetwork((V, E2))
          }
          
          LET G1 = BayesianNetwork((V, E1)), G2 = BayesianNetwork((V, E2))
          
          HAVE ‚àÄ v ‚àà V . MarkovBlanket(G1, v) = MarkovBlanket(G2, v) {
            ASSUME [v ‚àà V] 
            
            HAVE Pv|V\{v} SAME IN G1, G2 BY ASSUMPTION
            HENCE MarkovBlanket(G1, v) = MarkovBlanket(G2, v) 
          }
           
          HENCE ‚àÄ v w ‚àà V . AdjacentOrSpouses(v, w) IN G1 <-> AdjacentOrSpouses(v, w) IN G2
          HENCE Skeleton(G1) = Skeleton(G2) AND ImmoralsMatch(G1, G2)
          
          LET (R, F) = CPDAG(G1)
          
          HAVE (V, E1) IN PerfectMaps((R, F)) AND (V, E2) IN PerfectMaps((R, F))
          SHOW SameSkeletonSameV((V, E1), (V, E2))
        }
      }
    }
  }
}