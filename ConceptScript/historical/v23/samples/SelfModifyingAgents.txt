CONCEPT SelfModifyingAgents {

  LANGUAGE {
    TYPE Agent
    TYPE Environment
    TYPE Reward = â„
    TYPE ActionSpace
    TYPE ObservationSpace
    TYPE BeliefState
    
    FUNC Act : Agent -> Environment -> ActionSpace  
    FUNC Observe : Agent -> Environment -> ObservationSpace
    FUNC Modify : Agent -> Agent
    FUNC Reward : Agent -> Environment -> â„
    
    PRED GoalAchieved : Agent -> Environment -> ğ”¹
    
    NOTATION "âŸ¨BeliefâŸ©" = BeliefState
    NOTATION "a â† Ï€(âŸ¨bâŸ©)" = ActionChosenByPolicy(Agent, BeliefState)
    NOTATION "âŸ¨bâŸ© â† SE(âŸ¨bâŸ©, o, a)" = StateEstimationUpdate(BeliefState, Observation, Action) 
    NOTATION "U(âŸ¨bâŸ©, ğœ‹)" = UtilityOfPolicyInBelief(BeliefState, Policy)
  }
  
  STRUCTURE {
    DEF UtilityOfPolicy(âŸ¨bâŸ©, ğœ‹) = ğ”¼â‚~ğœ‹,â‚›~âŸ¨bâŸ©[ âˆ‘áµ— ğ›¾áµ—â‹…Reward(sâ‚œ, aâ‚œ) ]
    
    DEF OptimalPolicy(âŸ¨bâŸ©) = argmax {Ï€} U(âŸ¨bâŸ©, ğœ‹)
    
    DEF OptimalValue(âŸ¨bâŸ©) = max {Ï€} U(âŸ¨bâŸ©, ğœ‹)
    
    AXIOM RewardHypothesis : 
      âˆ€ (A : Agent) (E : Environment) . GoalAchieved(A, E) <-> âˆƒ (R* : â„) . âˆ€ (R' < R*) (t > T) . Reward(A, E, t) = R' 
    
    AXIOM AgentImprovability :
      âˆ€ (A : Agent) . âˆƒ (A' : Agent) . âˆ€ (E : Environment) . ExpectedReward(A', E) > ExpectedReward(A, E)
      
    DEF SelfModifyingAgent = 
      Î» (A : Agent) .
        REPEAT UNTIL GoalAchieved(A, E) {
          âŸ¨bâŸ© := InitialBeliefState
          
          REPEAT {
            a â† OptimalPolicy(âŸ¨bâŸ©)
            o â† Observe(E, a)
            r â† Reward(E, a)
            âŸ¨bâŸ© â† SE(âŸ¨bâŸ©, o, a) 
          }
          
          A := Modify(A) WHERE âˆ€ (E : Environment) . ExpectedReward(Modify(A), E) > ExpectedReward(A, E)
        }

    CONJECTURE SelfImprovement :
      âˆ€ (Aâ‚€ : Agent) (E : Environment) . âˆƒ (k : â„•) . GoalAchieved(SelfModifyingAgent^k(Aâ‚€), E)
  }
  
  PROOFS {
    THEOREM IncrementalImprovement {
      STATEMENT : âˆ€ (A : Agent) . Â¬GoalAchieved(A, E) -> 
        ExpectedReward(SelfModifyingAgent(A), E) > ExpectedReward(A, E)
        
      PROOF {
        ASSUME [A : Agent] : Â¬GoalAchieved(A, E)
        
        LET âŸ¨bâŸ© = InitialBeliefState
        
        HAVE âŸ¨b'âŸ©, r_SMA : (âŸ¨b'âŸ©, r_SMA) = RESULT_BELIEF_REWARD(SelfModifyingAgent(A), âŸ¨bâŸ©)
        
        HAVE : r_SMA = max {a} âˆ‘â‚› p(s|âŸ¨bâŸ©)â‹…âˆ‘â‚’ p(o|s,a)â‹…(r(s,a) + ğ›¾â‹…max{a'}âˆ‘â‚›'p(s'|s,a)â‹…V*(âŸ¨SE(b,o,a)âŸ©))
          BY BELLMAN_OPTIMALITY
        
        REWRITE ActionChosenByPolicy, UtilityOfPolicy
        
        HENCE r_SMA = U(âŸ¨bâŸ©, OptimalPolicy(âŸ¨bâŸ©))
        
        HAVE A' : A' = Modify(A) WHERE âˆ€ (E : Environment) . ExpectedReward(A', E) > ExpectedReward(A, E) 
          BY AgentImprovability
          
        HENCE âŸ¨b_AâŸ©, r_A : (âŸ¨b_AâŸ©, r_A) = RESULT_BELIEF_REWARD(A, âŸ¨bâŸ©) AND
              âŸ¨b_A'âŸ©, r_A' : (âŸ¨b_A'âŸ©, r_A') = RESULT_BELIEF_REWARD(A', âŸ¨bâŸ©)
                
        HAVE : r_A < r_A'
          BY A' DEFINITION              
                 
        HAVE : r_SMA = U(âŸ¨bâŸ©, OptimalPolicy(âŸ¨bâŸ©)) â‰¥ U(âŸ¨bâŸ©, ğœ‹) FOR ANY ğœ‹
          BY DEFINITION OF OptimalPolicy
          
        REWRITE UtilityOfPolicy 
        
        HENCE r_SMA â‰¥ r_A'
        
        HENCE r_SMA > r_A
        
        REWRITE ExpectedReward
                
        SHOW ExpectedReward(SelfModifyingAgent(A), E) > ExpectedReward(A, E)
      }        
    }
      
    THEOREM AsymptoticOptimality {
      STATEMENT : âˆ€ (Aâ‚€ : Agent) . GoalAchieved(SelfModifyingAgentáµ(Aâ‚€), E) FOR SOME k
      
      PROOF BY CONTRADICTION {
        ASSUME [Aâ‚€ : Agent] [k : â„•] : Â¬GoalAchieved(SelfModifyingAgentáµ(Aâ‚€), E) FOR ALL k
        
        HAVE Aâ‚– = SelfModifyingAgentáµ(Aâ‚€)
        
        HAVE : ExpectedReward(Aâ‚–â‚Šâ‚, E) > ExpectedReward(Aâ‚–, E) FOR ALL k BY IncrementalImprovement
        
        LET R* = sup {k} ExpectedReward(Aâ‚–, E)
        
        HAVE : R* < âˆ BY RewardHypothesis USING Â¬GoalAchieved
        
        LET Îµ > 0
        
        HAVE : âˆƒ (k : â„•) . ExpectedReward(Aâ‚–, E) > R* - Îµ BY DEFINITION OF SUPREMUM
        
        LET k_Îµ BE SUCH THAT ExpectedReward(Aâ‚–â‚‘, E) > R* - Îµ 
        
        HAVE : ExpectedReward(Aâ‚–â‚‘â‚Šâ‚, E) > R* BY IncrementalImprovement
        
        CONTRADICTION         
      }
    }
  }
}