CONCEPT GradientDescent {
  NOTATION {
    âˆ‡f(x) = Gradient of f at x
    x_i = i-th iterate 
    Î±_i = Learning rate at i-th step
    âŸ¨u,vâŸ© = Dot product of vectors u and v
    â€–vâ€– = Euclidean norm of vector v
    ğ”¼[X] = Expected value of random variable X
    ğ•[X] = Variance of random variable X
  }

  LANGUAGE {
    TYPE Vector = [Real]
    TYPE Function = Vector â†’ Real
    TYPE Trajectory = [Vector]

    FUNC GradientStep(f: Function, x: Vector, Î±: Real): Vector =
      x - Î± * âˆ‡f(x)

    FUNC GradientDescent(f: Function, x_0: Vector, 
                         Î±: Real, T: Int): Trajectory =
      GENERATE(T, i => GradientStep(f, x_{i-1}, Î±), x_0)
                    
    PRED StronglyConvex(f: Function, m: Real) = 
      âˆ€ x,y . f(y) â‰¥ f(x) + âŸ¨âˆ‡f(x), y-xâŸ© + (m/2)*â€–y-xâ€–Â²
                    
    PRED LipschitzSmooth(f: Function, L: Real) =
      âˆ€ x,y . â€–âˆ‡f(x) - âˆ‡f(y)â€– â‰¤ L*â€–x-yâ€–
                  
    FUNC OptimalLR(m: Real, L: Real): Real = 2/(m+L) 
                    
    CONST x^*_f = ARGMIN_x f(x)
  }
           
  STRUCTURES {
    STRUCTURE StochasticGradientDescent {
      FIELD f: Function
      FIELD x_0: Vector  
      FIELD Î±: Real
      FIELD T: Int
      FIELD B: Int       # Mini-batch size
      
      COMPUTE Trajectory: Trajectory = GENERATE(T, t => {
        LET g = âˆ‡f(x_t) ESTIMATED BY 
          (1/B) * âˆ‘_{i=1..B} âˆ‡f_i(x_t) on SAMPLE {f_i}  
        GradientStep(f, x_t, Î±) USING g
      }, x_0)
    }
                
    STRUCTURE AcceleratedGradientDescent {
      FIELD f: Function
      FIELD x_0: Vector
      FIELD Î±: Real > 0
      FIELD Î²: Real âˆˆ [0,1) 
      FIELD T : Int
        
      COMPUTE Trajectory : Trajectory = GENERATE(T, t => {
        LET y_t = x_t + Î²*(x_t - x_{t-1})  # Momentum  
        GradientStep(f, y_t, Î±)
      }, x_0)
    }
  }
           
  TRANSFORMERS {  
    REWRITE GradientDescentRecurrence:
      x_{i+1} -> GradientStep(f, x_i, Î±)
    
    REWRITE StepsToRate:
      GradientDescent(f, x_0, Î±, T) 
        -> {x_i}_i WHERE 
             x_{i+1} = GradientStep(f, x_i, 1/âˆši), x_0
             
    REWRITE VarianceReduction:
      ğ•[âˆ‡f(x) ESTIMATED BY (1/B)*âˆ‘ âˆ‡f_i(x)] 
        -> (1/B) * ğ•[âˆ‡f_i(x)]
             
    SIMPLIFY SeparableSum:  
      âˆ‘_i f(a_i, b_i) -> âˆ‘_i f(a_i, â€¢) + âˆ‘_i f(â€¢, b_i)
        IF f separable
  }
   
  PROOFS {
    PROOF GradientDescentConvergence:
      ASSUM StronglyConvex(f, m), LipschitzSmooth(f, L), 
        Î± = OptimalLR(m, L), {x_i} = GradientDescent(f, x_0, Î±, â€¢)
      PROVE f(x_i) - f(x^*_f) â‰¤ (L/2) * â€–x_0 - x^*_fâ€–Â² * (1-m/L)^i
    {
      f(x_i) - f(x^*_f)
        â‰¤ (1/2Î±) * â€–x_i - x^*_fâ€–Â² - (1/2Î±) * â€–x_{i+1} - x^*_fâ€–Â²  
            BY StronglyConvex, LipschitzSmooth, GradientStep
        â‰¤ (L/2) * â€–x_0 - x^*_fâ€–Â² * (1-m/L)^i
            BY INDUCTION, BASE CASE i=0 
      QED
    }   
         
    PROOF StochasticGDConvergence:
      ASSUM StronglyConvex(f, m), {f_i} i.i.d. from ğ’Ÿ_f,
        ğ”¼[âˆ‡f_i(x)] = âˆ‡f(x), ğ•[âˆ‡f_i(x)] â‰¤ ÏƒÂ²,
        {x_i} = StochasticGD(f, x_0, Î±, T, B)
      PROVE ğ”¼[f(x_T)] - f(x^*_f) â‰¤ O(exp(-mÎ±T)) + O(Î±(ÏƒÂ²/B))  
    {
      ğ”¼â€–x_{i+1} - x^*_fâ€–Â²  
        = ğ”¼â€–x_i - Î±*g_i - x^*_fâ€–Â²
        â‰¤ (1-2mÎ±+Î±Â²LÂ²)*ğ”¼â€–x_i - x^*_fâ€–Â² + 2Î±Â²ÏƒÂ²/B  
          BY g_i = (1/B)*âˆ‘ âˆ‡f_j(x_i), ğ”¼â€–g_i-âˆ‡f(x_i)â€–Â² â‰¤ ÏƒÂ²/B   
      â‡’ ğ”¼â€–x_T - x^*_fâ€–Â²     
        â‰¤ (1-2mÎ±+Î±Â²LÂ²)^T * â€–x_0 - x^*_fâ€–Â² + 2Î±(ÏƒÂ²/B)/(2m-Î±LÂ²)
          UNROLLING recursion, BOUNDING sum of geometric series
      â‡’ ğ”¼[f(x_T)] - f(x^*_f) 
        â‰¤ (m/2) * ğ”¼â€–x_T - x^*_fâ€–Â²
        â‰¤ O(exp(-mÎ±T)) + O(Î±(ÏƒÂ²/B))    
          CHOOSING Î± = O(1/âˆšT)
      QED  
    }
  }
       
  EXAMPLES {
    EXAMPLE LogisticRegressionWithSGD:
      LET X: [Real]^{N Ã— d} = DATASET  
      LET Y: [Real]^N = LABELS
      DEFINE f_i(Î¸) = LogLoss(Î¸; (x_i, y_i)) 
        = -y_i*âŸ¨Î¸,x_iâŸ© + log(1 + exp(âŸ¨Î¸,x_iâŸ©))
      LET Trajectory = StochasticGradientDescent(
        f(Î¸) = (1/N)*âˆ‘_i f_i(Î¸),
        x_0 = 0^d, 
        Î± = 0.1/âˆšT,
        T = 10000, 
        B = 100
      ).Trajectory
         
    EXAMPLE AcceleratedGradientForSparseRegression:
      LET X: [Real]^{N Ã— d} = DATASET
      LET Y: [Real]^N = RESPONSE  
      DEFINE f(Î²) = (1/2N)â€–Y - XÎ²â€–Â² + Î»â€–Î²â€–_1
      LET Trajectory = AcceleratedGradientDescent(
        f = f, 
        x_0 = 0^d,
        Î± = 1/â€–Xâ€–Â²,  
        Î² = 0.9,
        T = 1000
      ).Trajectory
  }
}