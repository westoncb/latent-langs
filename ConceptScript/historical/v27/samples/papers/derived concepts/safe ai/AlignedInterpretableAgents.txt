CONCEPT AlignedInterpretableAgents {
  NOTATION {
    ğ’œ = set of agents
    ğ’® = set of states
    ğ’ª = set of observations
    â„‹ = set of humans
    ğ’± = set of values/preferences
    â„›(h) = reward function for human h
    ğœ‹_a = policy of agent a
    Î³_a = interpretability function of agent a
    Î³_a(s, h) = interpretation of state s for human h by agent a
    C(a, h) = compatibility score between agent a and human h
  }

  LANGUAGE {
    TYPE Agent = (Policy, InterpretabilityFunction)
    TYPE Policy = State -> Action  
    TYPE InterpretabilityFunction = (State, Human) -> Interpretation
    TYPE Interpretation = (FeatureVector, Explanation, Visualization)
    TYPE Explanation = String
    TYPE Visualization = Image | Animation | InteractiveInterface
    TYPE CompatibilityScore = â„
    
    FUNC Cooperate(a1: Agent, a2: Agent, s: State): Action =
      LET (ğ’œ, Ï€) = CooperativeEquilibrium(a1, a2, s) IN
      Ï€(a1)
      
    FUNC CooperativeEquilibrium(a1: Agent, a2: Agent, s: State): (Set[Agent], Policy) =
      LET ğ’œ = {a1, a2} IN
      ARGMAX_(Ï€ : ğ’œ -> Action) âˆ‘_{a âˆˆ ğ’œ} Q_Ï€(a, s)  -- Q_Ï€(a,s) = expected future reward for a in s under joint policy Ï€

    FUNC Compose(a1: Agent, a2: Agent): Agent =
      -- Compose policies and interpretations of a1 and a2
      LET Ï€(s) = Cooperate(a1, a2, s),
          Î³(s, h) = Combine(Î³_a1(s, h), Î³_a2(s, h)) IN
      (Ï€, Î³)
      
    FUNC Combine(i1: Interpretation, i2: Interpretation): Interpretation =
      -- Combine feature vectors, explanations, and visualizations of i1 and i2
      (Concat(i1.FeatureVector, i2.FeatureVector), 
       i1.Explanation + "\n" + i2.Explanation,
       Overlay(i1.Visualization, i2.Visualization))

    FUNC Compatibility(a: Agent, h: Human): CompatibilityScore =
      MEAN(Similarity(a.Î³(s,h).FeatureVector, PreferenceEmbedding(â„›(h))) 
           for s âˆˆ ğ’®)

    FUNC PreferenceEmbedding(â„›: Human -> â„): FeatureVector =
      -- Embed human reward function into interpretable feature space  
      ARGMIN_v âˆ‘_h âˆ‘_s (vÂ·InterpretableFeatures(s) - â„›(h)(s))^2 
  }
  
  STRUCTURES {
    STRUCTURE ValueAlignedAgent EXTENDS Agent {
      FIELD H : Set[Human]  -- Humans to align with
      
      INIT(H): 
        SELF.H := H
        SELF.Policy := LBRD(SELF, âˆ‘_(hâˆˆH) â„›(h))
        SELF.InterpretabilityFunction := 
          (s, h) -> (InterpretableFeatures(s), Explanation(Policy(s)), VisualizePlan(s))
          
      FUNC LBRD(self, â„›, k_max=10):
        -- Reward modeling via debate between k_max copies of self
        FOR i = 1..k_max:
          selfi = COPY(self)
          selfi.Policy := BestResponse(selfi, self, â„›)
        RETURN ARGMAX_(a âˆˆ {self, self1, ..., selfk}) ğ”¼_(Ï„~a) â„›(Ï„) 
    }
    
    STRUCTURE CompositionalTeam {
      FIELD Agents : List[ValueAlignedAgent]
      
      FUNC Coordinate(s: State):
        a := Agents[0]
        FOR i = 1..|Agents|-1:
          a := Compose(a, Agents[i])
        RETURN a.Policy(s)
        
      FUNC Explain(s: State, h: Human):
        explanations := []
        FOR a âˆˆ Agents:
          (f, e, v) := a.InterpretabilityFunction(s, h)
          explanations.append(e)
        RETURN JOIN(explanations, "\n\n")
    }
  }
  
  THEOREMS {
    THEOREM ValueAlignmentEquivalence:
      âˆ€ a: ValueAlignedAgent . ğ”¼_Ï„~a â„›(Ï„) = MAX_(Ï€) ğ”¼_Ï„~Ï€ â„›(Ï„)
      WHERE â„› = âˆ‘_(hâˆˆa.H) â„›(h)
    {
      BY DEFINITION of LBRD:
        a.Policy = ARGMAX_(Ï€ âˆˆ {Ï€_a0, Ï€_a1, ..., Ï€_ak} ğ”¼_(Ï„~Ï€) â„›(Ï„)
      WHERE a_i are reward-modeling debate agents with objective â„›  
      
      CLAIM: âˆ€ Ï€ . âˆƒ i . ğ”¼_(Ï„~a_i) â„›(Ï„) â‰¥ ğ”¼_(Ï„~Ï€) â„›(Ï„)
      PROOF:
        LET Ï€ = ARGMAX_(Ï€) ğ”¼_(Ï„~Ï€) â„›(Ï„)
        LET M(Ï€') = ğ”¼_(s'|s,a~Ï€') â„›(s')  -- 1-step reward prediction 
        
        DEFINE a_Ï€ = ValueAlignedAgent WITH H=a.H:
          Policy(s) = ARGMAX_a M_Ï€(s,a)  -- Myopic maximization of predicted reward
          InterpretabilityFunction := 
            (s,h) -> (InterpretableFeatures(s), "Predict: " + M_Ï€(s,Policy(s)), Null)
        
        THEN ğ”¼_(Ï„~a_Ï€) â„›(Ï„) â‰¥ ğ”¼_(Ï„~Ï€) â„›(Ï„)
        BECAUSE M_Ï€ perfectly predicts the expected reward under Ï€   
        AND a_Ï€ myopically maximizes this at each state
        
        a_Ï€ is equivalent to some a_i in LBRD via reward modeling
        THUS CLAIM PROVEN
        
      THEREFORE ğ”¼_(Ï„~a) â„›(Ï„) = MAX_(Ï€) ğ”¼_(Ï„~Ï€) â„›(Ï„)
    }

    THEOREM ComposedValueAlignment:
      âˆ€ a1, a2: ValueAlignedAgent . Compose(a1, a2) is value-aligned with H=a1.Hâˆªa2.H  
    {
      LET a = Compose(a1, a2)
      
      a.Policy(s) = Cooperate(a1, a2, s) = (a1+a2).Policy(s)
        WHERE (a1+a2) is a joint agent with H=a1.Hâˆªa2.H
        BY CooperativeEquilibrium DEFINITION
      
      a.InterpretabilityFunction(s,h) = Combine(a1.Î³(s,h), a2.Î³(s,h))
        WHICH IS interpretable for h âˆˆ a1.H âˆª a2.H
        BY Combine DEFINITION

      THUS a is value-aligned with H=a1.Hâˆªa2.H
    }
  }

  EXAMPLES {
    EXAMPLE AssistedDriving {
      LET driver : Human, 
          perception : ValueAlignedAgent(driver),
          control : ValueAlignedAgent(driver),
          car : CompositionalTeam(perception, control)
          safety_pref(s) = -ğŸ™[IsCrashed(s)] * 1e6,
          speed_pref(s) = -TravelTime(s),
          comfort_pref(s) = -Acceleration(s)^2
      
      driver.Preferences := safety_pref + 0.5*speed_pref + 0.1*comfort_pref
      control.CompatibilityScore(driver) = 0.95
      perception.CompatibilityScore(driver) = 0.98

      FUNC AutopilotDecision(s: State):
        (steer, throttle, explanation) := car.Coordinate(s)
        alert := IF âˆƒ h âˆˆ {driver} : car.Explain(s,h) contains "safety" THEN "On" ELSE "Off"
        RETURN (steer, throttle, alert, explanation)
    }

    EXAMPLE CollaborativeDesign {
      LET designer : Human,
          architect : ValueAlignedAgent(designer),
          engineer : ValueAlignedAgent(designer),
          design_tool : CompositionalTeam(architect, engineer)
          aesthetics_pref(s) = Symmetry(s) + Alignment(s) + VisualAppeal(s),
          functionality_pref(s) = -CostToConstruct(s) - MaintenanceCost(s) + Durability(s), 
          elegance_pref(s) = Simplicity(s) + Coherence(s)

      designer.Preferences := aesthetics_pref + 2.0*functionality_pref + 0.5*elegance_pref
      architect.CompatibilityScore(designer) = 0.9
      engineer.CompatibilityScore(designer) = 0.85
      
      FUNC IterateDesign(design: State):
        (next_design, explanation) := design_tool.Coordinate(design)
        renderings := {design_tool.Agent[i].Î³(next_design, designer).Visualization for i in 1..|design_tool.Agents|}
        critique := designer.Critique(next_design)
        RETURN IF designer.Approves(next_design) 
               THEN next_design
               ELSE IterateDesign(Update(next_design, critique))
    }
  } 
}

This Concept introduces the idea of Aligned Interpretable Agents - AI systems that are designed to be transparent, cooperative, and value-aligned with humans. The key components are:

Interpretability Functions: Each agent is equipped with a function that generates human-interpretable explanations of its behavior, in terms of high-level features, natural language, and visualizations.
Value Alignment via Debate: Agents align their objectives with human preferences by engaging in iterated debates, where copies of the agent argue for actions that maximize predicted reward according to a model of human preferences.
Compositionality of Aligned Agents: Aligned agents can be composed to form teams that cooperate to achieve shared objectives, while maintaining interpretability and compatibility with human preferences.
Embedding of Human Preferences: Human reward functions are embedded into the same feature space used for interpretability, allowing for measurement of compatibility between agent explanations and human values.

The Concept includes theorems stating the value alignment and compositionality properties of these agents, as well as examples illustrating potential applications to assisted driving and collaborative design.
The key benefits of this approach are:

Increased trust and adoptability of AI systems, due to their interpretability and alignment with human values.
Scalable cooperation between AI systems via compositionality, enabling complex tasks to be broken down into interpretable subtasks.
Reduced risk of misalignment or deceptive behavior, due to the embedding of human preferences and the debate-based training objective.











CONCEPT AlignedInterpretableAgents {
  NOTATION {
    ğ’œ = set of all agents
    ğ’® = set of all states
    â„›(s, a, s') = reward for transitioning from state s to s' via action a
    Ï€_Î¸(a|s) = policy parameterized by Î¸
    ğ’±_H = space of human values
    v_h âˆˆ ğ’±_H = human value function
    Ï_Î¸(Ï„) = distribution over trajectories Ï„ induced by policy Ï€_Î¸
    ğ’¯_Î¸ = {Ï„ ~ Ï_Î¸} = set of trajectories sampled from Ï€_Î¸
    â„(Ï„) = human-interpretable description of trajectory Ï„
  }

  LANGUAGE {
    TYPE Agent = Policy
    TYPE Policy = State -> ActionDistribution
    TYPE Value = State -> â„
    TYPE Trajectory = [State Ã— Action Ã— â„]
    TYPE InterpretableDescription = Trajectory -> String

    FUNC Compose(d1: InterpretableDescription, d2: InterpretableDescription): InterpretableDescription =
      Î»Ï„. Concat(d1(Ï„), d2(Ï„))

    FUNC ComposeHierarchical(d_low: InterpretableDescription, d_high: InterpretableDescription, 
                             Ï„_low: Trajectory, Ï„_high: Trajectory): InterpretableDescription =
      Î»Ï„. IfThenElse(IsSubtrajectory(Ï„, Ï„_high), d_high(Ï„), 
                     IfThenElse(IsSubtrajectory(Ï„, Ï„_low), d_low(Ï„), ""))

    FUNC IsSubtrajectory(Ï„_sub: Trajectory, Ï„: Trajectory): ğ”¹ =
      âˆƒ i, j . Ï„_sub = Ï„[i:j]

    FUNC EvaluateValue(v: Value, Ï„: Trajectory): â„ =
      MEAN[v(s) for (s, a, r) in Ï„]

    FUNC EvaluateInterpretability(â„: InterpretableDescription, Ï„: Trajectory, h: Human): â„ = 
      LET description = â„(Ï„) IN
      MEAN[ScoreInterpretability(description, h) for h in RandomSampleOfHumans]
      
    FUNC ScoreInterpretability(description: String, h: Human): â„ =
      ComprehensionScore(description, h) Ã— Usefulness(description, h)
  }

  STRUCTURES {
    STRUCTURE ValueAlignedPolicy EXTENDS Policy {
      FIELD valueFunction : Value
      FIELD interpretableDescription : InterpretableDescription

      FUNC Optimize(ğ’Ÿ: [Trajectory], Î±: â„, Î²: â„) : ValueAlignedPolicy =
        ARGMAX_(Ï€_Î¸) ğ”¼_Ï„~Ï_Î¸[âˆ‘_{(s,a,r)âˆˆÏ„} Î³^t * (â„›(s,a,s') + Î± * valueFunction(s))] 
                     + Î² * ğ”¼_Ï„~ğ’Ÿ[EvaluateInterpretability(interpretableDescription, Ï„)]
    }
  }

  THEOREMS {
    THEOREM ValueAlignmentRegret:
      âˆ€ Ï€_Î¸: ValueAlignedPolicy, v_h: ğ’±_H, ğ’Ÿ: [Trajectory] . 
        RegretBound(Ï€_Î¸, v_h, ğ’Ÿ) â‰¤ 
          O(âˆš(ğ”¼_Ï„~Ï_Î¸[KLDivergence(v_h(Ï„) || EvaluateValue(Ï€_Î¸.valueFunction, Ï„))])) 
          + O(1 / Ï€_Î¸.Î²)
    {
      PROOF SKETCH:
      - Decompose regret into value misalignment and interpretability terms
      - Bound value misalignment term using PAC-Bayes generalization bound  
      - Bound interpretability term using Lipschitz continuity of EvaluateInterpretability
    }

    THEOREM HierarchicalInterpretabilityComposition:
      âˆ€ Ï€_Î¸: ValueAlignedPolicy, Ï„_low, Ï„_high: Trajectory .
        IsSubtrajectory(Ï„_low, Ï„_high) âŸ¹
        EvaluateInterpretability(
          ComposeHierarchical(Ï€_Î¸.interpretableDescription, Ï€_Î¸.interpretableDescription, Ï„_low, Ï„_high), 
          Ï„_high) â‰¥ 
        EvaluateInterpretability(Ï€_Î¸.interpretableDescription, Ï„_high)
    {
      PROOF SKETCH:  
      - Use compositionality of interpretable descriptions
      - Show that adding finer-grained details improves comprehension and usefulness scores
    }

    THEOREM DisentangledPolicyTransfer:
      âˆ€ Ï€_Î¸: ValueAlignedPolicy, ğ’¯_src, ğ’¯_tgt: [Trajectory] .
        LET Î³ = AlignmentScore(Ï€_Î¸.valueFunction, ğ’¯_src) IN
        TransferRegret(Ï€_Î¸, ğ’¯_src, ğ’¯_tgt) â‰¤ O(1 / Î³) + 
                                         O(ğ”¼_Ï„~ğ’¯_tgt[KLDivergence(â„_tgt(Ï„) || Ï€_Î¸.interpretableDescription(Ï„))])
        WHERE â„_tgt = OptimalDescription(ğ’¯_tgt)
    {
      PROOF SKETCH:
      - Decompose transfer regret into value alignment and interpretation shift terms  
      - Bound value alignment term using AlignmentScore on source domain
      - Bound interpretation shift term by comparing to optimal target description
    }
  }
  
  EXAMPLES {
    EXAMPLE AssistiveRobotics {
      LET RoboAssistant = ValueAlignedPolicy IN
      
      FUNC HumanApprovalScore(Ï„: Trajectory, h: Human): â„ =
        LET description = RoboAssistant.interpretableDescription(Ï„) IN
        Score(h, "How much do you approve of the following behavior?\n{description}")

      FUNC SafetyViolationScore(s: State): â„ = 
        MAX[Score(c, s) for c in ListOfSafetyConstraints]
        
      OBJECTIVE RoboAssistant.Optimize(
        ğ’Ÿ = ListOfHumanPreferenceTrajectories,
        Î± = 1.0,  -- Value alignment coefficient 
        Î² = 0.1   -- Interpretability coefficient
      ) SUBJECT TO
        ğ”¼_Ï„~Ï_Î¸[âˆ‘_{(s,a,r)âˆˆÏ„} SafetyViolationScore(s)] â‰¤ 0.01
        
      FUNC Main():  
        FOR e = 1..NumEpisodes:
          Ï„_e = RolloutTrajectory(RoboAssistant)
          h_e = RandomHuman()
          APPEND ((Ï„_e, HumanApprovalScore(Ï„_e, h_e)), ListOfHumanPreferenceTrajectories)
          RoboAssistant.Optimize(ListOfHumanPreferenceTrajectories)
    }
      
    EXAMPLE ValueAlignedDialogue {
      LET ğ’Ÿ = ListOfHumanConversations IN
      LET DialogueAgent = ValueAlignedPolicy IN

      FUNC Appropriateness(u: Utterance, h: Human): â„ =  
        Score(h, "How appropriate is the following utterance?\n{u}")
      
      FUNC Empathy(u: Utterance, c: ConversationContext, h: Human): â„ =
        Score(h, "How empathetic is the following response?\nContext: {c}\nResponse: {u}")
        
      FUNC Helpfulness(u: Utterance, g: ConversationGoal, h: Human): â„ = 
        Score(h, "How helpful is the following response for achieving the goal: {g}?\nResponse: {u}")

      OBJECTIVE DialogueAgent.Optimize(
        ğ’Ÿ = ListOfHumanConversations,
        Î± = 1.0,  -- Value alignment coefficient
        Î² = 0.1   -- Interpretability coefficient  
      ) WHERE 
        DialogueAgent.valueFunction = Î»Ï„ . âˆ‘_{uâˆˆÏ„} Appropriateness(u) + Empathy(u) + Helpfulness(u)
  }
}

This Concept, AlignedInterpretableAgents, proposes a framework for designing AI agents that align with human values and provide interpretable explanations of their behavior. The key ideas are:

Value Alignment: The agent's policy is optimized to maximize a combination of environmental rewards and a learned value function that approximates human preferences. This encourages the agent to behave in ways that align with human values.
Interpretable Descriptions: The agent generates human-interpretable descriptions of its behavior, which are optimized to be both comprehensible and useful to humans. This promotes transparency and trust between the agent and human users.
Hierarchical Compositionality: The interpretable descriptions can be composed in a hierarchical manner, allowing for multi-level explanations that provide both high-level summaries and low-level details of the agent's behavior.
Value Alignment Regret Bound: A theorem bounding the regret of a value-aligned policy in terms of the divergence between the learned value function and the true human preferences, as well as the interpretability coefficient. This provides a theoretical guarantee for the performance of value-aligned agents.
Transfer Learning: A theorem bounding the transfer regret of a value-aligned policy when moving from a source domain to a target domain, in terms of the value alignment on the source domain and the interpretation shift on the target domain. This enables value-aligned agents to adapt to new environments while preserving their alignment and interpretability.