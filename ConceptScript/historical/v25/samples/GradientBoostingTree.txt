CONCEPT GradientBoostingTree {
  LANGUAGE {
    TYPE Feature
    TYPE Sample = List[Feature]
    TYPE Label
    TYPE WeakLearner = Sample -> Label
    TYPE Loss = (Label, Label) -> Real
    TYPE GradientStep = (WeakLearner, Real) 

    FUNC Entropy(p : Real) : Real
    FUNC TreeSplit(X : List[Sample], y : List[Label]) : (Feature, Real)
    FUNC TreeLeaf(y : List[Label]) : Label
    FUNC GradientDescent(steps : List[GradientStep], loss : Loss, 
                         X : List[Sample], y : List[Label]) : List[WeakLearner]

    AXIOM EntropyBounds {
      âˆ€ (p : Real). 0 â‰¤ p â‰¤ 1 â‡’ 0 â‰¤ Entropy(p) â‰¤ 1
    }

    AXIOM StepSizeConvergence {
      âˆ€ (steps : List[GradientStep]) (loss : Loss) (X : List[Sample]) (y : List[Label]).
        LET h_opt = GradientDescent(steps, loss, X, y),
            L(h) = Mean(Map((x, y) -> loss(h(x), y), Zip(X, y)))  
        IN ConvergesTo(Map(L, Prefixes(Map(Fst, steps))), L(h_opt))
    }

    NOTATION "H(p)" = Entropy(p)
    NOTATION "I(x ; y)" = Mutual(Marginal(x), Marginal(y))  
    NOTATION "ğ”¼[f(x)]" = Expectation(x -> f(x), Marginal(x))
    NOTATION "x âˆˆ ğ’³" = Sample(x)
    NOTATION "y âˆˆ ğ’´" = Label(y)
    NOTATION "â„‹" = Hypothesis
  }

  STRUCTURE DecisionStump(X : List[Sample], y : List[Label]) : WeakLearner {
    DEF SplitCriterion(feature : Feature, threshold : Real) : Real =
      LET (Xl, yl) = Filter((x, y) -> x[feature] â‰¤ threshold, Zip(X, y)),  
          (Xr, yr) = Filter((x, y) -> x[feature] > threshold, Zip(X, y))
      IN I(y ; y | feature > threshold)

    DEF (feature, threshold) : (Feature, Real) = 
      ArgMax((f, t) -> SplitCriterion(f, t), Cartesian(Features(X), Thresholds(X)))

    REQUIRE âˆ€ (x : Sample). x[feature] â‰¤ threshold â‡’ Classify(x) = TreeLeaf(yl)  
    REQUIRE âˆ€ (x : Sample). x[feature] > threshold â‡’ Classify(x) = TreeLeaf(yr)
  }

  STRUCTURE GradientBoostedTrees(X : List[Sample], y : List[Label], 
                                 T : Nat, Î· : Real, loss : Loss) : List[WeakLearner] {
    DEF h_0 : WeakLearner = (x : Sample) -> TreeLeaf(y)

    DEF Residuals(h : WeakLearner) : List[Label] = 
      MapThread((y, y_h) -> -âˆ‡loss(y, y_h), Zip(y, Map(h, X)))

    DEF GradientStep(h : WeakLearner, i : Nat) : GradientStep =
      LET r = Residuals(h),  
          d = DecisionStump(X, r)
      IN (d, Î·)

    REQUIRE GradientBoostedTrees(X, y, T, Î·, loss) =
      GradientDescent(Map(GradientStep(_, i), Range(T)), loss, X, y)
  }

  PROOFS {
    THEOREM GradientBoostingConvergence {
      STATEMENT:
        âˆ€ (X : List[Sample]) (y : List[Label]) (loss : Loss) (T : Nat) (Î· : Real).
          LET h_opt = GradientBoostedTrees(X, y, T, Î·, loss),  
              L(h) = ğ”¼[loss(h(x), y) | (x, y) âˆˆ Zip(ğ’³, ğ’´)]
          IN ConvergesTo(L(h_opt), Inf(h âˆˆ â„‹, L(h)))

      PROOF:
        LET X : List[Sample], y : List[Label], loss : Loss, T : Nat, Î· : Real
        LET h_opt = GradientBoostedTrees(X, y, T, Î·, loss)
        
        REWRITE L(h_opt)  
          = ğ”¼[loss(h_opt(x), y) | (x, y) âˆˆ Zip(ğ’³, ğ’´)]                      BY DEFINITION L
          = ğ”¼[loss(GradientDescent(...)(x), y) | (x, y) âˆˆ Zip(ğ’³, ğ’´)]       BY DEFINITION h_opt
          â‰¤ Inf(h âˆˆ â„‹, ğ”¼[loss(h(x), y) | (x, y) âˆˆ Zip(ğ’³, ğ’´)])              BY StepSizeConvergence
          = Inf(h âˆˆ â„‹, L(h))                                              BY DEFINITION L

        SHOW ConvergesTo(L(h_opt), Inf(h âˆˆ â„‹, L(h))) BY REWRITE  
    }

    THEOREM StumpBoundedError {  
      STATEMENT:
        âˆ€ (X : List[Sample]) (y : List[Label]).
          LET stump = DecisionStump(X, y)  
          IN ğ”¼[stump(x) â‰  y | (x, y) âˆˆ Zip(ğ’³, ğ’´)] â‰¤ 1 - Max(p, 1-p)  
             WHERE p = ğ”¼[y = 1 | y âˆˆ ğ’´]

      PROOF:
        LET X : List[Sample], y : List[Label]  
        LET stump = DecisionStump(X, y)
        LET p = ğ”¼[y = 1 | y âˆˆ ğ’´]
        
        REWRITE ğ”¼[stump(x) â‰  y | (x, y) âˆˆ Zip(ğ’³, ğ’´)]
          = ğ”¼[IF stump(x) = y THEN 0 ELSE 1 | (x, y) âˆˆ Zip(ğ’³, ğ’´)]         BY INDICATOR
          = p * ğ”¼[IF stump(x) = 1 THEN 0 ELSE 1 | y = 1] 
            + (1-p) * ğ”¼[IF stump(x) = 0 THEN 0 ELSE 1 | y = 0]           BY TOTAL_EXPECTATION
          = p * ğ”¼[stump(x) â‰  1 | y = 1] + (1-p) * ğ”¼[stump(x) â‰  0 | y = 0] BY INDICATOR
          â‰¤ p * (1 - Max(ğ”¼[stump(x) = 1 | y = 1], ğ”¼[stump(x) = 0 | y = 1]))  
            + (1-p) * (1 - Max(ğ”¼[stump(x) = 1 | y = 0], ğ”¼[stump(x) = 0 | y = 0])) 
                                                                        BY STUMP_MAX_CORRECT
          = p * (1 - ğ”¼[stump(x) = 1 | y = 1]) 
            + (1-p) * (1 - ğ”¼[stump(x) = 0 | y = 0])                      BY STUMP_OPTIMAL_SPLIT
          â‰¤ p * (1 - p) + (1-p) * p                                     BY ARITHMETIC  
          = 2p(1-p)
          â‰¤ 1 - Max(p, 1-p)                                             BY MAX_ENTROPY_BOUND
    }
  }
}