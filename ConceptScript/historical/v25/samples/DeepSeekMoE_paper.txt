PAPER DeepSeekMoE


ConceptScript v25

<Concept> ::= "CONCEPT" <ConceptName> ["[" <Params> "]"] [":" <ParentConcept>] "{"
               <Background>
               <Architecture>
               <Experiments>
               <Analysis>
               <Conclusions>  
             "}"

<Background> ::= "BACKGROUND" "{"
                   <PriorWork>
                   <Motivation> 
                 "}"

<PriorWork> ::= "PRIOR_WORK" "{"
                  (<PaperSummary>)+
                "}"

<PaperSummary> ::= "PAPER" <PaperName> "{"
                     <PaperIdeas>
                   "}"
                   
<PaperIdeas> ::= (<IdeaSummary>)+

<IdeaSummary> ::= "IDEA" <IdeaName> ":" <Description>                  

<Motivation> ::= "MOTIVATION" "{"
                   <ProblemStatement>
                   <ProposedApproach>
                 "}"

<ProblemStatement> ::= "PROBLEM" ":" <Description>

<ProposedApproach> ::= "PROPOSED_APPROACH" ":" <Description>
                 
<Architecture> ::= "ARCHITECTURE" "{"
                     (<ArchitectureComponent>)+
                   "}"

<ArchitectureComponent> ::= <ComponentName> "{"
                              (<Attribute>)+
                              (<Equation>)*
                              (<Explanation>)*
                            "}"
                            
<Attribute> ::= "ATTRIBUTE" <AttributeName> ":" <AttributeDefinition>                            
                   
<Equation> ::= "EQUATION" <EquationName> ":" <EquationDefinition>

<Explanation> ::= "EXPLANATION" ":" <Description>
  
<Experiments> ::= "EXPERIMENTS" "{"
                    (<ExperimentalSetup>)?
                    (<ExperimentalResult>)+
                    (<Ablation>)*
                    (<AnalysisItem>)*
                  "}"

<ExperimentalSetup> ::= "EXPERIMENTAL_SETUP" "{"
                          (<Hyperparameter>)+  
                          (<Dataset>)+
                          (<Baseline>)+
                          (<EvaluationMetric>)+
                        "}"

<Hyperparameter> ::= "HYPERPARAMETER" <HyperparameterName> ":" <HyperparameterValue>
  
<Dataset> ::= "DATASET" <DatasetName> ":" <DatasetDescription>

<Baseline> ::= "BASELINE" <BaselineName> ":" <BaselineDescription>  

<EvaluationMetric> ::= "EVALUATION_METRIC" <MetricName> ":" <MetricDescription>
  
<ExperimentalResult> ::= "RESULT" "{"  
                           (<ModelResult>)+
                           <Conclusion>
                         "}"
                         
<ModelResult> ::= "MODEL" <ModelName> "{"
                    (<MetricResult>)+  
                  "}"
                  
<MetricResult> ::= "METRIC" <MetricName> ":" <MetricValue>                  

<Conclusion> ::= "CONCLUSION" ":" <Description>

<Ablation> ::= "ABLATION" ":" <Description>

<AnalysisItem> ::= "ANALYSIS" <AnalysisName> ":" <Description>
  
<Analysis> ::= "ANALYSIS" "{"
                 (<AnalysisItem>)+
               "}"

<Conclusions> ::= "CONCLUSIONS" "{"
                    (<Conclusion>)+  
                    (<FutureWork>)?
                  "}"

<FutureWork> ::= "FUTURE_WORK" ":" <Description>





CONCEPT DeepSeekMoE {
  BACKGROUND {
    PRIOR_WORK {
      PAPER GShard {
        IDEA Top2Routing: Each token is assigned to the top 2 experts based on learned routing scores.
      }
      PAPER SwitchTransformer {
        IDEA Top1Routing: Each token is assigned to the top 1 expert based on learned routing scores.
      }
    }
    
    MOTIVATION {
      PROBLEM: Existing MoE architectures suffer from knowledge hybridity and knowledge redundancy, which limit expert specialization. 
      PROPOSED_APPROACH: DeepSeekMoE aims to achieve ultimate expert specialization through fine-grained expert segmentation and shared expert isolation.
    }
  }
  
  ARCHITECTURE {
    FineGrainedExperts {
      ATTRIBUTE NumExperts: ğ‘šğ‘, where ğ‘ is the number of original experts and ğ‘š is the segmentation factor.
      ATTRIBUTE NumActivatedExperts: ğ‘šğ¾, where ğ¾ is the number of activated experts in the original architecture.
      EXPLANATION: Each original expert is split into ğ‘š smaller experts. More experts are activated to keep computation cost constant.
    }
    
    SharedExperts {  
      ATTRIBUTE NumSharedExperts: ğ¾ğ‘ , a subset of experts isolated as shared experts. 
      ATTRIBUTE NumRoutedExperts: ğ‘šğ‘ âˆ’ ğ¾ğ‘ , the remaining experts used for routing.
      EXPLANATION: Shared experts are always activated to capture common knowledge and reduce redundancy in routed experts.
    }
    
    Router {
      EQUATION ExpertScores: ğ‘ ğ‘–,ğ‘¡ = Softmaxğ‘–(uğ‘™ğ‘¡ğ‘‡ Â· eğ‘™ğ‘–), where uğ‘™ğ‘¡ is the token embedding and eğ‘™ğ‘– is the expert embedding.
      EQUATION GateValues: ğ‘”ğ‘–,ğ‘¡ = I[ğ‘ ğ‘–,ğ‘¡ âˆˆ Topk({ğ‘ ğ‘—,ğ‘¡|ğ¾ğ‘  + 1 â©½ ğ‘— â©½ ğ‘šğ‘}, ğ‘šğ¾ âˆ’ ğ¾ğ‘ )] Â· ğ‘ ğ‘–,ğ‘¡, where I is the indicator function.
    }
    
    LoadBalanceLosses {
      EQUATION ExpertLoss: â„’expbal = ğ›¼1ğ‘â€² Â· âˆ‘ğ‘– ğ‘“ğ‘– ğ‘ƒğ‘–, where ğ‘“ğ‘– is the fraction of tokens assigned to expert ğ‘– and ğ‘ƒğ‘– is the average of ğ‘ ğ‘–,ğ‘¡.
      EQUATION DeviceLoss: â„’devbal = ğ›¼2 Â· âˆ‘ğ·ğ‘– ğ‘“â€²ğ‘– ğ‘ƒâ€²ğ‘–, where ğ‘“â€²ğ‘– and ğ‘ƒâ€²ğ‘– are aggregated ğ‘“ğ‘– and ğ‘ƒğ‘– within each device.
      EXPLANATION: ExpertLoss encourages balanced assignment of tokens to experts. DeviceLoss encourages balanced computation across devices.
    }
  }
  
  EXPERIMENTS {
    EXPERIMENTAL_SETUP {
      HYPERPARAMETER ExpertParams: Number of parameters per expert.
      HYPERPARAMETER BaseParams: Number of parameters in the base model (non-expert parameters).
      DATASET TrainingCorpus: A large-scale multilingual corpus created by DeepSeek-AI with 2T tokens.
      BASELINE Dense: Standard dense Transformer model. 
      BASELINE GShard: MoE model following GShard architecture.
      EVALUATION_METRIC Accuracy: Classification accuracy on downstream tasks.
      EVALUATION_METRIC Perplexity: Language modeling perplexity on the Pile test set.
    }
    
    RESULT {
      MODEL DeepSeekMoE {
        METRIC Accuracy: Achieves comparable or better accuracy than baselines on most tasks.
        METRIC Perplexity: Achieves lower perplexity than GShard baseline.  
      }
      CONCLUSION: DeepSeekMoE outperforms GShard and matches performance of larger dense models with significantly fewer FLOPS.
    }
    
    ABLATION: Removing shared experts leads to large perplexity increase, showing their importance for capturing common knowledge.
    
    ABLATION: Increasing ğ‘š (finer-grained experts) consistently improves downstream accuracy.
    
    ANALYSIS ExpertSpecialization: DeepSeekMoE achieves higher expert specialization than GShard, measured by lower expert redundancy and more accurate knowledge acquisition.
  }
  
  ANALYSIS {
    ANALYSIS ScalabilityAndTransferability: DeepSeekMoE scales favorably to 16B parameters and is effectively transferable to downstream tasks via finetuning.
  }
  
  CONCLUSIONS {
    CONCLUSION: The DeepSeekMoE architecture achieves strong performance through ultimate expert specialization using fine-grained expert segmentation and shared expert isolation.
    
    FUTURE_WORK: Explore scaling DeepSeekMoE to even larger model sizes beyond 16B parameters.
  }
}