CONCEPT DynamicProgrammingOnManifolds {
  LANGUAGE {
    TYPE Manifold(M)
    TYPE Tensor[M : Manifold, p : M, r s : â„•] = (Tangent[M, p] -> )^r -> (Cotangent[M, p] -> )^s -> â„
    TYPE Policy[M : Manifold] = M -> Tangent[M]
    TYPE Value[M : Manifold] = M -> â„
    TYPE Reward[M : Manifold] = Tensor[M, 0, 0]
    TYPE TransitionProb[M : Manifold] = Tensor[M, 1, 1]

    FUNC Bellman(ğ’« : Policy[M], ğ’± : Value[M], â„› : Reward[M], ğ’¯ : TransitionProb[M], Î³ : â„) : Value[M]
    FUNC Greedy(ğ’¬ : Tensor[M, 1, 0], ğ’¯ : TransitionProb[M]) : Policy[M]
    FUNC ExpectedValue(ğ’« : Policy[M], ğ’± : Value[M], ğ’¯ : TransitionProb[M]) : Value[M]
    FUNC GreedyImprovement(ğ’« : Policy[M], ğ’± : Value[M], â„› : Reward[M], ğ’¯ : TransitionProb[M], Î³ : â„) : Policy[M]

    AXIOM BellmanOptimality {
      âˆ€ (M : Manifold) (ğ’«* : Policy[M]) (ğ’±* : Value[M]) (â„› : Reward[M]) (ğ’¯ : TransitionProb[M]) (Î³ : â„) .
        IsOptimal(ğ’«*) <-> ğ’±* = Bellman(ğ’«*, ğ’±*, â„›, ğ’¯, Î³)
    }

    AXIOM PolicyImprovement {
      âˆ€ (M : Manifold) (ğ’« : Policy[M]) (ğ’± : Value[M]) (â„› : Reward[M]) (ğ’¯ : TransitionProb[M]) (Î³ : â„) .
        Bellman(ğ’«, ğ’±, â„›, ğ’¯, Î³) â‰¤ ğ’± => Bellman(GreedyImprovement(ğ’«, ğ’±, â„›, ğ’¯, Î³), ğ’±, â„›, ğ’¯, Î³) â‰¥ ğ’± 
    }

    NOTATION "âŸ¨âŸ¨" = Greedy
    NOTATION "âŸ©âŸ©" = ExpectedValue
    NOTATION "Î´" = GreedyImprovement
  }

  PROOFS {
    THEOREM PolicyIteration {
      STATEMENT : âˆ€ (M : Manifold) (ğ’«0 : Policy[M]) (â„› : Reward[M]) (ğ’¯ : TransitionProb[M]) (Î³ : â„) (Îµ : â„) .
        LET ğ’±[k] = Bellman(ğ’«[k], ğ’±[k-1], â„›, ğ’¯, Î³), ğ’«[k+1] = Î´(ğ’«[k], ğ’±[k], â„›, ğ’¯, Î³)
        IN âˆƒ (k : â„•) . |ğ’±*-ğ’±[k]| < Îµ âˆ§ IsOptimal(ğ’«[k])

      PROOF {
        LET M : Manifold, ğ’«0 : Policy[M], â„› : Reward[M], ğ’¯ : TransitionProb[M], Î³ : â„, Îµ : â„
        
        DEF ğ’±[k] = Bellman(ğ’«[k], ğ’±[k-1], â„›, ğ’¯, Î³)
        DEF ğ’«[k+1] = Î´(ğ’«[k], ğ’±[k], â„›, ğ’¯, Î³)

        HAVE (1) : âˆ€ (k : â„•) . ğ’±[k] â‰¤ ğ’±[k+1] BY {
          LET k : â„•
          ğ’±[k] 
            â‰¤ Bellman(ğ’«[k+1], ğ’±[k], â„›, ğ’¯, Î³) BY PolicyImprovement
            = ğ’±[k+1]                          BY DEFINITION
        }

        HAVE (2) : âˆ€ (k : â„•) . ğ’±[k] â‰¤ ğ’±* BY {
          LET k : â„•  
          SHOW ğ’±[k] â‰¤ ğ’±* BY INDUCTION {
            BASE CASE: ğ’±[0] â‰¤ ğ’±*  
            INDUCTIVE STEP: ASSUME ğ’±[k] â‰¤ ğ’±*
              SHOW ğ’±[k+1] â‰¤ ğ’±* BY {
                ğ’±[k+1] 
                  â‰¤ Bellman(ğ’«*, ğ’±[k], â„›, ğ’¯, Î³)   BY PolicyImprovement
                  â‰¤ Bellman(ğ’«*, ğ’±*, â„›, ğ’¯, Î³)    BY ASSUMPTION, Bellman monotone
                  = ğ’±*                           BY BellmanOptimality
              }
          }
        }

        HAVE (3) : âˆ€ (k : â„•) . ğ’±* - ğ’±[k] â‰¤ Î³^k * (ğ’±* - ğ’±[0]) BY {
          ; Follows from contraction mapping
        }

        SHOW âˆƒ (k : â„•) . |ğ’±* - ğ’±[k]| < Îµ âˆ§ IsOptimal(ğ’«[k]) BY {
          CHOOSE k : â„• SUCH THAT Î³^k * |ğ’±* - ğ’±[0]| < Îµ  ; Exists by (3)
          |ğ’±* - ğ’±[k]| < Îµ                              BY CHOICE of k
          IsOptimal(ğ’«[k])                              BY {
            ğ’±[k] = Bellman(ğ’«[k], ğ’±[k-1], â„›, ğ’¯, Î³)       BY DEFINITION
            ğ’±[k] = Bellman(ğ’«[k], ğ’±[k], â„›, ğ’¯, Î³)         BY PREV, (1)
            IsOptimal(ğ’«[k])                            BY BellmanOptimality
          }
        }
      }
    }

    THEOREM ValueIteration {
      STATEMENT : âˆ€ (M : Manifold) (â„› : Reward[M]) (ğ’¯ : TransitionProb[M]) (Î³ : â„) (Îµ : â„) .
        LET ğ’±[k+1](x) = (â„› âŠ• Î³âŸ¨âŸ¨ğ’±[k]âŸ©âŸ©ğ’¯)(x)  
        IN âˆƒ (k : â„•) . |ğ’±[k] - ğ’±*| < Îµ

      PROOF {
        LET M : Manifold, â„› : Reward[M], ğ’¯ : TransitionProb[M], Î³ : â„, Îµ : â„

        DEF ğ’±[k+1](x) = (â„› âŠ• Î³âŸ¨âŸ¨ğ’±[k]âŸ©âŸ©ğ’¯)(x)

        HAVE (1) : âˆ€ (k : â„•) . ğ’±[k] â‰¤ ğ’±[k+1] BY {
          ; Similar to PolicyIteration  
        }

        HAVE (2) : âˆ€ (k : â„•) . ğ’±[k] â‰¤ ğ’±* BY {
          ; Similar to PolicyIteration
        }
        
        HAVE (3) : âˆ€ (k : â„•) . ğ’±* - ğ’±[k] â‰¤ Î³^k * (ğ’±* - ğ’±[0]) BY {
          ; Follows from contraction mapping  
        }

        SHOW âˆƒ (k : â„•) . |ğ’±[k] - ğ’±*| < Îµ BY {
          CHOOSE k : â„• SUCH THAT Î³^k * |ğ’±* - ğ’±[0]| < Îµ  ; Exists by (3)
          |ğ’±[k] - ğ’±*| â‰¤ Î³^k * |ğ’±* - ğ’±[0]| < Îµ           BY CHOICE of k
        }
      }
    }
  }
}