CONCEPT GradientDescent {
  LANGUAGE {
    TYPE Params  -- Parameters of model
    TYPE Loss = Params -> Real  -- Loss function to minimize
    TYPE Gradient = Params -> Params  -- Gradient of loss function
    
    FUNC Optimize : (Params, Loss, Gradient, Nat) -> Params
    PRED Converged : (Params, Loss, Real) -> Bool

    AXIOM Descent {
      âˆ€ (Î¸ : Params) (L : Loss) (Î± : Real) (ğ›L : Gradient).
        L(Î¸ - Î± * ğ›L(Î¸)) â‰¤ L(Î¸) 
    }
    
    NOTATION "ğ›" = Gradient
    NOTATION "||" <Vector> "||" = Norm
  }

  STRUCTURE BatchGradientDescent(Î¸â‚€ : Params, L : Loss, B : Nat) {
    DEF ğ›L = AutoDifferentiate(L) 

    DEF Î· = 0.01  -- Learning rate

    DEF Optimize : (Params, Loss, Gradient, Nat) -> Params = Î» Î¸ L ğ›L n.
      MATCH n WITH
      | 0 -> Î¸
      | n' -> LET âˆ‡ = SUM i FROM 1 TO B OF ğ›L(Sample(Î¸, i)) / B
              IN Optimize(Î¸ - Î· * âˆ‡, L, ğ›L, n'-1) 
              
    DEF Converged : (Params, Loss, Real) -> Bool = Î» Î¸ L Îµ.
      ||ğ›L(Î¸)|| â‰¤ Îµ  
  }

  STRUCTURE Momentum(Î¸â‚€ : Params, L : Loss, B : Nat, Î³ : Real) EXTENDS BatchGradientDescent {
    DEF vâ‚€ : Params = 0

    DEF Optimize : (Params, Params, Loss, Gradient, Nat) -> Params = Î» Î¸ v L ğ›L n.
      MATCH n WITH 
      | 0 -> Î¸
      | n' -> LET âˆ‡ = SUM i FROM 1 TO B OF ğ›L(Sample(Î¸, i)) / B
              LET v' = Î³ * v + Î· * âˆ‡  
              IN Optimize(Î¸ - v', v', L, ğ›L, n'-1)
  }

  PROOFS {
    THEOREM Convergence {
      STATEMENT:
        âˆ€ (Î¸â‚€ : Params) (L : Loss) (ğ›L : Gradient) (Îµ : Real).
          (âˆƒ (N : Nat). âˆ€ (n : Nat). n â‰¥ N â‡’ Converged(Optimize(Î¸â‚€, L, ğ›L, n), L, Îµ))

      PROOF:
        LET Î¸â‚€ : Params, L : Loss, ğ›L : Gradient, Îµ : Real
        SUFFICES TO SHOW
          âˆƒ (N : Nat). âˆ€ (n : Nat). n â‰¥ N â‡’ ||ğ›L(Optimize(Î¸â‚€, L, ğ›L, n))|| â‰¤ Îµ

        DEFINE A = Î» n. L(Optimize(Î¸â‚€, L, ğ›L, n))
        
        HAVE (A 0 â‰¥ A 1 â‰¥ A 2 â‰¥ ...) BY {
          INDUCT ON n
          CASE 0: TRIVIAL
          CASE n': 
            A (n'+1) 
              = L(Optimize(Î¸â‚€, L, ğ›L, n'+1))
              = L(Optimize(Optimize(Î¸â‚€, L, ğ›L, n'), L, ğ›L, 1))
              â‰¤ L(Optimize(Î¸â‚€, L, ğ›L, n')) 
                BY Descent, Optimize(Î¸â‚€, L, ğ›L, n') - Î· * ğ›L(Optimize(Î¸â‚€, L, ğ›L, n'))
              = A n'
        }
        
        HAVE âˆ€ n. A n â‰¥ 0 BY {
          LET n : Nat
          L(Optimize(Î¸â‚€, L, ğ›L, n)) â‰¥ 0   BY <<L is nonnegative>>
        }
        
        HAVE Bounded(A) âˆ§ Decreasing(A) BY ABOVE
        HENCE LimExists(A) BY MonotoneConvergence

        LET Î¸* = Lim(Î» n. Optimize(Î¸â‚€, L, ğ›L, n))
        HAVE A(Î¸*) = Lim(A) BY Continuity(L), Lim(Î¸â‚™) = Î¸*
        HAVE ğ›L(Î¸*) = 0 BY GradientVanishesAtMinimum

        LET N = FIND n. ||ğ›L(Optimize(Î¸â‚€, L, ğ›L, n))|| â‰¤ Îµ   USING << ğ›L continuous >>
        SHOW âˆ€ (n : Nat). n â‰¥ N â‡’ ||ğ›L(Optimize(Î¸â‚€, L, ğ›L, n))|| â‰¤ Îµ BY {
          LET n â‰¥ N
          ||ğ›L(Optimize(Î¸â‚€, L, ğ›L, n))||
            â‰¤ ||ğ›L(Optimize(Î¸â‚€, L, ğ›L, N))||   BY ABOVE, A Decreasing
            â‰¤ Îµ   BY CHOICE OF N
        }
    }

    THEOREM MomentumSpeedup {
      STATEMENT:
        âˆƒ (Î¸â‚€ : Params) (L : Loss) (B N : Nat) (Î³ : Real).
          âˆ€ (n : Nat). n â‰¤ N â‡’ 
            ||ğ›L(Optimize(Î¸â‚€, L, AutoDifferentiate(L), n))|| â‰¥
            ||ğ›L(Momentum.Optimize(Î¸â‚€, 0, L, AutoDifferentiate(L), n))||

      PROOF:
        DEFINE QuadraticBowl(x, y) = x^2 + 100y^2
        LET Î¸â‚€ = [1, 1], L = QuadraticBowl, B = 10, N = 100, Î³ = 0.9

        SUFFICES TO SHOW 
          âˆ€ (n : Nat). n â‰¤ 100 â‡’  
            ||ğ›L(Optimize(Î¸â‚€, L, ğ›L, n))|| â‰¥ ||ğ›L(Momentum.Optimize(Î¸â‚€, 0, L, ğ›L, n))||
          USING Î¸â‚€, L, B, N, Î³    
          
        ARGUE_BY_COMPUTATION
    }
  }
}