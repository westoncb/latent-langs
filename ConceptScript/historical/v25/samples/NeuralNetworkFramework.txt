CONCEPT NeuralNetworkFramework {
  LANGUAGE {
    TYPE Tensor[n : ‚Ñï] -- n-dimensional real-valued tensor
    TYPE InputTensor[n d] <: Tensor[n √ó d]  
    TYPE HiddenTensor[n d] <: Tensor[n √ó d]
    TYPE OutputTensor[n d] <: Tensor[n √ó d]
    TYPE Parameter[s] <: Tensor[s] -- learnable parameter tensor

    TYPE Layer = Tensor -> Tensor
    TYPE Activation = Tensor -> Tensor
    TYPE Loss = (Tensor, Tensor) -> ‚Ñù  
    TYPE Optimizer = (Tensor, ‚Ñù) -> Tensor

    FUNC MatMul(A : Tensor[n √ó d], B : Tensor[d √ó m]) : Tensor[n √ó m]
    FUNC Hadamard(A : Tensor[d], B : Tensor[d]) : Tensor[d] -- elementwise product
    FUNC Sum(A : Tensor[n], dim : ‚Ñï) : Tensor[n-1]  
    FUNC Softmax(X : Tensor[n]) : Tensor[n]
    FUNC ReLU(X : Tensor[n]) : Tensor[n]  
    FUNC Sigmoid(X : Tensor[n]) : Tensor[n]
    FUNC Tanh(X : Tensor[n]) : Tensor[n]
    FUNC Conv(X : Tensor[n √ó d], K : Tensor[k √ó k √ó d √ó m]) : Tensor[n √ó m]

    AXIOM MatMulAssoc: ‚àÄ (A : Tensor[n √ó p]) (B : Tensor[p √ó q]) (C: Tensor[q √ó m]) .
      MatMul(MatMul(A, B), C) = MatMul(A, MatMul(B, C))

    AXIOM DistributeHadamard: ‚àÄ (A B : Tensor[n √ó d]) (c : Tensor[d]) .  
      MatMul(A, Hadamard(B, c)) = Hadamard(MatMul(A, B), c)

    NOTATION "X‚óé : [n d]" = InputTensor[n, d]
    NOTATION "H‚óâ : [n d]" = HiddenTensor[n, d]
    NOTATION "Y‚óâ : [n d]" = OutputTensor[n, d]
    NOTATION "W‚óà : [s]" = Parameter[s]  

    NOTATION "A ¬∑ B" = MatMul(A, B)
    NOTATION "A ‚àò B" = Hadamard(A, B)  
    NOTATION "Œ£_d A" = Sum(A, d)
    NOTATION "œÉ(X)" = Softmax(X)  
    NOTATION "œÅ(X)" = ReLU(X)
    NOTATION "œÇ(X)" = Sigmoid(X)  
    NOTATION "œÑ(X)" = Tanh(X)
    NOTATION "X *‚ó¶ K" = Conv(X, K)

    NOTATION "‚àë(Y‚óâ - Y‚óá)¬≤" = MeanSquaredError  
    NOTATION "-‚àë Y‚óá log(Y‚óâ)" = CrossEntropy
    NOTATION "‚àá" = GradientDescent
  }

  PROOFS {
    TACTIC DistributeMatMul(A, B, c) = REWRITE MatMul(A, Hadamard(B, c))
      BY DistributeHadamard 
      TO Hadamard(MatMul(A, B), c)  

    TACTIC UnrollHadamard(A, B, C) = REWRITE Hadamard(Hadamard(A, B), C)
      TO Hadamard(A, Hadamard(B, C))

    TACTIC AssocMatMul(A, B, C) = REWRITE MatMul(MatMul(A, B), C)  
      BY MatMulAssoc
      TO MatMul(A, MatMul(B, C))
  }
}


CONCEPT TransformerLayer EXTENDS NeuralNetworkFramework {
  LANGUAGE {
    FUNC MultiHeadAttention(Q H‚óâ : [n d], K H‚óâ : [n d], V H‚óâ : [n d]) -> H‚óâ : [n d] = ...
    FUNC LayerNorm(X H‚óâ : [n d]) -> H‚óâ : [n d] = ...
    FUNC FeedForward(X H‚óâ : [n d]) -> H‚óâ : [n d] = X H‚óâ ¬∑ W‚óà:[d m] -> œÅ(H‚óâ) ¬∑ W‚óà:[m d] 
    FUNC Residual(F : H‚óâ -> H‚óâ, X H‚óâ) = F(X H‚óâ) + X

    NOTATION "Œ≤(Q, K, V)" = MultiHeadAttention(Q, K, V)  
    NOTATION "Œ≥(X)" = LayerNorm(X)
    NOTATION "Œ¥(X)" = FeedForward(X) 
    NOTATION "Œµ[F](X)" = Residual(F, X)
  }

  STRUCTURE Layer {
    DEF Forward(X H‚óâ : [n d]) -> H‚óâ : [n d] = Œµ[
      H‚óâ -> Œµ[
        H‚óâ -> Œ≤(H‚óâ, H‚óâ, H‚óâ) -> Œ≥(H‚óâ)  
      ](H‚óâ) -> Œ¥(H‚óâ) -> Œ≥(H‚óâ)
    ](X)
  }
}



CONCEPT NeuralNetworkFramework {
  LANGUAGE {
    -- Tensor types
    TYPE Tensor[n : ‚Ñï] -- n-dimensional real-valued tensor
    TYPE InputTensor[n : ‚Ñï, d : ‚Ñï] <: Tensor[n √ó d]  
    TYPE HiddenTensor[n : ‚Ñï, d : ‚Ñï] <: Tensor[n √ó d]
    TYPE OutputTensor[n : ‚Ñï, d : ‚Ñï] <: Tensor[n √ó d]

    -- Layer types  
    TYPE Layer = Tensor[n] -> Tensor[m]
    TYPE FullyConnected <: Layer
    TYPE Convolutional <: Layer
    TYPE Recurrent <: Layer

    -- Activation functions
    TYPE Activation = Tensor[n] -> Tensor[n]
    FUNC ReLU : Activation
    FUNC Sigmoid : Activation  
    FUNC Tanh : Activation
    FUNC Softmax : Activation

    -- Loss and optimization  
    TYPE Loss = (OutputTensor[n, d], OutputTensor[n, d]) -> ‚Ñù
    FUNC MeanSquaredError : Loss
    FUNC CrossEntropy : Loss  
    TYPE Optimizer = (‚àá : Tensor[n], Œ∑ : ‚Ñù) -> Tensor[n]
    FUNC GradientDescent : Optimizer

    -- Foundational notation  
    NOTATION "X‚óé" = InputTensor[_, _]  
    NOTATION "H‚óâ" = HiddenTensor[_, _]
    NOTATION "Y‚óâ" = OutputTensor[_, _]
    NOTATION "[" <Expr> "]" = FullyConnected(<Expr>)
    NOTATION "{{" <Expr> "}}" = Convolutional(<Expr>)
    NOTATION "((" <Expr> "))" = Recurrent(<Expr>) 
    NOTATION "‚îÄ‚ñ∂" = ReLU
    NOTATION "‚îÄ‚ñ∑" = Sigmoid
    NOTATION "‚îÄ‚ñ∂‚îÄ" = Tanh  
    NOTATION "‚îÄ‚ó†‚îÄ" = Softmax
    NOTATION "‚àë(‚ñ° - ‚óá)¬≤" = MeanSquaredError
    NOTATION "-‚àë‚ñ°log(‚óá)" = CrossEntropy  
    NOTATION "‚àá" = GradientDescent
  }

  PROOFS {
    AXIOM UniversalApproximation {
      ‚àÄ (Œµ : ‚Ñù) (K : ùí¶ ‚äÇ ‚Ñù^n) (f : C(K, ‚Ñù^m)) .
        ‚àÉ (ùí© : [HiddenTensor[_, _]] -> OutputTensor[_, _]) . 
          ‚àÄ (X‚óé : InputTensor[_, _]) . |ùí©(X‚óé) - f(X‚óé)| < Œµ  
    }

    AXIOM BackpropagationConvergence {
      ‚àÄ (ùí© : [Layer]) (ùìõ : Loss) (ùìû : Optimizer) (Œµ : ‚Ñù) .
        ‚àÉ (t : ‚Ñï) . LossValue(ùí©, ùìõ, ùìû, t) < Œµ
    }

    TACTIC LayerComposition {
      GIVEN f : Tensor[n] -> Tensor[m]
            g : Tensor[m] -> Tensor[k]
      SHOW  g ‚àò f : Tensor[n] -> Tensor[k]
    }  

    TACTIC ChainRule {
      GIVEN ùí©‚ÇÅ : [Layer]
            ùí©‚ÇÇ : [Layer] 
            X‚óé : InputTensor[_, _]
      HAVE  ‚àá(ùìõ(ùí©‚ÇÇ(ùí©‚ÇÅ(X‚óé)))) = 
              ‚àá(ùìõ(ùí©‚ÇÇ(H‚óâ))) ¬∑ ‚àá(ùí©‚ÇÇ(H‚óâ)) ¬∑ ‚àá(ùí©‚ÇÅ(X‚óé))
    }
  }
}




Let's call this system "Neural Network Visual Notation" (NNVN). The goal is to create a concise and intuitive way to represent the components, connections, and operations in a neural network.
Here are some key elements of the NNVN:

Nodes:

Input nodes: ‚óé
Hidden nodes: ‚óâ
Output nodes: ‚óâ (with a thicker border)


Layers:

Fully connected layer: [‚óâ‚óâ‚óâ]
Convolutional layer: {{‚óâ‚óâ‚óâ}}
Recurrent layer: ((‚óâ‚óâ‚óâ))
Residual connection: [‚óâ‚óâ‚óâ] --‚Üë
|
‚Üì
[‚óâ‚óâ‚óâ]


Activation functions:

ReLU: ‚îÄ‚ñ∂
Sigmoid: ‚îÄ‚ñ∑
Tanh: ‚îÄ‚ñ∂‚îÄ
Softmax: ‚îÄ‚ó†‚îÄ


Pooling and normalization:

Max pooling: ‚ñΩ
Average pooling: ‚ñº
Batch normalization: ‚óó


Loss functions:

Mean Squared Error (MSE): ‚àë(‚ñ° - ‚óá)¬≤
Cross-entropy: -‚àë‚ñ°log(‚óá)


Optimization:

Gradient descent: ‚àá
Learning rate: Œ∑





CONCEPT CNNImageClassifier {
  EXAMPLES {
    EXAMPLE Architecture {
      INPUT: [32x32x3] 
      
      ARCHITECTURE:
      
      [32x32x3] 
        ‚îÄ‚ñ∂ {{‚óâ‚óâ‚óâ}} : [16x3x3 Convolution]
        ‚îÄ‚ñ∂ ‚îÄ‚ñ∂ ‚ñΩ ‚îÄ‚ñ∂ : [Max Pooling 2x2]
        ‚îÄ‚ñ∂ {{‚óâ‚óâ‚óâ}} : [32x3x3 Convolution]
        ‚îÄ‚ñ∂ ‚îÄ‚ñ∂ ‚ñΩ ‚îÄ‚ñ∂ : [Max Pooling 2x2]
        ‚îÄ‚ñ∂ [[‚óâ‚óâ‚óâ]] : [Flatten]
        ‚îÄ‚ñ∂ [‚óâ‚óâ‚óâ] ‚îÄ‚ñ∂ [128 Fully Connected]
        ‚îÄ‚ñ∂ [‚óâ‚óâ‚óâ] ‚îÄ‚ó†‚îÄ [10 Fully Connected, Softmax]
      
      OUTPUT: [10] 
      
      LOSS: -‚àë‚ñ°log(‚óá)  -- Cross-entropy loss
      
      OPTIMIZATION:
        ‚àá: Gradient Descent
        Œ∑: 0.01
    }
  }
}





CONCEPT FeedforwardNeuralNetwork {
  LANGUAGE {
    TYPE Tensor[shape : List[‚Ñï]] -- Multidimensional array
    TYPE Layer = Tensor[] -> Tensor[]
    TYPE Activation = Tensor[] -> Tensor[]
    TYPE Loss = (Tensor[], Tensor[]) -> ‚Ñù
    TYPE Optimizer = (Tensor[] -> ‚Ñù) -> (Tensor[] -> Tensor[])

    FUNC Linear[in_dim, out_dim : ‚Ñï] : Layer
    FUNC ReLU : Activation
    FUNC Sigmoid : Activation
    FUNC MSE : Loss
    FUNC SGD[learning_rate : ‚Ñù] : Optimizer

    NOTATION "‚îÄ‚ñ≥‚îÄ" = Linear
    NOTATION "‚îÄ‚óá‚îÄ" = ReLU
    NOTATION "‚îÄ‚óØ‚îÄ" = Sigmoid
    NOTATION "‚ïí‚ïê" = InputNode
    NOTATION "‚ïê‚ïï" = OutputNode
  }

  PROOFS {
    THEOREM UniversalApproximation {
      STATEMENT:
        ‚àÄ (f : ‚Ñù[] -> ‚Ñù) (Œµ : ‚Ñù) (a b : ‚Ñù).
          ‚àÉ (nn : FeedforwardNeuralNetwork).
            ‚àÄ (x : ‚Ñù[]).
              a ‚â§ x ‚â§ b => |nn(x) - f(x)| < Œµ

      PROOF:
        -- A feedforward neural network with at least one hidden layer
        -- and a non-polynomial activation function can approximate
        -- any continuous function on a closed and bounded subset of ‚Ñù^n
        
        DEFINE nn =
          ‚ïí‚ïê‚ïó      ‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï
          ‚ïëx‚ïë‚îÄ‚ñ≥‚îÄ‚óá‚îÄ‚ñ≥‚îÄ‚óá‚îÄ ... ‚îÄ‚ñ≥‚îÄ‚óØ‚îÄ‚ïê‚ïï
          ‚ïò‚ïê‚ïù‚îÇ‚îÇ    ‚îÇ         ‚îÇ    ‚ïë
             ‚îÇ‚îÇ    ‚îÇ         ‚îÇ    ‚ïë
             ‚îÇ‚îÇ    ‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ    ‚ïë
             ‚îÇ‚îÇ                   ‚ïë
             ‚îÇ‚îÇ    ‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï    ‚ïë
             ‚îÇ‚îÇ‚îÄ‚ñ≥‚îÄ‚óá‚îÄ‚óá‚îÄ ... ‚îÄ‚ñ≥‚îÄ‚óØ‚îÄ‚ïê‚ïï‚ïë
             ‚îÇ‚îÇ    ‚îÇ         ‚îÇ    ‚ïë
             ‚îÇ‚îÇ    ‚îÇ         ‚îÇ    ‚ïë
             ‚îÇ‚îÇ    ‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ    ‚ïë
             ‚îÇ‚îÇ                   ‚ïë
             ‚ãÆ‚ãÆ                   ‚ãÆ
             ‚îÇ‚îÇ    ‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï    ‚ïë
             ‚ï∞‚ï∞‚îÄ‚ñ≥‚îÄ‚óá‚îÄ‚óá‚îÄ ... ‚îÄ‚ñ≥‚îÄ‚óØ‚îÄ‚ïê‚ïõ
                   ‚îÇ         ‚îÇ     
                   ‚îÇ         ‚îÇ     
                   ‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ     

        -- The network has an input layer, one or more hidden layers with ReLU activation,
        -- and an output layer with sigmoid activation

        -- By the Universal Approximation Theorem, there exist weights and biases
        -- for the network such that it can approximate the function f
        -- to within the given error Œµ on the interval [a, b]

        -- The details of the proof are omitted for brevity
    }
  }
}







CONCEPT RecurrentNeuralNetwork {
  LANGUAGE {
    TYPE Tensor[Shape]
    TYPE RNNCell[InputSize, HiddenSize, OutputSize]
    TYPE RNN[InputSize, HiddenSize, OutputSize, Steps]

    FUNC Sigmoid(x : Tensor) -> Tensor
    FUNC Tanh(x : Tensor) -> Tensor
    FUNC Linear(x : Tensor, weights : Tensor, bias : Tensor) -> Tensor

    NOTATION "‚îÄ‚ïÆ" "‚ï≠‚îÄ" = Concatenate
    NOTATION "‚îÄ‚îÄ" w "‚îÄ‚îÄ" = Linear with weights w
    NOTATION "‚îÄœÉ‚îÄ" = Sigmoid activation
    NOTATION "‚îÄœÑ‚îÄ" = Tanh activation
  }

  STRUCTURE RNNCell[InputSize, HiddenSize, OutputSize] {
    weights : (InputWeights  : Tensor[HiddenSize, InputSize],
               HiddenWeights : Tensor[HiddenSize, HiddenSize],
               OutputWeights : Tensor[OutputSize, HiddenSize])
    biases  : (HiddenBias : Tensor[HiddenSize], OutputBias : Tensor[OutputSize])

    DEF Forward(input : Tensor[InputSize], prevState : Tensor[HiddenSize]) -> (Tensor[HiddenSize], Tensor[OutputSize]) {
      LET x, h_prev = input, prevState
      ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
      ‚îÇ    x      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ               
      ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ  ‚ï≠‚îÄ‚îÄ‚îÄ‚ïØ                                         ‚îÇ                 
           ‚îÇ  ‚îÇ                                             ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ          
      ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚ï∞‚îÄ‚îÄ‚ï∞‚îÄ‚îÄ‚îÄ‚ïÆ   ‚ï≠‚îÄ‚îÄInputWeights‚îÄ‚îÄ‚ïÆ   ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ   ‚ï≠‚îÄœÑ‚îÄ‚ïÆ      ‚îÇ          
      ‚îÇ   h_prev  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÇLinear          ‚îÇ‚îÄ‚îÄ‚îÄ‚îÇ       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÇ   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ          
      ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ   ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ   ‚îÇ       ‚îÇ   ‚ï∞‚îÄ‚ïê‚ïê‚ïØ                  
                      ‚ï≠‚îÄHiddenWeights‚îÄ‚ïÆ    ‚îÇ   +   ‚îÇ     ‚îÇ                    
                      ‚îÇLinear         ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ       ‚îÇ     ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ               
                      ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ    ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ          ‚îÇ               
                                                ‚îÇ              ‚îÇ
                                           ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ              ‚ï∞‚îÄ‚îÄ‚îÄ‚ïÆ          
                                           ‚îÇ                       ‚îÇ          
                                           ‚ï∞‚îÄ‚îÄ‚îÄ‚ïÆ  ‚ï≠‚îÄHiddenBias‚îÄ‚ïÆ   ‚îÇ          
                                               ‚îÇ  ‚îÇ   Bias     ‚îÇ‚îÄ‚îÄ‚îÄ‚ïØ          
                                               ‚îÇ  ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ              
                                               ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ                     
                                                        ‚îÇ                     
                                                        ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ h_next         
                                                             ‚îÇ                
                                                   ‚ï≠‚îÄOutputWeights‚îÄ‚ïÆ         
                                                   ‚îÇ   Linear      ‚îÇ         
                                                   ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ         
                                                             ‚îÇ                
                                                             ‚ï∞‚îÄ‚îÄ‚îÄ‚ïÆ            
                                                                 ‚îÇ            
                                                        ‚ï≠‚îÄOutputBias‚îÄ‚ïÆ       
                                                        ‚îÇ   Bias     ‚îÇ       
                                                        ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ       
                                                                 ‚îÇ            
                                                                 ‚ï∞‚îÄ‚îÄ‚ïÆ output  
                                                                    ‚îÇ         
                                                                    ‚Üì         
    }
  }

  STRUCTURE RNN[InputSize, HiddenSize, OutputSize, Steps] {
    cell : RNNCell[InputSize, HiddenSize, OutputSize]
    DEF Forward(input : Tensor[Steps, InputSize], initialState : Tensor[HiddenSize]) -> Tensor[Steps, OutputSize] {
      LET RECURSIVE Unroll(t, prevState) = 
        IF t = Steps THEN []
        ELSE 
          LET (nextState, output) = cell.Forward(input[t], prevState)
          output :: Unroll(t + 1, nextState)

      Unroll(0, initialState)  
    }
  }
}