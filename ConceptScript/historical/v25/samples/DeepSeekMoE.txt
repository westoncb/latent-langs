CONCEPT DeepSeekMoE {
  LANGUAGE {
    TYPE Token
    TYPE Expert
    TYPE SharedExpert <: Expert  
    TYPE RoutedExpert <: Expert
    TYPE Embedding = Vector[Real, d]
    TYPE FFN = Embedding -> Embedding
    TYPE AdamWOptimizer = (lr : Real, beta1 : Real, beta2 : Real, eps : Real, weight_decay : Real, Params) -> Params
    TYPE Corpus
    
    FUNC NumParams : Nat
    FUNC NumLayers : Nat  
    FUNC NumExperts : Nat
    FUNC NumSharedExperts : Nat
    FUNC NumRoutedExperts : Nat
    FUNC HiddenDimension : Nat
    FUNC NumAttentionHeads : Nat
    FUNC RoutedExpertsPerToken : Nat
    FUNC ActivationFactor : Real
    FUNC RelativeExpertSize : Real
    FUNC SequenceLength : Nat
    FUNC BatchSize : Nat
    FUNC InitStdDev : Real

    FUNC Tokenize : Corpus -> List[Token]
    FUNC TrainTokenizer : Corpus -> Tokenizer  
    FUNC SampleTrainingData : Corpus -> (Nat, List[List[Token]])
    
    FUNC Initialize : Params  
    FUNC Optimizer : AdamWOptimizer
    FUNC Train : (Corpus, Params) -> Params

    FUNC Router : Token -> List[RoutedExpert]
    FUNC Eval : Expert -> Embedding -> Embedding
    FUNC Combine : List[Embedding] -> Embedding

    FUNC ExpertLoss : (List[Real], List[Real]) -> Real
    FUNC DeviceLoss : (List[Real], List[Real]) -> Real

    PRED Specialized : Expert -> Bool

    AXIOM LoadBalanceLemma : ∀ (t : Token) (experts : List[RoutedExpert]).
      Uniform(Map((e : Expert) -> P(Route(t) = e), experts))

    NOTATION "𝑁" = NumExperts  
    NOTATION "𝐾𝑠" = NumSharedExperts
    NOTATION "𝑚" = ActivationFactor  
    NOTATION "𝐾" = NumRoutedExperts
    NOTATION "𝑑" = HiddenDimension
  }

  STRUCTURE ModelConfig {
    REQUIRE NumParams = NumLayers * (12 * HiddenDimension^2 + 13 * HiddenDimension +
      (NumLayers - 1) * (NumSharedExperts * RelativeExpertSize * 8 * HiddenDimension^2 + 
                         NumRoutedExperts * RelativeExpertSize * 8 * HiddenDimension^2))
    
    REQUIRE NumExperts = NumSharedExperts + NumRoutedExperts  

    REQUIRE ∀ (layer : 1..NumLayers - 1).
      NumSharedExperts(layer) = NumSharedExperts ∧
      NumRoutedExperts(layer) = NumRoutedExperts ∧  
      ∀ (expert : Expert) ∈ layer. Dimension(expert) = 4 * RelativeExpertSize * HiddenDimension

    REQUIRE SINGLE_GPU_DEPLOYABLE(
      (NumParams < 16.5B) ∧
      (ActivatedParams = NumSharedExperts * 8 * RelativeExpertSize * HiddenDimension^2 +
                         RoutedExpertsPerToken * 8 * RelativeExpertSize * HiddenDimension^2 
                        < 3B)
    )
  }

  STRUCTURE TrainingProcess {  
    DEF Tokenizer : Tokenizer = TrainTokenizer(Corpus)

    DEF Vocab : Nat =
      MATCH NumParams {  
        2B -> 8K
        16B -> 100K
        145B -> 100K
      }

    DEF (NumTokens, Data) : (Nat, List[List[Token]]) = SampleTrainingData(Corpus)
      WHERE MATCH NumParams {
        2B -> NumTokens = 100B
        16B -> NumTokens = 2T
        145B -> NumTokens = 245B  
      }

    DEF InitParams : Params = Initialize(Params) WHERE InitStdDev = 0.006

    DEF TrainedParams : Params = Train(Data, InitParams)  
      WHERE {
        MATCH NumParams {
          2B -> {  
            Optimizer = AdamWOptimizer(lr = 1.08e-3, beta1 = 0.9, beta2 = 0.95, eps = 1e-8, weight_decay = 0.1)
            SequenceLength = 2K
            BatchSize = 2K  
            NumSteps = 25K
            AlphaExpBal = 0.01
          }
          16B -> {
            Optimizer = AdamWOptimizer(lr = 4.2e-4, beta1 = 0.9, beta2 = 0.95, eps = 1e-8, weight_decay = 0.1) 
            SequenceLength = 4K
            BatchSize = 4.5K
            NumSteps = 106449  
            AlphaExpBal = 0.001
          }  
          145B -> {
            Optimizer = AdamWOptimizer(lr = 3.0e-4, beta1 = 0.9, beta2 = 0.95, eps = 1e-8, weight_decay = 0.1)
            SequenceLength = 4K
            BatchSize = 4.5K
            NumSteps = 13K
            AlphaExpBal = 0.003
            AlphaDevBal = 0.05
          }
        }
        REQUIRE ∀ (step : 1..NumSteps).  
          LET lr_scale = 
            MATCH step {
              IN 1..2K -> step / 2K
              IN 80%*NumSteps+1..90%*NumSteps -> 0.316  
              ELSE -> IF NumParams = 145B THEN 1 ELSE 0.1
            }  
          IN Optimizer.lr(step) = lr_scale * Optimizer.lr
      }
  }

  STRUCTURE MoE(shared_experts, routed_experts) : FFN {
    REQUIRE NumExperts(shared_experts ++ routed_experts) = 𝑁 
    REQUIRE NumSharedExperts(shared_experts) = 𝐾𝑠
    REQUIRE NumRoutedExperts(routed_experts) = 𝑚 * (𝑁 - 𝐾𝑠)
    REQUIRE ∀ (e : Expert) ∈ routed_experts. Dimension(e) = 𝑑 / 𝑚

    DEF Forward(input : List[Token]) -> List[Embedding] = 
      LET activated_shared = Map((t : Token) -> MapReduce(Eval, Combine, shared_experts, t), input),
          activated_routed = FlatMap((t : Token) -> Map(Eval(t), Router(t)), input)  
      IN Map(Combine, Zip(activated_shared, activated_routed))
  }

  FUNC Router(t : Token) -> List[RoutedExpert] = 
    Topk(Map((e : RoutedExpert) -> Softmax(DotProduct(e, t)), routed_experts), 𝑚𝐾 - 𝐾𝑠)

  FUNC ExpertLoss(frequencies, probabilities) : Real =
    𝛼_exp * Sum(Zip(frequencies, probabilities))  

  FUNC DeviceLoss(frequencies, probabilities) : Real =
    𝛼_dev * Sum(Map(Combine, Zip(frequencies, probabilities)))

  PROOFS {
    THEOREM ExpertSpecialization {
      STATEMENT:  
        ∀ (input : List[Token]).
          LET routed_experts = Drop(𝐾𝑠, MoE.experts) 
          IN ∀ (e : RoutedExpert) ∈ Activate(input, routed_experts). Specialized(e)

      PROOF:
        LET N = NumRoutedExperts,  
            K = RoutedExpertsPerToken,
            old_N = N / 4,
            old_K = 2  
        IN {
          HAVE NumCombinations(N, K) > NumCombinations(old_N, old_K) BY CombinatoricsLemma
          SHOW HigherSpecialization(routed_experts)  
        }
    }

    THEOREM ComputationEfficiency {
      STATEMENT:
        ∀ (input : List[Token]).  
          ComputationCost(DeepSeekMoE, input) ≈ 40% * ComputationCost(DenseModel, input)
      
      PROOF:
        MATCH NumParams {
          16B -> {
            LET ActivatedParamsMoE = NumSharedExperts * 8 * RelativeExpertSize * HiddenDimension^2 +
                                     RoutedExpertsPerToken * 8 * RelativeExpertSize * HiddenDimension^2,
                TotalParamsDense = 12 * NumLayers * HiddenDimension^2
            HAVE ActivatedParamsMoE ≈ 32 * HiddenDimension^2
            HAVE TotalParamsDense ≈ 80 * HiddenDimension^2  
            SHOW ComputationCost(DeepSeekMoE, input) / ComputationCost(DenseModel, input) 
              ≈ ActivatedParamsMoE / TotalParamsDense
              ≈ 40%
          }

          145B -> {
            LET ActivatedParamsMoE = 32 * HiddenDimension^2,  
                ActivatedParamsHalfMoE = 16 * HiddenDimension^2,
                TotalParamsDense = 176 * HiddenDimension^2
            SHOW ComputationCost(DeepSeekMoE, input) / ComputationCost(DenseModel, input) ≈ 18.2%
            SHOW ComputationCost(DeepSeekMoE, HalfRouted, input) / ComputationCost(DenseModel, input) ≈ 9.1%  
          }
        }
    }

    THEOREM ParameterRedundancy {
      STATEMENT: 
        ∀ (ratio : Real).
          LET accuracy_drop(model, ratio) =
                Accuracy(model, full_experts) - Accuracy(model, (1-ratio) * full_experts)  
          IN accuracy_drop(DeepSeekMoE, ratio) > accuracy_drop(GShard, ratio)

      PROOF:
        LET shared_pct = NumSharedExperts / NumExperts,  
            routed_pct = 1 - shared_pct
        IN {  
          ASSUME accuracy_drop(GShard, ratio) = routed_pct * ratio
          ASSUME accuracy_drop(DeepSeekMoE, ratio) = shared_pct * 0 + routed_pct * 2 * ratio

          SHOW accuracy_drop(DeepSeekMoE, ratio) > accuracy_drop(GShard, ratio) 
            BY routed_pct * 2 * ratio > routed_pct * ratio
        }
    }
  }
}